
---
## **Locality**

프로그램은 실행 중에 **전체 주소 공간 중 일부만 집중적으로 접근**하는 경향이 있다. 이로부터 두 가지 중요한 **locality(지역성)** 개념이 도출된다:

##### ✅ Temporal Locality (시간적 지역성)

**최근에 접근한 데이터나 명령어가 곧 다시 접근될 가능성이 높다.**

- 예시: 반복문 안에서 반복적으로 사용되는 변수나 instruction
    
- 의미: 캐시에 최근 사용된 데이터를 유지하면 성능 향상 가능

##### ✅ Spatial Locality (공간적 지역성)

**최근 접근된 메모리 위치의 인접한 위치도 곧 접근될 가능성이 높다.**

- 예시: 연속된 배열 요소, 순차적으로 실행되는 명령어
    
- 의미: 한 번에 블록 단위로 메모리를 가져오면 효율적

----
## **Taking Advantage of Locality**

실제 메모리는 **계층적 구조(hierarchical structure)** 로 이루어져 있다.

가장 아래에는 **디스크**가 있다. **매우 느리지만 용량이 크며**, 모든 데이터를 영구적으로 저장한다.
    
디스크보다 빠르지만 상대적으로 작은 메모리가 바로 **DRAM**, 즉 **메인 메모리(Main Memory)** 다.  프로그램이 실행 중일 때 필요한 데이터 중 **자주 접근하고, 가까운 데이터들**을 이곳에 올려둔다.
    
DRAM보다 더 빠르고, 더 작은 메모리가 바로 **SRAM**, 즉 **캐시 메모리(Cache)** 다.  DRAM에 있는 데이터 중에서도 **가장 자주 사용되고, 최근에 접근된 데이터들**을 캐시에 저장한다.

이러한 메모리 계층은 **속도와 용량의 절충** 구조로 설계되며,  **Locality(지역성) 원리**를 기반으로 하여  
자주 쓰이는 데이터가 **더 빠른 계층에 위치하도록 하여 전체 시스템 성능을 향상**시킨다.

---
## **Memory Hierarchy Levels**

![](../images/Pasted%20image%2020250528000025.png)

각 단계에서 데이터를 가져올 때는 **데이터 하나씩이 아니라 일정 단위로 묶어서 가져오며**, 이 단위를 **block** 또는 **cache line**이라고 부른다.  데이터를 하나만 가져오면 **공간적 지역성(spatial locality)** 을 활용하지 못하므로 손해이기 때문이다.  예를 들어 현대 컴퓨터에서는 보통 **32바이트 또는 64바이트**를 하나의 block으로 사용한다.

프로세스가 데이터를 접근할 때는 **가장 빠른 단계(보통 캐시)부터 확인**하게 되며, 찾고자 하는 데이터가 해당 단계에 **존재하면 이를 "Hit"** 라고 하고, 전체 접근 중에서 Hit이 발생한 비율을 **Hit Ratio**라고 한다. 만약 그 단계에 없다면, 다음 단계(더 느리고 큰 계층)로 내려가야 하며,  이 경우를 **"Miss"** 라고 한다.

데이터를 발견했을 경우에는 **그 데이터만 가져오는 것이 아니라**, **그 데이터가 포함된 block 전체를 상위 계층으로 가져온다.** 이는 다음 번에 인접한 데이터를 사용할 가능성이 높기 때문 (→ 공간적 지역성 활용).

만약 더 밑의 단계까지 내려가서 데이터를 찾게 되면, **그만큼 접근 시간이 더 길어지며**, 이 추가 시간을 **Miss Penalty**라고 한다.

마지막으로 전체 접근 중 **Miss가 발생한 비율은 Miss Ratio**라고 하며,  
이는 단순히 `1 - Hit Ratio`로 계산할 수 있다.

---
## **Memory Technology**

| **Memory Type**        | **Access Time**      | **Cost per GB**         |        |
| ---------------------- | -------------------- | ----------------------- | ------ |
| **Static RAM (SRAM)**  | 0.5ns ~ 5ns          | $400 ~ $800             | 캐시     |
| **Dynamic RAM (DRAM)** | 20ns ~ 50ns          | $2 ~ $4                 | 메인 메모리 |
| **NAND Flash (SSD)**   | 250µs ~ 1000µs       | $0.04 ~ $0.10           | 스토리지   |
| **Magnetic Disk**      | 2ms ~ 20ms           | $0.01 ~ $0.02           | 스토리지   |
| **Ideal Memory**       | SRAM 수준의 Access Time | 디스크 수준의 Capacity 및 Cost |        |
프로그램이 사용하는 주소는 메인 메모리에서 저장된다고 보면 된다.


---
## **Cache Memory**

CPU와 굉장히 가까이에 있어서 빠른 속도로 접근할 수 있다.

메인 메모리의 경우 특정 주소로 접근하려고 하면 그냥 하면 되지만, cache 메모리에는 접근하려는 주소가 있을 수도 있고 없을 수 있다. 따라서 해당 데이터가 cache 메모리에 있는지 없는지 확인해 봐야 한다.

가장 쉬운 방법은 cache 메모리에 있는 데이터에 어디서 왔는지 주소를 다 적어서 넣어놓는 것이다. 그러나 이것은 굉장히 비효율적이다.

---
## **Direct Mapped Cache**

위의 비효율적인 방법 대신에, **주소마다 저장될 수 있는 칸을 고정하는 방법**이 있다.  즉, **주소가 주어지면 cache 메모리의 특정 위치에만 저장되도록 연결된 공간이 정해지는 방식**이다.

![](../images/Pasted%20image%2020250528001858.png)

자, 위의 예시처럼 메인 메모리에는 32개의 칸이 있고, Cache에는 8개의 칸이 있다고 하자.  
그렇다면 메인 메모리의 각 칸을 구별하기 위해서는 2<sup>5</sup>=32 이므로 **5개의 비트**로 주소를 구분할 수 있고,  
Cache 메모리는 2<sup>3</sup>=8이므로 **3개의 비트**로 구분할 수 있다.

그래서 **메인 메모리의 하위 3비트**를 확인해서 **Cache 메모리의 3비트와 비교**했을 때,  **똑같은 위치에만 메인 메모리의 데이터가 저장될 수 있는 것**이다.  

>즉, **Cache 메모리의 각 칸에는 올 수 있는 메인 메모리 주소가 정해져 있다는 의미**다.

그래서 **하위 3비트는 Cache 메모리의 위치를 결정**하는 데 사용되므로,  해당 위치를 통해 이미 알고 있는 정보이다.  따라서 **Cache 메모리 내부에는 나머지 상위 2비트만 저장**해두면,  이를 통해 **메인 메모리의 전체 주소를 복원하거나 확인**할 수 있게 된다.

---
## **Tags and Valid Bits**

따라서 해당 Cache 메모리 안에 있는 메인 메모리를 나타내는 값들을 **Tag**라고 한다.

만약 컴퓨터를 처음 켰을 때를 생각해보면, **Cache 메모리는 비어 있는 상태**가 된다.  이처럼 Cache 메모리 안에 있는 데이터가 **유효할 수도 있고, 유효하지 않을 수도 있기 때문에**,  이를 확인하기 위한 **Valid bit**가 하나 포함되어 있다.

따라서 주소의 **하위 비트**를 통해 Cache 메모리에서 **위치를 찾고**,  **상위 비트(Tag)** 를 통해 메인 메모리의 해당 주소가 **실제로 맞는지 확인**하며,  **Valid bit**를 통해 그 데이터가 **유효한지 아닌지를 판단**하게 된다.

---
## **Cache Memory**

8개의 block으로 이뤄져있고, block마다 1word가 저장된다. 즉 용량이 32byte인 cache이다 그리고 direct mapped 방식을 사용한다고 가정하자.

##### 1️⃣ 초기값

![](../images/Pasted%20image%2020250528003243.png)
완전 초기값에는 **Tag나 Data 값이 임의의 값으로 채워져 있을 수 있지만**,  **Valid bit가 0**이기 때문에 **이 데이터는 무시되며 적용되지 않는다.**

즉, **Valid bit가 1이 되어야만** 해당 Tag와 Data가 **유효한 값으로 인식**되고,  **Valid bit가 0이면 아무리 Tag와 Data가 일치하더라도 cache miss로 처리**된다.

![](../images/Pasted%20image%2020250528003308.png)

22번 주소가 왔다고 가정하면, 2진수로 표현하면 **10110**이다.

이 중에서 **하위 3비트(110)** 를 확인해 보면, 해당 위치의 **Valid bit가 0**, 즉 **유효하지 않다.**  따라서 이 주소에 해당하는 데이터는 **Cache에 존재하지 않는 것(miss)** 으로 판단하게 된다. 그러면 **메인 메모리에서 원하는 데이터를 가져와서 사용**하게 되며,  **같은 Cache 위치(하위 3비트 = 110)** 에 해당 데이터를 **저장**한다.

이때 상위 비트인 **Tag = 10** (즉, 상위 2비트) **Valid bit는 1로 설정**한다. 즉, 다음부터 해당 위치를 다시 접근할 경우,  **Tag가 일치하고 Valid bit도 1이면 cache hit으로 동작**하게 된다.

![](../images/Pasted%20image%2020250528003620.png)

만약 cache 메모리에 원하는 Tag값이 있고, valid 하다면 메인 메모리까지 가지 않고, cache에서 가져가면 된다!

---

![](../images/Pasted%20image%2020250528003729.png)

Index는 맞지만 Tag가 다르다면 원하는 데이터가 아니므로, 메인 메모리에서 가져온 다음 원래 있던 데이터 대신에 새로 가져온 데이터를 Cache에 넣는다. 원래 저장되어 있던 데이터는 쫓겨나게 되는데 이 과정을 evict이라고 한다. 여기서 만약 쫓겨난 데이터가 변경되었다면 어떻게 처리할지도 나중에 알아보자.


---
## **Address Subdivision**

![](../images/Pasted%20image%2020250530165232.png)

---
## **Example: Larger Block Size**

![](../images/Pasted%20image%2020250530165423.png)

offset 4비트 = 한 블록의 크기가 2<sup>4</sup> byte
Index 6비트 = 블록의 개수가 2<sup>6</sup>

Cache size = block 크기 * block의 개수 =  2<sup>10</sup> byte  = 16KB

만약에 offset 즉 block 의 크기를 늘리면 index 즉 block의 개수는 줄어든다. 

---
## **Block Size Considerations**

한 블록의 크기를 크게 하면, 일반적으로 **공간적 지역성(spatial locality)**에 따라 **캐시 miss rate를 줄이는 데 도움이 될 수 있다.**  그 이유는, 블록 크기를 키운다는 것은 한 번에 **더 많은 인접 데이터를 함께 캐시에 불러온다는 뜻**이며,  프로세스는 실제로 인접한 메모리 영역을 자주 참조하는 경향이 있기 때문이다.

하지만 캐시 용량이 **고정되어 있다면**, 블록 크기를 키우는 것은 곧 **블록 개수를 줄이는 것**을 의미한다.  예를 들어, 캐시 전체가 1KB이고 블록 크기를 1KB로 설정하면, 캐시에는 **블록 하나만** 저장할 수 있게 된다.  이럴 경우 **인접하지 않은 여러 데이터 블록들이 동일한 캐시 공간을 경쟁하게 되며**,  오히려 캐시 교체가 빈번해지고 miss rate가 높아질 수도 있다.

이런 현상은 특히 **서로 다른 주소를 반복적으로 접근하는 워크로드**에서 문제가 심각해진다.  즉, cache pollution 이 발생하게 되며, 이는 불필요한 데이터가 캐시에 적재되어 실제로 필요한 데이터를 덮어쓰는 현상이다.

miss가 발생했을 때는 해당 데이터를 메인 메모리에서 캐시로 가져와야 하므로, 이때 걸리는 시간인 **miss penalty**가 크게 된다. miss penalty가 크면, 단순히 블록 크기를 늘려서 miss rate를 조금 줄이는 것이 오히려 역효과를 낼 수도 있다. 다시 말해, miss rate를 줄이려는 효과보다 miss penalty로 인한 성능 저하가 더 클 수 있다는 것이다.

---
## **Cahce Misses**

Cache hit이 발생하면, CPU가 접근하려는 메모리에 대한 데이터가 이미 캐시에 존재하므로 **매우 빠르게 접근이 가능하며**, 보통 **1사이클 내에 처리**된다. 이 경우, 지금까지 배운 **정상적인 파이프라인 흐름대로** CPU가 명령어를 수행하게 된다.

반면, cache miss가 발생하면 **miss penalty**가 발생하게 되며, 이로 인해 **CPU 파이프라인에 stall(정지)**이 생긴다. 즉, 캐시에 없는 데이터를 **메인 메모리에서 가져오는 동안** CPU는 기다려야 한다.

CPU에는 일반적으로 **Instruction cache(I-cache)** 와 **Data cache(D-cache)** 가 따로 존재하며, 이 둘은 접근 방식이 다르다.

**Instruction cache**는 프로그램의 흐름에 따라 연속된 명령어들을 가져오는 경우가 많아 **locality가 높다.** 반면, **Data cache**는 조건문, 포인터, 동적 접근 등이 많기 때문에 **locality가 상대적으로 낮다.**

>원래는 명령어와 데이터가 모두 하나의 메모리에 존재하지만, **성능 향상**을 위해 캐시에서는 **분리된 구조로 관리**하는 경우가 많다 (Harvard 구조 개념).

**Instruction cache miss**가 발생하면, **PC의 증가를 멈추고**, 해당 명령어를 **메인 메모리에서 불러온 후**, **IF(Instruction Fetch)** 단계부터 **다시 시작**하면 된다. **Data cache miss**가 발생하면, **MEM 단계에서 stall이 발생**하고, 메모리 접근이 완료되면 그 지점부터 **다시 실행**하게 된다.

---
## **Write Though**

사실 **cache hit**이 발생하더라도, 단순히 빠르게 처리된다고 끝나는 것은 아니다. 특히 **store(쓰기)** 연산일 때는 **더 신중한 처리가 필요하다.**

store는 결국 **메모리에 데이터를 기록(write)** 하는 연산이다. 그런데 store 명령이 **cache에만 데이터를 쓰고 메인 메모리에 반영하지 않으면**, cache와 메모리 간에 **데이터 불일치** 가 생길 수 있다.

이 문제를 해결하기 위해 두 가지 대표적인 정책이 존재한다.

첫 번째 방법은 **Write Through** 방식이다. 즉, 데이터를 **캐시에 쓸 때 동시에 메인 메모리에도 함께 쓰는 방식**이다. 이 방식은 캐시와 메모리 간 **일관성을 유지하기 쉽다**는 장점이 있지만, 단점은 **쓰기 연산이 느려진다는 점**이다. 왜냐하면 **메모리에 접근하는 시간은 캐시에 비해 훨씬 오래 걸리기 때문**이다.

> [!example]
> 예를 들어,**기본 CPI가 1**이고, 전체 명령어의 **10%가 store 명령**이며, **메모리 쓰기 시간이 100 사이클** 걸린다고 가정하면, 다음과 같이 계산된다:
> 
> $$Effective CPI=1+0.1×100=11$$
>
>이처럼 **store 명령어 비율이 작더라도**, 메모리 쓰기 지연 때문에 전체 성능이 크게 떨어질 수 있다.

##### Write Buffer
이 문제를 해결하기 위한 방법이 바로 **Write Buffer**이다.

Write Through 방식에서는 원칙적으로 **메모리에 즉시 쓰기를 해야** 하지만,  **수정된 데이터를 write buffer라는 임시 공간에 먼저 저장**해두고,  CPU는 곧바로 다음 명령어를 실행할 수 있게 만든다.

즉, **CPU는 store 연산을 수행하자마자 다음 명령으로 진행**하고,  **실제 메모리 쓰기는 write buffer가 비동기적으로 처리**하게 된다. 다만, **write buffer가 가득 찼을 경우**에는 CPU가 더 이상 쓰기를 진행할 수 없으므로  **stall이 발생**하게 되며, 그때는 기다려야 한다.

---
## **Write-Back**

**Write hit**이 발생했을 때, 데이터를 **캐시에만 반영하고 메인 메모리에는 반영하지 않는 방식**을 **Write Back**이라 한다.

이 방식에서는 **캐시와 메모리의 내용이 일시적으로 달라질 수 있으므로**, 이를 관리하기 위해 **각 캐시 블록마다 dirty bit을 둔다.**

- **dirty bit = 1**: 해당 캐시 블록이 **수정되었으며**, 아직 메모리에 반영되지 않았음을 의미
    
- **dirty bit = 0**: 해당 블록은 메모리와 **동일한 상태**임을 의미

이후 **dirty 블록이 캐시에서 교체(eviction)** 될 때,  즉 **다른 데이터를 위해 해당 블록을 버려야 할 때**,  **그제서야 메모리에 데이터를 다시 쓰게 된다.** 이때도 메모리에 바로 쓰는 것이 아니라, **쓰기 병목을 줄이기 위해 write buffer를 함께 사용할 수 있다.**


---
## **Write Allocation**

만약 write miss가 발생하면 어떻게 해야 할까? 메모리에 접근하는 시간은 매우 길기 때문에, **메모리에 언제 접근할지**가 중요하다.

write-through 방식을 생각해보면, 캐시에도 데이터를 쓰고 메모리에도 데이터를 쓴다. 그렇다면 과연 데이터를 **굳이 캐시로 가져올 필요가 있을까?**

이와 관련된 두 가지 방식이 있다: **write allocation**과 **write around**이다.


> [!note] Write miss 라면?
> - **write allocation**은 miss가 발생했을 때 해당 블록을 **캐시로 가져온 후** 업데이트를 한다.
>     
> - **write around**는 miss가 발생했을 때 **캐시로 가져오지 않고**, 메모리에만 직접 데이터를 쓴다.

보통 **write-through 방식은 write allocation 또는 write around와 함께 사용**되고,  **write-back 방식은 write allocation과 함께 사용**된다.

write around는 **재사용 가능성이 낮은 경우**, 예를 들어 메모리를 초기화하는 상황에서 **부분적으로 사용된다.** 하지만 자주 사용되지 않고, 대부분 write allocation 방식이 사용된다.

---
## **Example: Intrinsity FastMATH**

예를 들어, **FastMATH**라는 프로세서가 있다고 하자. 이 프로세서는 **12단계의 파이프라인**을 가지고 있으며, instruction과 data 접근을 각각 **다른 캐시**에서 처리한다. 즉, instruction은 **I-cache**, 데이터는 **D-cache**를 통해 접근하게 된다.

> [!note] 캐시 구성
> 
> - **각 캐시의 용량**: 16KB = 2¹⁴ bytes
>     
> - **Block 수**: 2⁸ blocks
>     
> - **Block 당 단어 수**: 2⁶ words/block
>     


D-cache는 **write-through**와 **write-back** 중 하나를 선택해서 사용 가능하다. 반면, **I-cache는 데이터를 쓰지 않고 읽기만** 하기 때문에, 별도로 write 정책을 설정할 필요는 없다.

이제 **SPEC2000 벤치마크**를 사용해서 성능을 측정해 보면, **I-cache의 miss rate은 0.4%**, **D-cache의 miss rate은 11.4%**, 이들을 종합한 **전체 평균 miss rate은 약 3.2%** 로 나타난다.

I-cache는 일반적으로 **공간적 지역성(spatial locality)** 이 높기 때문에 miss rate이 낮은 편이다. 또한 **모든 명령어마다 I-cache에 접근**하지만, D-cache는 **load/store 명령어**에서만 접근하기 때문에, 각 캐시의 접근 비율을 고려하여 평균 miss rate을 계산하면 **3.2% 수준**이 되는 것이다.

![](../images/Pasted%20image%2020250530231218.png)

load/store 을 할 때는 한 word만 사용하기 때문에 내가 원하는 cache 블록이 있어도 그 중 에
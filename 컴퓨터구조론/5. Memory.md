
---
## **Locality**

프로그램은 실행 중에 **전체 주소 공간 중 일부만 집중적으로 접근**하는 경향이 있다. 이로부터 두 가지 중요한 **locality(지역성)** 개념이 도출된다:

##### ✅ Temporal Locality (시간적 지역성)

**최근에 접근한 데이터나 명령어가 곧 다시 접근될 가능성이 높다.**

- 예시: 반복문 안에서 반복적으로 사용되는 변수나 instruction
    
- 의미: 캐시에 최근 사용된 데이터를 유지하면 성능 향상 가능

##### ✅ Spatial Locality (공간적 지역성)

**최근 접근된 메모리 위치의 인접한 위치도 곧 접근될 가능성이 높다.**

- 예시: 연속된 배열 요소, 순차적으로 실행되는 명령어
    
- 의미: 한 번에 블록 단위로 메모리를 가져오면 효율적

----
## **Taking Advantage of Locality**

실제 메모리는 **계층적 구조(hierarchical structure)** 로 이루어져 있다.

가장 아래에는 **디스크**가 있다. **매우 느리지만 용량이 크며**, 모든 데이터를 영구적으로 저장한다.
    
디스크보다 빠르지만 상대적으로 작은 메모리가 바로 **DRAM**, 즉 **메인 메모리(Main Memory)** 다.  프로그램이 실행 중일 때 필요한 데이터 중 **자주 접근하고, 가까운 데이터들**을 이곳에 올려둔다.
    
DRAM보다 더 빠르고, 더 작은 메모리가 바로 **SRAM**, 즉 **캐시 메모리(Cache)** 다.  DRAM에 있는 데이터 중에서도 **가장 자주 사용되고, 최근에 접근된 데이터들**을 캐시에 저장한다.

이러한 메모리 계층은 **속도와 용량의 절충** 구조로 설계되며,  **Locality(지역성) 원리**를 기반으로 하여  
자주 쓰이는 데이터가 **더 빠른 계층에 위치하도록 하여 전체 시스템 성능을 향상**시킨다.

---
## **Memory Hierarchy Levels**

![](../images/Pasted%20image%2020250528000025.png)

각 단계에서 데이터를 가져올 때는 **데이터 하나씩이 아니라 일정 단위로 묶어서 가져오며**, 이 단위를 **block** 또는 **cache line**이라고 부른다.  데이터를 하나만 가져오면 **공간적 지역성(spatial locality)** 을 활용하지 못하므로 손해이기 때문이다. 예를 들어 현대 컴퓨터에서는 보통 **32바이트 또는 64바이트**를 하나의 block으로 사용한다.

프로세스가 데이터를 접근할 때는 **가장 빠른 단계(보통 캐시)부터 확인**하게 되며, 찾고자 하는 데이터가 해당 단계에 **존재하면 이를 "Hit"** 라고 하고, 전체 접근 중에서 Hit이 발생한 비율을 **Hit Ratio**라고 한다. 만약 그 단계에 없다면, 다음 단계(더 느리고 큰 계층)로 내려가야 하며,  이 경우를 **"Miss"** 라고 한다.

데이터를 발견했을 경우에는 **그 데이터만 가져오는 것이 아니라**, **그 데이터가 포함된 block 전체를 상위 계층으로 가져온다.** 이는 다음 번에 인접한 데이터를 사용할 가능성이 높기 때문 (→ 공간적 지역성 활용).

만약 더 밑의 단계까지 내려가서 데이터를 찾게 되면, **그만큼 접근 시간이 더 길어지며**, 이 추가 시간을 **Miss Penalty**라고 한다.

마지막으로 전체 접근 중 **Miss가 발생한 비율은 Miss Ratio**라고 하며,  
이는 단순히 `1 - Hit Ratio`로 계산할 수 있다.

---
## **Memory Technology**

| **Memory Type**        | **Access Time**      | **Cost per GB**         |        |
| ---------------------- | -------------------- | ----------------------- | ------ |
| **Static RAM (SRAM)**  | 0.5ns ~ 5ns          | $400 ~ $800             | 캐시     |
| **Dynamic RAM (DRAM)** | 20ns ~ 50ns          | $2 ~ $4                 | 메인 메모리 |
| **NAND Flash (SSD)**   | 250µs ~ 1000µs       | $0.04 ~ $0.10           | 스토리지   |
| **Magnetic Disk**      | 2ms ~ 20ms           | $0.01 ~ $0.02           | 스토리지   |
| **Ideal Memory**       | SRAM 수준의 Access Time | 디스크 수준의 Capacity 및 Cost |        |
프로그램이 사용하는 주소는 메인 메모리에서 저장된다고 보면 된다.
0.5ns = 1cycle 이라고 생각하면 된다.

---
## **Cache Memory**

CPU와 굉장히 가까이에 있어서 빠른 속도로 접근할 수 있다.

메인 메모리의 경우 특정 주소로 접근하려고 하면 그냥 하면 되지만, cache 메모리에는 접근하려는 주소가 있을 수도 있고 없을 수 있다. 따라서 해당 데이터가 cache 메모리에 있는지 없는지 확인해 봐야 한다.

가장 쉬운 방법은 cache 메모리에 있는 데이터에 어디서 왔는지 주소를 다 적어서 넣어놓는 것이다. 그러나 이것은 굉장히 비효율적이다.

---
## **Direct Mapped Cache**

위의 비효율적인 방법 대신에, **주소마다 저장될 수 있는 칸을 고정하는 방법**이 있다.  즉, **주소가 주어지면 cache 메모리의 특정 위치에만 저장되도록 연결된 공간이 정해지는 방식**이다.

![](../images/Pasted%20image%2020250528001858.png)

자, 위의 예시처럼 메인 메모리에는 32개의 칸이 있고, Cache에는 8개의 칸이 있다고 하자.  
그렇다면 메인 메모리의 각 칸을 구별하기 위해서는 2<sup>5</sup>=32 이므로 **5개의 비트**로 주소를 구분할 수 있고,  
Cache 메모리는 2<sup>3</sup>=8이므로 **3개의 비트**로 구분할 수 있다.

그래서 **메인 메모리의 하위 3비트**를 확인해서 **Cache 메모리의 3비트와 비교**했을 때,  **똑같은 위치에만 메인 메모리의 데이터가 저장될 수 있는 것**이다.  

>즉, **Cache 메모리의 각 칸에는 올 수 있는 메인 메모리 주소가 정해져 있다는 의미**다.

그래서 **하위 3비트는 Cache 메모리의 위치를 결정**하는 데 사용되므로,  해당 위치를 통해 이미 알고 있는 정보이다.  따라서 **Cache 메모리 내부에는 나머지 상위 2비트만 저장**해두면,  이를 통해 **메인 메모리의 전체 주소를 복원하거나 확인**할 수 있게 된다.

---
## **Tags and Valid Bits**

따라서 해당 Cache 메모리 안에 있는 메인 메모리를 나타내는 값들을 **Tag**라고 한다.

만약 컴퓨터를 처음 켰을 때를 생각해보면, **Cache 메모리는 비어 있는 상태**가 된다.  이처럼 Cache 메모리 안에 있는 데이터가 **유효할 수도 있고, 유효하지 않을 수도 있기 때문에**,  이를 확인하기 위한 **Valid bit**가 하나 포함되어 있다.

따라서 주소의 **하위 비트**를 통해 Cache 메모리에서 **위치를 찾고**,  **상위 비트(Tag)** 를 통해 메인 메모리의 해당 주소가 **실제로 맞는지 확인**하며,  **Valid bit**를 통해 그 데이터가 **유효한지 아닌지를 판단**하게 된다.

---
## **Cache Memory**

8개의 block으로 이뤄져있고, block마다 1word가 저장된다. 즉 용량이 32byte인 cache이다 그리고 direct mapped 방식을 사용한다고 가정하자.

##### 1️⃣ 초기값

![](../images/Pasted%20image%2020250528003243.png)
완전 초기값에는 **Tag나 Data 값이 임의의 값으로 채워져 있을 수 있지만**,  **Valid bit가 0**이기 때문에 **이 데이터는 무시되며 적용되지 않는다.**

즉, **Valid bit가 1이 되어야만** 해당 Tag와 Data가 **유효한 값으로 인식**되고,  **Valid bit가 0이면 아무리 Tag와 Data가 일치하더라도 cache miss로 처리**된다.

![](../images/Pasted%20image%2020250528003308.png)

22번 주소가 왔다고 가정하면, 2진수로 표현하면 **10110**이다.

이 중에서 **하위 3비트(110)** 를 확인해 보면, 해당 위치의 **Valid bit가 0**, 즉 **유효하지 않다.**  따라서 이 주소에 해당하는 데이터는 **Cache에 존재하지 않는 것(miss)** 으로 판단하게 된다. 그러면 **메인 메모리에서 원하는 데이터를 가져와서 사용**하게 되며,  **같은 Cache 위치(하위 3비트 = 110)** 에 해당 데이터를 **저장**한다.

이때 상위 비트인 **Tag = 10** (즉, 상위 2비트) **Valid bit는 1로 설정**한다. 즉, 다음부터 해당 위치를 다시 접근할 경우,  **Tag가 일치하고 Valid bit도 1이면 cache hit으로 동작**하게 된다.

![](../images/Pasted%20image%2020250528003620.png)

만약 cache 메모리에 원하는 Tag값이 있고, valid 하다면 메인 메모리까지 가지 않고, cache에서 가져가면 된다!

---

![](../images/Pasted%20image%2020250528003729.png)

Index는 맞지만 Tag가 다르다면 원하는 데이터가 아니므로, 메인 메모리에서 가져온 다음 원래 있던 데이터 대신에 새로 가져온 데이터를 Cache에 넣는다. 원래 저장되어 있던 데이터는 쫓겨나게 되는데 이 과정을 evict이라고 한다. 여기서 만약 쫓겨난 데이터가 변경되었다면 어떻게 처리할지도 나중에 알아보자.


---
## **Address Subdivision**

![](../images/Pasted%20image%2020250530165232.png)

---
## **Example: Larger Block Size**

![](../images/Pasted%20image%2020250530165423.png)

offset 4비트 = 한 블록의 크기가 2<sup>4</sup> byte
Index 6비트 = 블록의 개수가 2<sup>6</sup>

Cache size = block 크기 * block의 개수 =  2<sup>10</sup> byte  = 16KB

만약에 offset 즉 block 의 크기를 늘리면 index 즉 block의 개수는 줄어든다. 

---
## **Block Size Considerations**

한 블록의 크기를 크게 하면, 일반적으로 **공간적 지역성(spatial locality)** 에 따라 **캐시 miss rate를 줄이는 데 도움이 될 수 있다.**  그 이유는, 블록 크기를 키운다는 것은 한 번에 **더 많은 인접 데이터를 함께 캐시에 불러온다는 뜻**이며,  프로세스는 실제로 인접한 메모리 영역을 자주 참조하는 경향이 있기 때문이다.

하지만 캐시 용량이 **고정되어 있다면**, 블록 크기를 키우는 것은 곧 **블록 개수를 줄이는 것**을 의미한다.  예를 들어, 캐시 전체가 1KB이고 블록 크기를 1KB로 설정하면, 캐시에는 **블록 하나만** 저장할 수 있게 된다.  이럴 경우 **인접하지 않은 여러 데이터 블록들이 동일한 캐시 공간을 경쟁하게 되며**,  오히려 캐시 교체가 빈번해지고 miss rate가 높아질 수도 있다.

이런 현상은 특히 **서로 다른 주소를 반복적으로 접근하는 워크로드**에서 문제가 심각해진다.  즉, cache pollution 이 발생하게 되며, 이는 불필요한 데이터가 캐시에 적재되어 실제로 필요한 데이터를 덮어쓰는 현상이다.

miss가 발생했을 때는 해당 데이터를 메인 메모리에서 캐시로 가져와야 하므로, 이때 걸리는 시간인 **miss penalty**가 크게 된다. miss penalty가 크면, 단순히 블록 크기를 늘려서 miss rate를 조금 줄이는 것이 오히려 역효과를 낼 수도 있다. 다시 말해, miss rate를 줄이려는 효과보다 miss penalty로 인한 성능 저하가 더 클 수 있다는 것이다.

---
## **Cahce Misses**

Cache hit이 발생하면, CPU가 접근하려는 메모리에 대한 데이터가 이미 캐시에 존재하므로 **매우 빠르게 접근이 가능하며**, 보통 **1사이클 내에 처리**된다. 이 경우, 지금까지 배운 **정상적인 파이프라인 흐름대로** CPU가 명령어를 수행하게 된다.

반면, cache miss가 발생하면 **miss penalty**가 발생하게 되며, 이로 인해 **CPU 파이프라인에 stall(정지) **이 생긴다. 즉, 캐시에 없는 데이터를 **메인 메모리에서 가져오는 동안** CPU는 기다려야 한다.

CPU에는 일반적으로 **Instruction cache(I-cache)** 와 **Data cache(D-cache)** 가 따로 존재하며, 이 둘은 접근 방식이 다르다.

**Instruction cache**는 프로그램의 흐름에 따라 연속된 명령어들을 가져오는 경우가 많아 **locality가 높다.** 반면, **Data cache**는 조건문, 포인터, 동적 접근 등이 많기 때문에 **locality가 상대적으로 낮다.**

>원래는 명령어와 데이터가 모두 하나의 메모리에 존재하지만, **성능 향상**을 위해 캐시에서는 **분리된 구조로 관리**하는 경우가 많다 (Harvard 구조 개념).

**Instruction cache miss**가 발생하면, **PC의 증가를 멈추고**, 해당 명령어를 **메인 메모리에서 불러온 후**, **IF(Instruction Fetch)** 단계부터 **다시 시작**하면 된다. **Data cache miss**가 발생하면, **MEM 단계에서 stall이 발생**하고, 메모리 접근이 완료되면 그 지점부터 **다시 실행**하게 된다.

---
## **Write Though**

사실 **cache hit**이 발생하더라도, 단순히 빠르게 처리된다고 끝나는 것은 아니다. 특히 **store(쓰기)** 연산일 때는 **더 신중한 처리가 필요하다.**

store는 결국 **메모리에 데이터를 기록(write)** 하는 연산이다. 그런데 store 명령이 **cache에만 데이터를 쓰고 메인 메모리에 반영하지 않으면**, cache와 메모리 간에 **데이터 불일치** 가 생길 수 있다.

이 문제를 해결하기 위해 두 가지 대표적인 정책이 존재한다.

첫 번째 방법은 **Write Through** 방식이다. 즉, 데이터를 **캐시에 쓸 때 동시에 메인 메모리에도 함께 쓰는 방식**이다. 이 방식은 캐시와 메모리 간 **일관성을 유지하기 쉽다**는 장점이 있지만, 단점은 **쓰기 연산이 느려진다는 점**이다. 왜냐하면 **메모리에 접근하는 시간은 캐시에 비해 훨씬 오래 걸리기 때문**이다.

> [!example]
> 예를 들어,**기본 CPI가 1**이고, 전체 명령어의 **10%가 store 명령**이며, **메모리 쓰기 시간이 100 사이클** 걸린다고 가정하면, 다음과 같이 계산된다:
> 
> Effective CPI=1+0.1×100=11
>
>이처럼 **store 명령어 비율이 작더라도**, 메모리 쓰기 지연 때문에 전체 성능이 크게 떨어질 수 있다.

##### Write Buffer
이 문제를 해결하기 위한 방법이 바로 **Write Buffer**이다.

Write Through 방식에서는 원칙적으로 **메모리에 즉시 쓰기를 해야** 하지만,  **수정된 데이터를 write buffer라는 임시 공간에 먼저 저장**해두고,  CPU는 곧바로 다음 명령어를 실행할 수 있게 만든다.

즉, **CPU는 store 연산을 수행하자마자 다음 명령으로 진행**하고,  **실제 메모리 쓰기는 write buffer가 비동기적으로 처리**하게 된다. 다만, **write buffer가 가득 찼을 경우**에는 CPU가 더 이상 쓰기를 진행할 수 없으므로  **stall이 발생**하게 되며, 그때는 기다려야 한다.

---
## **Write-Back**

**Write hit**이 발생했을 때, 데이터를 **캐시에만 반영하고 메인 메모리에는 반영하지 않는 방식**을 **Write Back**이라 한다.

이 방식에서는 **캐시와 메모리의 내용이 일시적으로 달라질 수 있으므로**, 이를 관리하기 위해 **각 캐시 블록마다 dirty bit을 둔다.**

- **dirty bit = 1**: 해당 캐시 블록이 **수정되었으며**, 아직 메모리에 반영되지 않았음을 의미
    
- **dirty bit = 0**: 해당 블록은 메모리와 **동일한 상태**임을 의미

이후 **dirty 블록이 캐시에서 교체(eviction)** 될 때,  즉 **다른 데이터를 위해 해당 블록을 버려야 할 때**,  **그제서야 메모리에 데이터를 다시 쓰게 된다.** 이때도 메모리에 바로 쓰는 것이 아니라, **쓰기 병목을 줄이기 위해 write buffer를 함께 사용할 수 있다.**


---
## **Write Allocation**

만약 write miss가 발생하면 어떻게 해야 할까? 메모리에 접근하는 시간은 매우 길기 때문에, **메모리에 언제 접근할지**가 중요하다.

write-through 방식을 생각해보면, 캐시에도 데이터를 쓰고 메모리에도 데이터를 쓴다. 그렇다면 과연 데이터를 **굳이 캐시로 가져올 필요가 있을까?**

이와 관련된 두 가지 방식이 있다: **write allocation**과 **write around**이다.


> [!note] Write miss 라면?
> - **write allocation**은 miss가 발생했을 때 해당 블록을 **캐시로 가져온 후** 업데이트를 한다.
>     
> - **write around**는 miss가 발생했을 때 **캐시로 가져오지 않고**, 메모리에만 직접 데이터를 쓴다.

보통 **write-through 방식은 write allocation 또는 write around와 함께 사용**되고,  **write-back 방식은 write allocation과 함께 사용**된다.

write around는 **재사용 가능성이 낮은 경우**, 예를 들어 메모리를 초기화하는 상황에서 **부분적으로 사용된다.** 하지만 자주 사용되지 않고, 대부분 write allocation 방식이 사용된다.

---
## **Example: Intrinsity FastMATH**

예를 들어, **FastMATH**라는 프로세서가 있다고 하자. 이 프로세서는 **12단계의 파이프라인**을 가지고 있으며, instruction과 data 접근을 각각 **다른 캐시**에서 처리한다. 즉, instruction은 **I-cache**, 데이터는 **D-cache**를 통해 접근하게 된다.

> [!note] 캐시 구성
> 
> - **각 캐시의 용량**: 16KB = 2¹⁴ bytes
>     
> - **Block 수**: 2⁸ blocks
>     
> - **Block 당 단어 수**: 2⁶ words/block
>     


D-cache는 **write-through**와 **write-back** 중 하나를 선택해서 사용 가능하다. 반면, **I-cache는 데이터를 쓰지 않고 읽기만** 하기 때문에, 별도로 write 정책을 설정할 필요는 없다.

이제 **SPEC2000 벤치마크**를 사용해서 성능을 측정해 보면, **I-cache의 miss rate은 0.4%**, **D-cache의 miss rate은 11.4%**, 이들을 종합한 **전체 평균 miss rate은 약 3.2%** 로 나타난다.

I-cache는 일반적으로 **공간적 지역성(spatial locality)** 이 높기 때문에 miss rate이 낮은 편이다. 또한 **모든 명령어마다 I-cache에 접근**하지만, D-cache는 **load/store 명령어**에서만 접근하기 때문에, 각 캐시의 접근 비율을 고려하여 평균 miss rate을 계산하면 **3.2% 수준**이 되는 것이다.

![](../images/Pasted%20image%2020250530231218.png)

load/store를 수행할 때는 **하나의 word**만 사용하기 때문에, 캐시 블록 내에 원하는 블록이 있어도 **그 중 하나의 word만 선택하여 사용**하게 된다. 즉, 하나의 블록에 여러 word가 있어도, **필요한 건 오직 하나**이므로, 해당 word만 **선택적으로 읽거나 쓴다**는 것이다.

---
## **Measuring Cache Performance**

일반적으로 지금까지는 프로그램의 실행 사이클을 **cache hit이 발생한다고 가정하고** 계산해왔다. 하지만 보다 정확한 성능 분석을 위해서는, **cache miss가 발생했을 때 추가되는 stall cycle**도 고려해야 한다. 이를 반영하여 cache의 전체적인 성능을 계산할 수 있다.

다음 식을 사용하면 된다:

$${Memory stall cycles} = \frac{\text{Memory accesses}}{\text{Program}} \times \text{Miss rate} \times \text{Miss penalty}$$

$$= \frac{\text{Instructions}}{\text{Program}} \times \frac{\text{Misses}}{\text{Instruction}} \times \text{Miss penalty}
$$

> [!note]
> - **Memory accesses**: 주로 `lw`, `sw` 같은 load/store 명령과 instruction fetch를 포함한다.
>     
> - **Miss rate**: 해당 접근들 중 캐시에서 miss가 발생하는 비율
>     
> - **Miss penalty**: miss가 발생했을 때 메모리에서 데이터를 가져오는 데 걸리는 시간

이 값들을 바탕으로 **메모리 접근으로 인한 stall 시간이 전체 실행 시간에 얼마나 영향을 주는지**를 정량적으로 계산할 수 있다.

---
## **Cache Performance Example**

> [!example]
> • I-cache miss rate = 2%
> • D-cache miss rate = 4%
> • Miss penalty = 100 cycles
> • Base CPI (ideal cache) = 2 #100% hit 일 경우
> • Load & stores are 36% of instructions
> 

I-cache는 **모든 instruction fetch**에 영향을 주므로, **전체 instruction**에 대해 miss 여부를 고려해야 한다.

- I-cache miss rate = 0.02
    
- miss penalty = 100 cycles
    
- → instruction 하나당 평균 **0.02 × 100 = 2 cycles** stall 발생
    

D-cache는 **load/store 명령어**에서만 사용되므로,

- 전체 instruction 중 **36%만 D-cache 관련**
    
- 그 중 miss rate = 0.04
    
- miss penalty = 100 cycles
    
- → instruction 하나당 평균 **0.36 × 0.04 × 100 = 1.44 cycles** stall 발생

따라서 전체 **실제 CPI**는 다음과 같다:

$${실제 CPI} = \text{Base CPI (2)} + \text{I-cache stall (2)} + \text{D-cache stall (1.44)} = \mathbf{5.44}$$

이상적인 상황에서는 cache miss 없이 base CPI만큼만 걸리므로,  **이상적인 CPI = 2**

따라서 **이상적인 경우는 실제보다** $\frac{5.44}{2} = {2.72\ }$배 빠르다.

---
## **Average Access Time**

기본적으로 **모든 메모리 접근은 hit time**을 깔고 가며, 여기에 **일정 비율의 miss가 발생했을 때 추가적으로 발생하는 비용(miss penalty)** 를 더하면 **AMAT(Average Memory Access Time)** 을 계산할 수 있다.


$${AMAT} = \text{Hit Time} + \text{Miss Rate} \times \text{Miss Penalty}$$

>**Hit Time**은 항상 발생하는 기본 접근 시간이고, **Miss Rate × Miss Penalty**는 **평균적으로 발생하는 stall 비용**을 의미한다.

ex ) CPU with 1ns clock, hit time = 1 cycle, miss penalty = 20 cycles, I-cache miss rate = 5%
 Instruction 한정 AMAT = 1 + 0.05 * 20 = 2ns

---
## **Performance Summary**

메모리에 직접 접근하는 시간은 **물리적인 한계**로 인해 거의 변하지 않는다. 반면, **CPU의 성능은 계속 향상**되면서 클럭 속도도 빨라지고 있기 때문에, 상대적으로 **메모리에 접근하는 데 걸리는 시간**, 즉 **miss penalty는 점점 커지고 있는 상황**이다.

결국, 예전에는 메모리 접근이 몇 cycle 정도밖에 안 걸렸던 것이, 현대에는 수십에서 수백 cycle을 stall 시키는 문제가 되어버린다. 즉, **CPU는 빨라졌지만, 메모리는 그 속도를 따라가지 못하고 있는 셈**이다.

이로 인해 **cache miss로 인한 stall이 전체 성능 저하의 주요 원인**이 되고 있으며,  따라서 **캐시 성능의 중요성은 점점 더 커지고 있다.**  우리는 이런 상황에서 **cache miss를 줄이고, miss penalty를 완화하려는 다양한 구조적, 정책적 개선**을 고민해야 한다. miss rate을 줄이거나 miss penalty를 줄이거나!

---
## **Associative Caches**

##### Fully associative
Direct mapped 방식은 **찾는 속도가 매우 빠르다**는 장점이 있지만, **각 메모리 주소가 딱 정해진 하나의 캐시 블록에만 들어갈 수 있기 때문에**, 충돌이 자주 발생할 수 있고, 그로 인해 **miss rate가 높아질 수 있다**.

이 문제를 해결하기 위한 방법 중 하나가 바로 **Fully Associative 방식**이다. 이 방식은 **메모리 블록이 캐시의 어느 블록에든 저장될 수 있도록** 허용하는 방식이다. 즉, **공간이 고정되지 않고 유연하게 할당**되므로 충돌로 인한 miss를 줄일 수 있다.

하지만 단점도 있다. 어떤 블록에 저장되어 있는지를 알기 위해 **모든 캐시 엔트리를 검사해야 하기 때문**에, 단순히 순차적으로 검사하면 시간이 오래 걸린다. 이를 해결하기 위해 **각 엔트리에 비교기를 붙여서 병렬로 확인**할 수 있지만, 그렇게 되면 **하드웨어 비용이 매우 증가**하게 된다.

결국 Fully Associative 방식은 **유연성은 높지만, 구현 비용이 크고 복잡도도 높다**는 trade-off를 가진다.

##### N-way set associative
주소가 주어지면, 그 주소가 들어갈 수 있는 캐시 블록이 **N개 존재**하는 방식이 바로 **N-way Set Associative** 캐시이다. 이 방식에서는 전체 캐시를 여러 개의 **set**으로 나누고, **하나의 set 안에 N개의 블록**이 존재한다.

Direct mapped 방식에서는 주소의 **index 부분으로 set을 정하고**, 그 set에는 **블록이 하나뿐**이라 비교할 것도 없이 딱 그 자리만 보면 된다. 반면 N-way 방식에서는 **index로 set을 정한 뒤**, 해당 set 안의 **N개 블록 중에 해당 주소가 있는지**를 비교해야 한다. 즉, **index → set 결정 → set 내 N개 중 비교기 N번 동작** 구조이다.

이렇게 하면 **충돌로 인한 miss를 줄일 수 있고**,  **Fully Associative**에 비해 비교기의 수를 줄여서 **비용도 절감**할 수 있다. 결과적으로 N-way Set Associative는 **속도와 유연성의 절충안**으로,  현대 캐시 설계에서 가장 널리 사용되는 방식이다.

![](../images/Pasted%20image%2020250530234520.png)

---
## **Spectrum of Associativity**

![](../images/Pasted%20image%2020250531013622.png)

---
## **Associativy Example**

##### Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• blue miss : initial / red miss : conflict

![](../images/Pasted%20image%2020250531014022.png)

지금 블록이 4개로 나뉘어져 있으니까, 하위 2비트만 보고 index를 찾고, Tag를 비교한다. 6빼고 모두 하위 2비트가 00이기 때문에 같은 cache 인덱스를 가리키게 되서 처음 빼고는 계속 miss를 발생시키고 있다. 

![](../images/Pasted%20image%2020250531014530.png)

set이 두 개이므로, 하위 1비트만 보고 Cache index 결정한다. Set 안에서 자리는 늘었지만, Cache index 자체는 줄어들었다. 

![](../images/Pasted%20image%2020250531014449.png)

**캐시의 way 수(N-way set associative에서의 N)** 를 늘리면 **충돌로 인한 miss(conflict miss)** 를 줄일 수 있어 **hit rate가 증가**하긴 하지만, 그 **효과는 점점 작아지는 경향**이 있다. 하지만 way 수가 많아질수록**하드웨어 복잡도 증가**, **비용 증가**, **access 시간 증가** 등의 부작용이 생기기 때문에  **무작정 way 수를 늘리는 것은 바람직하지 않다.**

---
## **Set Associative Cache Organization**

![](../images/Pasted%20image%2020250531015039.png)

1024 block
 Direct mapped -> index 10bit
 4 way set associative -> 256 set -> index 8bit

 Tag 비교를 동시에 여러개 할 필요가 있다. 
 
 ---
## **Replacement Policy**

miss가 발생하여 메모리에서 데이터를 가져올 때, **캐시에 어떤 블록을 교체할지 결정**하는 것이 중요하다.
**Direct mapped 캐시**에서는 이미 특정한 위치가 정해져 있으므로 **선택의 여지가 없다**. 무조건 해당 위치의 데이터를 덮어쓴다. 하지만 **Set Associative 캐시**에서는 한 set 안에 **N개의 블록(way)** 이 존재하므로, **그 중 하나를 골라서 교체해야 한다**.  

>이때 어떤 것을 선택하는 게 가장 이상적일까?

가장 이상적인 방법은 **앞으로 가장 오랫동안 사용되지 않을 블록을 교체**하는 것이다. 하지만 **우리는 미래의 접근을 알 수 없기 때문에**, **과거의 접근 기록을 기반으로 예측**하게 된다.

##### ✅ **Least Recently Used (LRU)**

- **가장 오랫동안 사용되지 않은 블록을 교체**하는 방식.
    
- 가정: **최근에 사용되지 않은 데이터는 앞으로도 당분간 사용되지 않을 것이다.**
    
- 구현을 위해서는 각 블록의 **접근 순서를 추적해야 하며**,  특히 way 수가 많아질수록 순서를 **재조정하는 데 부담이 크다.**
    
- 정확한 LRU는 비용이 크기 때문에, **근사 LRU(예: pseudo-LRU)** 가 실제에서는 더 자주 쓰인다.

##### ✅ **Random Replacement**

- 단순히 **무작위로 하나를 선택해서 교체**한다.
    
- 예측 불가능하지만, **간단하고 하드웨어 구현이 매우 쉽다.**
    
- 실제로 **성능도 꽤 괜찮은 경우가 많으며**,  특히 way 수가 많은 경우에는 LRU보다 **비슷하거나 더 나은 성능**을 보이기도 한다.

---
## **Multilevel Caches**

지금까지는 **miss rate를 줄이는 방법**에 대해 살펴보았고,  이번에는 **miss penalty**, 즉 **miss가 발생했을 때 데이터를 메모리에서 가져오는 데 걸리는 시간**을 **줄이는 방법**에 대해 알아보자.

가장 단순하게 생각하면, **메모리 접근 속도를 빠르게 하면 된다**. 하지만 현실적으로는, **메모리는 물리적인 제약**이 있어서 접근 속도를 **크게 향상시키기 어렵다**.

그래서 **다른 관점**에서 접근한다. 바로 **메모리와 CPU 사이의 거리 자체를 줄이는 것**이다.  
거리(즉, 접근 시간)가 줄어들면, 자연스럽게 penalty도 줄어들기 때문이다.

이를 위해 **CPU와 메모리 사이에 또 하나의 Cache를 두는 방법**이 등장했다.  기존에는 L1 Cache에서 miss가 나면 곧바로 메인 메모리로 갔지만,  이제는 그 중간에 **속도는 메모리보다 빠르고, L1보다는 느린 Cache**, 즉  **L2 Cache(Level-2 Cache)**를 추가로 둔다.

즉, miss 발생 시 다음 순서로 접근한다:

```
CPU → L1 Cache → L2 Cache → Main Memory
```

이렇게 하면 **L1에서 miss가 나도 곧바로 느린 메인 메모리에 접근하지 않게 되므로**,  **miss penalty를 효과적으로 줄일 수 있다.** 현대 시스템에서는 이를 더 확장하여  **L3 Cache**까지 두는 경우가 많다.  L1은 hit 되었을 때 time을 줄이는 용도로 사용되고, L2, L3는 miss penalty를 줄이기 위한 용도로 사용된다고 생각하면 된다. 

---
## **Multilevel Cache Example**

```c
• CPU base CPI = 1 //항상 cache hit인 이상적인 CPU, 
• clock rate = 4GHz, -> clock cycle time = 0.25ns
• Miss rate/instruction = 2%
• Main memory access time = 100ns
```

 L1 Miss penalty = 100ns / 0.25ns = 400cycles
 Effective CPI = 1(base CPI) + 0.02(Miss Rate) * 400(Miss penalty) = 9

##### L2 Cache 추가했다면?

```c
• Access time = 5ns 
• Global miss rate to main memory = 0.5% 
// 한 instruction 당 평균적으로 몇 번의 main memory 접근을 필요로 하는지
```

L1에서 전체의 2%가 L2로 오게 될 것이고, 몇 퍼센트인지 모르겠지만, 전체의 0.5%가 메인 메모리에 접근한다는 뜻이다. 즉 L2에서 miss rate은 25%라는 것이다.

L2 Miss rate = 25% 

penalty = 100ns / 5ns  = 20cycles ( L2에서 탐색하는데 드는 비용 ) 
Extra penalty = 400ns ( L2에서 Miss일 때  메인메모리까지 탐색하는데 드는 비용 )

L1 hit = 1cycle
L1 miss - L2 hit = 1cycle + 20cycle
L1 miss - L2 miss = 1cycle + 20cycle + 400cycle

CPI = 1(base CPI) + 0.02(L1 miss rate) * 20 + 0.005(L1 miss  rate * L2 miss rate) * 400 = 3.4

성능 비교해보면 9/3.4 = 2.6배 성능 상승했다!

---
## **Multilevel Cache Considerations**

Primary Cache는 크기를 줄여도 hit time(hit 되었을 때 시간)을 최소화하는데 집중하고, L2-Cache는 크기는 커져도 miss rate은 줄일 수 있도록 하여 메인 메모리의 접근을 피할 수 있도록 한다. 

---
## **Interactions with Advanced CPUS**

사실 CPU는 out of orders로 instruction들을 수행하기 때문에, 메모리에 대한 작업으로 stall이 일어나도 그것이 실제 stall로 일어나는 것이 아니라 다른 작업들을 수행한다. 즉, 메모리에 대한 시간으로 성능에 대로 들어나진 않는다. 그래서 실제로는 분석하기 너무 어렵다.

---
## **Interactions with Software**

![](../images/Pasted%20image%2020250531023023.png)

---
## **Software Optimization via Blocking**

DGEMM에서 교체되기 전에 데이터에 대한 접근을 최대로!!

```c
for (int j = 0; j < n; ++j)
{
	double cij = C[i][j];
	for(int k = 0; k < n; k++)
		cij += A[i][k] * B[k][j];
	C[i][j] = cij;
}
```

![](../images/Pasted%20image%2020250605094931.png)

1행의 값을 넣기 위해서 A의 1행은 계속 수행되고, B는 전체 값에 접근한다. 그런데 matrix의 크기가 cache보다 크면 B 행렬은 cache에 들어가는 값이 많이 때문에 앞에서 사용된 값은 나중에 Cache에 없을 수 있어서 메인 메모리에 접근해서 가져와야 한다. 이때 해결법은 Blocking이다.

---
## **Cache Blocked DGEMM**

```c
#define BLOCKSIZE 32
void do_block (int n, int si, int sj, int sk, double **A, double *B, double *C){
	for (int i = si; i < si+BLOCKSIZE; ++i)
		for (int j = sj; j < sj+BLOCKSIZE; ++j){
			double cij = C[j+i*n]; /* cij = C[i][j] */
			for( int k = sk; k < sk+BLOCKSIZE; k++ )
				cij += A[k+i*n] * B[j+k*n];/* cij+=A[i][k]*B[k][j] */
		C[j+i*n] = cij; /* C[i][j] = cij */
	}
}
void dgemm (int n, double* A, double* B, double* C){
	for ( int sj = 0; sj < n; sj += BLOCKSIZE )
		for ( int si = 0; si < n; si += BLOCKSIZE )
			for ( int sk = 0; sk < n; sk += BLOCKSIZE )
				do_block(n, si, sj, sk, A, B, C);
}
```

---
## **Blocked DGEMM Access Pattern**

![](../images/Pasted%20image%2020250605095632.png)

완성은 나중에 하더라도, 부분적으로 수행하여 효율을 높이자!
사용되는 공간을 줄여서, Cache 안에서 남아있도록!

---
## **Dependability**

신뢰성에 대한 이야기로 메모리에 있는 값이 얼마나 믿을만한가? 에 대해 이야기 해보겠다.

HW 또는 SW 에 의해서 발생하는 어떤 실패를 Fault라고 하며 시스템의 실패로 이어질 수도 있고, 아닐 수도 있다.

![](../images/Pasted%20image%2020250605095938.png)

---
## **Dependability Measures**

**Reliability**는 시스템이 **고장(failure) 없이 동작하는 평균 시간**, 즉 **MTTF(Mean Time To Failure)** 를 의미한다.  **Service interruption**은 고장이 발생했을 때 이를 **복구하는 데 걸리는 평균 시간**, 즉 **MTTR(Mean Time To Repair)**을 의미한다.

이 둘을 합친 값인 **MTBF(Mean Time Between Failures)** 는 다음과 같이 정의된다:

$$\text{MTBF} = \text{MTTF} + \text{MTTR}$$


시스템의 **Availability(가용성)** 는 시스템이 정상적으로 동작하는 비율을 뜻하며, 다음과 같이 계산된다:

$$\text{Availability} = \frac{\text{MTTF}}{\text{MTTF} + \text{MTTR}}$$


> [!example]
> - MTTF = 1시간 (즉, 1시간마다 고장 발생)  
> - MTTR = 30분 (복구에 30분 소요)  
> 
> - $\text{Availability} = \frac{1}{1 + 0.5} = \frac{1}{1.5} \approx 0.667$
> - 즉, 약 **66.7%의 시간 동안 시스템이 정상 동작**한다.

Availability를 향상시키기 위해서는 **MTTF를 늘려야** 한다, 즉 고장이 덜 발생하도록 설계한다.  두번째 방법으로는 **MTTR을 줄여야** 한다, 즉 고장이 나도 빠르게 복구할 수 있도록 설계하면 된다!

---
## **The Hamming SEC Code**

Hamming SEC Code는 failure가 덜 발생하도록 하는 방법 중 하나이다. 메모리는 완벽하지 않아서 시간이 지남에 따라 바뀌기도 한다. 이럴 때마다 복구하게 되면 cost 손해가 크기 때문에 1~2 바뀐 건 그냥 넘어갈 수 있으면 좋겠다는 목표를 가진다.

SEC code를 알기 전에 알아야 할 개념이 **Hamming distance**인데, 이는 **두 개의 비트 패턴이 서로 얼마나 다른지를 나타내는 값**이다.

예를 들어, 1110과 0000은 비트가 3개 다르므로 Hamming distance는 3이고, 0000과 1111은 Hamming distance가 4이다.

> 1 bit를 저장하고 싶다고 가정하자.

만약 1비트를 저장하고 싶어서 단순히 1비트로 저장한다면, 값은 0 또는 1이 될 것이다. 하지만 이 경우, 원래 값이 0이었는지, 저장 중에 값이 바뀌어서 0이 된 것인지 알 수 있는 방법이 없다. 즉, 이 값이 올바른지 아닌지를 **확인할 수 없다**.

>이제 2비트를 사용해서 저장한다면 어떻게 될까?

예를 들어 1을 저장하고 싶다면 `11`, 0을 저장하고 싶다면 `00`으로 2비트로 구성한다고 하자. 그런데 만약 나중에 `10`이라는 값을 읽게 되면, 이는 사전에 정의되지 않은 값이므로 **존재할 수 없는 수**이다. 따라서 오류가 발생했다는 것을 **판단하고 에러를 탐지(detection)**할 수 있다.

>그다음 3비트를 사용한다면 어떻게 될까?

마찬가지로 1을 저장하고 싶다면 `111`, 0을 저장하고 싶다면 `000`으로 구성할 수 있다. 이때 두 값의 Hamming distance는 3이 된다. 만약 `101`이라는 값이 나오면, 이는 정의된 값은 아니지만, 1이 2개, 0이 1개이므로 높은 확률로 원래 값은 `111`이었고, 중간 비트 하나가 0으로 잘못 바뀐 것이라고 **추정**할 수 있다. 즉, **단일 비트 오류(single error)** 가 발생했을 때 **오류를 탐지(detection)** 할 수 있을 뿐만 아니라, **정정(correction)** 도 가능하다.

따라서 정리하면 다음과 같다:

- **Minimum distance = 2**: 단일 비트 오류 **탐지** 가능

- **Minimum distance = 3**: 단일 비트 오류 **정정**, 2비트 오류 **탐지** 가능

> [!note] Parity bit
> **Parity bit**는 전체 1의 개수가 **짝수(even parity)**가 되도록 추가하는 비트이다.
> 
> 예를 들어, 다음과 같이 2비트 데이터에 parity bit를 추가한다고 하자
> 
> 00<u>0</u>  01<u>1</u> 00<u>0</u> 10<u>1</u> 11<u>0</u>  
> 
> 이렇게 구성하면 전체 데이터들 간의 **최소 Hamming distance가 2**가 된다. 즉, **단일 비트 오류는 탐지할 수 있지만 정정은 불가능**하다.
> 
> 
> 
> 이제 여기에 **원래 비트(데이터 부분)**를 한 번 더 **반복**해서 붙이면 다음과 같이 된다.
> 
> 00<u>0</u>00  01<u>1</u>01 00<u>0</u>00 10<u>1</u>10 11<u>0</u>11  
> 
> 이렇게 하면 **최소 Hamming distance가 3**이 되어, **단일 비트 오류에 대해 탐지와 정정 모두 가능**해진다.
> 
> 즉, **Parity bit + 원 데이터 반복** 구조는 간단한 방법으로 **SEC(Single Error Correction)**을 구현할 수 있게 해준다.

---
## **Encoding SEC**

최소한의 Parity bit로 SEC을 할 수 있는 저장 체계를 만드는 인코딩하는 방법이다. 

2<sup>n</sup> 자리에 parity bit를 채워 넣으면 된다.

##### 8 bit 데이터를 4 bit party bit를 추가해서 저장

![](../images/Pasted%20image%2020250605103529.png)

12비트(그중 4비트는 parity bit)를 사용하면 총 2⁸개의 데이터를 구분할 수 있다. 이때 전체 코드의 **최소 Hamming distance는 3**이 된다. 즉, **단일 비트 오류에 대해서는 탐지와 정정이 가능**하다.

예를 들어, `d2`의 값이 원래는 1인데 **실수로 0으로 저장되었다고 하자**. 이 경우, 이 오류는 **`P1`과 `P4` parity bit가 마킹하는 영역에 포함**되기 때문에, 이 두 parity bit에서 오류가 감지된다. 이를 통해 **어느 위치의 데이터가 잘못되었는지 식별**할 수 있고, **해당 비트를 반전시켜 복구(correction)** 할 수 있다.

이것이 바로 **Hamming SEC(Single Error Correction) Code**의 핵심 원리이다.

---
## **Decoding SEC**

**Parity bits의 값은 어떤 비트에서 오류가 발생했는지를 알려준다.**  이때 **인코딩 시 사용한 비트 번호 체계**를 그대로 사용해야 한다.

- `Parity bits = 0000` → **오류 없음**
    
- `Parity bits = 1010` → **이진수 1010 = 10번 비트에 오류 발생**
    

이 값은 `P2`와 `P8` parity bit의 값이 **틀리고,** `P1`과 `P4`는 **정상**임임을 의미한다. 즉, P2와 P8이 모두 커버하는 비트들 중에서 **공통으로 포함된 비트 위치를 찾아서** **bit 10**이 문제라는 것을 확인할 수 있다.

각 parity bit는 특정 위치의 비트들을 **마스킹(검사)**한다. 오류 발생 시, **틀린 parity bit들을 XOR**하면 **오류 위치를 정확히 역추적**할 수 있다. 이 오류 위치 번호는 **1부터 시작하는 비트 위치**이며, 실제 코드 배열에서 해당 위치의 비트를 반전시키면 오류가 복구된다.

----
## **SEC/DED Code

전체 비트에 대해 **추가적인 parity bit(전체를 덮는 even parity)** 를 하나 더 추가할 수 있다.  이렇게 하면 **Hamming distance는 4**가 되어, **2비트 오류까지 탐지 가능**하고,  심지어 **parity bit 자체가 오류인지 여부도 구분할 수 있게** 된다.

Decoding 할 때는 H를 SEC parity bits이라고 하면 만약 H가 짝수고, P_n도 짝수이면 문제가 없다. H가 홀 수이고, P_n이 odd 라면 correctable single bit err

> 📌 **그렇다면 parity bit은 최소 몇 개가 필요할까?**

Single Error Correction(SEC)을 위해,  
데이터 비트가 **M개**, parity 비트가 **K개**일 때, 다음 부등식을 만족해야 한다:

$$M + K \leq 2^K - 1$$

이 부등식은 K개의 parity 비트로 표현할 수 있는 **오류 위치의 총 수$(2^K$)** 에서,  한 개는 "오류 없음"을 위해 사용하므로, 나머지 $2^K - 1개$의 비트 위치를 **모두 커버할 수 있어야 함**을 의미한다.



지금까지 다뤘던 외부 단편화(external fragmentation)를 줄이기 위한 방법인 **페이징(Paging)** 은, 메인 메모리에 프로세스가 원하는 데이터가 이미 존재한다는 가정을 바탕으로 설명되었다. 이제부터 다룰 **가상 메모리(Virtual Memory)** 는, 메인 메모리에 원하는 데이터가 존재하지 않을 수도 있다는 상황까지 포함하는 개념이다. 즉, 가상 메모리는 실제 메모리에 전부 올리지 않고도 프로그램을 실행할 수 있게 해주는 확장된 메모리 관리 기법이다.

---
## **Motivation of Virtual Memory**

프로그램이 실행되는 매 순간을 살펴보면, 실제로 그들이 사용하는 코드나 데이터 전체가 항상 필요한 것은 아니다. 실행 상황에 따라 사용되는 코드가 달라지고, 전체 코드가 모두 사용되는 경우는 드물다. 예를 들어, 오류 처리 코드(error code)나 특이한 경로의 루틴(unusual routines) 등은 프로그램 실행 중에 한 번도 사용되지 않을 수 있다.

페이징을 통해 외부 단편화를 줄일 수는 있었지만, 페이지 단위로 메모리에 적재할 때 굳이 사용되지도 않을 데이터를 모두 불러올 필요는 없지 않을까?라는 의문이 생겼고, 결국 필요한 부분만 메모리에 가져오기 위한 방법이 고민되었다.

그 해결책이 바로 **프로그램을 부분적으로 나누어 필요한 부분만 가져오자!** 는 접근이다. 이렇게 하면 프로그램은 더 이상 물리적 메모리 크기에 제약을 받지 않게 된다. 예를 들어, 8GB짜리 게임이 있다고 해도, 실제 실행 시 필요한 부분만 메모리에 불러온다면, 메인 메모리 크기가 4GB여도 실행이 가능해지는 것이다.

또한 각 프로그램이 실행 중에 사용하는 메모리의 양이 줄어들게 되므로, 전체적으로 더 많은 프로세스를 동시에 메모리에 적재할 수 있게 된다. 이로 인해 **CPU utilization**과 **throughput**도 자연스럽게 향상된다.

뿐만 아니라, 메모리에 실제로 데이터를 로드하거나 교체(swap)하는 횟수가 줄어들게 되므로, 불필요한 I/O가 감소하고 결과적으로 프로그램의 실행 속도도 빨라진다. 이러한 개념과 기법이 바로 **가상 메모리(Virtual Memory)** 이다.

---
## **Virtual Memory**

**Logical 메모리와 Physical 메모리를 분리**하면, **논리 주소 공간(logical address space)** 은 실제 물리 주소 공간(physical address space)보다 훨씬 커질 수 있다. 그 이유는, 어차피 실행 시점에 **실제로 필요한 데이터만 메모리에 가져오면 되기 때문**이다.

> [!note] 예시
> 한 프로그램이 8GB라는 Logical한 데이터 크기를 가지고 있지만, 애초에 그 데이터 중 일부만 physical memory에서 사용하기 때문에 훨씬 커져도 상관없다고 이해하면 될 듯 하다.

즉, 모든 데이터를 한꺼번에 물리 메모리에 올릴 필요가 없으므로, 전체 논리 주소 공간이 물리 메모리의 크기를 넘어도 문제가 되지 않는다. 이러한 분리를 통해 **프로세스는 매우 큰 주소 공간을 할당받는 것처럼 동작할 수 있으며**, 이는 가상 메모리 덕분에 가능하다. 결과적으로 주소 공간의 제약이 사라지고, 더 유연하고 효율적인 메모리 관리가 가능해진다.

**Virtual address space**는 "프로세스가 메모리에 어떻게 저장되는가"에 대한 **논리적인 관점(logical view)** 을 제공하며, 이는 어떤 프로세스든지 동일한 방식으로 사용된다.

> [!note]
> - 가상 주소 공간은 보통 **주소 0부터 시작하여**, **연속적인 주소들**로 끝까지 이어지는 형식을 갖는다.
>     
> - 반면에 실제 물리 메모리는 **페이지 프레임(page frame)** 단위로 나뉘어 저장된다.
>     
> - 따라서 **MMU(Memory Management Unit)**는 논리 주소(logical address)를 물리 주소(physical address)로 적절히 **매핑(mapping)**해 주어야 한다.

Virtual 메모리는 **Demand paging, Demand Segmentation**으로 구현할 수 있다.

![](../images/Pasted%20image%2020250531180248.png)

현재 사용되는 데이터만 올라간다. 유효하긴 하지만 현재 사용되지 않는 애들은 메모리에 있을 수도 있고, 디스크에 있을 수도 있다. 

---

![](../images/Pasted%20image%2020250531181502.png)

> **가상 주소 공간은 0번 주소부터 시작해서 최대 주소까지 연속된 메모리 공간처럼 보인다.**

사용자가 보기엔 연속된 메모리이지만, 실제 물리 메모리는 전혀 그렇지 않고, 이공간에는 코드, 데이터, 힙, 스택 등이 배치된다.

> **스택과 힙 사이에는 사용되지 않는 (reserved or guard) 주소 공간이 존재할 수 있다.**

힙은 위로, 스택은 아래로 성장하기 때문에 둘 사이에 **비워진 가상 주소 공간이 존재**한다.  
이 영역은 아직 물리 메모리로 매핑되지 않았지만,  **힙 또는 스택이 확장될 경우를 대비해 예약된 공간**으로 볼 수 있다.

> **시스템 라이브러리(shared libraries)는 각 프로세스의 가상 주소 공간에 매핑된다.**

물리 메모리에서는 같은 페이지를 여러 프로세스가 공유하므로 공간 절약을 할 수 있다. 또 매핑만 다르고 물리적 내용은 동일하다. 

> **공유 메모리는 읽기-쓰기 권한을 가진 페이지를 여러 프로세스의 가상 주소 공간에 매핑함으로써 구현된다.**

예: `mmap`, `shmget`, `shmat` 등을 사용한 IPC. 실제로는 하나의 물리 페이지가 여러 가상 주소로 접근되는 구조.

> **fork() 시 부모 프로세스의 메모리 페이지를 자식과 공유(Copy-on-Write 방식)하면 프로세스 생성이 빨라진다.**


![](../images/Pasted%20image%2020250531181639.png)

shared library같은 경우 stack heap 사이 어딘가가 있을 것이고, 그냥 shared pages 에서 잘 맵핑해주면 된다. 

---
## **Demand Paging**

**Demand paging**은 실행 시점에 **필요한 데이터만 메모리에 적재**하는 방식이다. 불필요한 데이터는 메모리에 올라오지 않기 때문에, **불필요한 I/O가 줄고**, **메모리 사용량도 절약**된다.

과거에는 컨텍스트 스위치 시 프로세스 전체 메모리를 모두 불러와야 했기 때문에 시간이 오래 걸렸지만, Demand paging은 실행에 필요한 페이지만 메모리에 적재하므로 **프로세스 생성 및 전환 속도**가 빨라진다.

이 방식은 **paging system과 swapping 개념과 유사**하지만, 중요한 차이가 있다.  기존의 **swapping**은 프로세스 전체를 통째로 디스크와 메모리 사이에서 옮겼던 반면,  Demand paging은 **페이지 단위로 데이터를 디스크에서 가져오거나 내보낼 수 있다.**

어떤 페이지가 필요한지는 **실제로 접근하는 순간에 확인된다.**  해당 페이지가 메모리에 없으면 운영체제가 **page fault**를 발생시키고, 그 접근이 올바른 경우라면 디스크에서 해당 페이지를 불러온다. 접근 자체가 잘못되었다면, 해당 프로세스는 **abort(비정상 종료)** 된다.
    
이처럼, **필요할 때만 페이지를 메모리에 적재하는 방식**은 **Lazy Swapper** 또는 **Demand Paging**이라고 하며, 시스템의 **메모리 효율성과 응답성**을 크게 향상시킨다.

![](../images/Pasted%20image%2020250531182518.png)

기존의 MMU(Memory Management Unit)는 단순히 **논리 주소를 물리 주소로 변환하는 역할**만 수행했다.  하지만 Demand Paging이 도입되면서, 이제 MMU는 **주소 변환뿐만 아니라 해당 페이지가 메모리에 존재하는지도 확인**해야 한다.

만약 요청한 페이지가 이미 메모리에 존재한다면, 기존 방식과 동일하게 정상적으로 접근하면 된다.  하지만 **해당 페이지가 아직 메모리에 존재하지 않는다면**, MMU는 **page fault 예외를 발생시키고**,  운영체제가 디스크에서 해당 페이지를 읽어와 메모리에 적재한 뒤, 다시 접근을 시도하게 된다.

이 모든 과정은 **프로그램의 동작 흐름에 영향을 주지 않고**, **자동으로 투명하게 처리되어야 한다.**  즉, 프로그래머는 이 과정에 대해 신경 쓰지 않아도 되며, **프로그램 코드 역시 변경할 필요가 없다.**

---
## **Valid-Invalid Bit**

MMU는 주소를 변환하는 과정에서 **해당 페이지가 실제 메모리에 존재하는지도 확인**해야 한다. 이때 사용할 수 있는 것이 바로 **페이지 테이블의 Valid/Invalid 비트**이다.

처음에는 대부분의 페이지 엔트리가 **Invalid(i)** 상태로 표시되어 있으며, 이는 해당 페이지가 아직 메모리에 적재되지 않았음을 의미한다.

만약 변환된 주소가 **Invalid로 표시된 페이지**를 참조하면, 운영체제는 **디스크에서 해당 페이지를 메모리로 로딩**해야 한다. 이 과정은 매우 느리다. 왜냐하면 **디스크 접근 속도는 메모리보다 훨씬 느리기 때문**이다.

아무리 하나의 페이지를 불러오는 작업이라 하더라도, 수천 배 이상 느린 디스크 I/O가 발생하게 되며, 이 때 발생하는 예외 상황을 **Page Fault**라고 한다.

![](../images/Pasted%20image%2020250531183820.png)

---
## **Page Fault**

프로세스가 어떤 페이지에 **처음 접근**하려고 할 때, 해당 페이지가 아직 메모리에 존재하지 않는다면, **trap이 발생하고 이것이 바로 Page Fault이다.**

Page Fault가 발생하면 운영체제는 **우선 해당 접근이 유효한 주소 범위인지 검사**한다. 만약 **접근이 허용되지 않은 주소라면**, 프로세스는 **abort(비정상 종료)** 된다. 반대로, **정상적인 접근이지만 그 페이지가 메모리에 없는 상태**라면(즉, 페이지 테이블의 해당 엔트리가 invalid 상태), 다음 절차를 따른다.
    
1. **Free frame(빈 프레임)** 을 찾아본다.
    
    - 만약 프레임이 없다면, **기존 페이지 중 하나를 선택해 교체(eviction)** 해야 한다.
        
    - 교체 대상이 수정된 페이지(dirty)라면 디스크에 쓰는 작업까지 함께 이뤄진다.
        
2. 디스크에서 필요한 페이지를 읽어와, **선택된 프레임에 적재(swap-in)** 한다.  이 과정은 **디스크 I/O를 동반**하기 때문에 비교적 시간이 오래 걸린다.
    
3. 페이지가 메모리에 올라오면, **페이지 테이블을 업데이트**하여  해당 페이지의 Valid/Invalid 비트를 **Valid**로 바꾼다.
    
4. 마지막으로, **Page Fault를 유발한 명령어는 아직 실행되지 않았기 때문에**,  해당 명령어를 **다시 실행(restart)** 하여 정상적으로 수행을 마친다.

![](../images/Pasted%20image%2020250531184915.png)

---
## **Characteristics of Demand Paging**

**Demand Paging**은 프로세스가 시작할 때 **메모리에 어떤 페이지도 존재하지 않는다고 가정**하는 방식이다. 따라서 운영체제가 **프로세스의 첫 번째 명령어를 수행하려고 instruction pointer를 설정**하는 순간,  해당 명령어가 메모리에 없기 때문에 **즉시 Page Fault가 발생**한다.  

>결국, **모든 페이지에 대한 첫 접근 시에는 반드시 Page Fault가 발생**한다.

예를 들어, 메모리에서 **두 개의 숫자를 더하고 그 결과를 다시 메모리에 저장**하는 명령어를 실행한다고 하자.  이 명령어가 여러 개의 페이지를 참조하고, 각 페이지가 아직 메모리에 없는 상태라면,  **하나의 명령어 수행을 위해 여러 번의 Page Fault가 발생할 수도 있다.**

하지만 실제로는 이렇게 많은 Page Fault가 연달아 발생하는 경우는 드물다.  그 이유는 바로 **공간적 지역성(spatial locality)** 때문이다.  즉, 한 페이지를 참조하면 그 주변의 데이터도 함께 사용하는 경우가 많아,  **한 번 적재된 페이지가 연속된 명령어 수행에 반복적으로 활용**되기 때문이다.

##### Demand Paging을 지원하는 주요 메커니즘

> [!note] Demand Paging을 지원하는 주요 메커니즘
> 
> - **Valid/Invalid Bit**  
>     → 페이지가 메모리에 존재하는지 확인하는 플래그.
>
> - **Backing Store (Secondary Storage / Swap Device)**  
>     → 메모리에 없는 페이지를 디스크에서 가져올 수 있는 저장 공간.
>     
> - **Instruction Restart**  
>     → Page Fault 발생으로 중단된 명령어를, 페이지 적재 후 **다시 처음부터 재실행**할 수 있는 기능.

---
## **Instruction Restart**

**Page fault**는 보통 **명령어가 실행되는 도중에 발생**한다.  이 경우 해당 명령어는 완전히 수행되지 않았기 때문에, **프로세스는 그 상태에서 계속 진행될 수 없다.**

운영체제는 페이지를 디스크에서 메모리로 적재한 뒤,  **중단된 명령어를 정확히 처음부터 다시 실행해야 한다.**  이 과정을 **Instruction Restart**라고 한다.

중요한 점은, **프로세스는 자신이 Page Fault를 경험했는지 전혀 알지 못해야 하며**,  코드 또한 그로 인해 변경되거나 영향을 받아서는 안 된다.

예를 들어 `increment`나 `decrement` 명령처럼 **레지스터나 메모리를 수정하는 연산**은  명령이 **중간까지 수행되고 멈춘 경우** 문제가 될 수 있다.  따라서 하드웨어는 이러한 상황을 감안하여, **해당 명령이 완전히 끝나기 전에는 아무런 부작용(side-effect)이 남지 않도록 설계**해야 한다.

즉, **하드웨어는 instruction의 재시작을 지원할 수 있어야 하며**,  PageFault가 발생해도 **정확하게 이전 상태로 되돌아가 명령을 처음부터 다시 수행**할 수 있어야 한다.

---
## **TLB fault VS Page Fault**

많은 사람들이 **TLB Fault(TLB Miss)** 와 **Page Fault** 를 혼동한다.  하지만 이 둘은 **역할과 의미가 완전히 다르다.**

**TLB Fault** 는 단순히 “최근에 이 페이지에 접근한 적이 있는가?”를 확인하는 과정이다.  TLB는 **최근 접근한 페이지의 주소 변환 정보를 캐싱**해둔 구조이며,  해당 페이지가 TLB에 없다면 **TLB miss (TLB fault)** 가 발생한다.
    
TLB miss가 발생하면, 그제서야 **운영체제나 하드웨어가 페이지 테이블을 조회하여**  해당 페이지가 **실제로 메모리에 존재하는지** 확인한다.
    
이때 페이지 테이블에 해당 페이지가 **Valid** 하다면 **Page Hit**,  페이지가 **Invalid** 하다면 **Page Fault** 가 발생하게 된다.
    
> ✅ **Page Fault** 는 **실제로 메모리에 페이지가 없는 상태**를 의미하고,  
> ✅ **TLB Fault** 는 **주소 변환 캐시에 정보가 없는 상태**를 의미한다.

---
## **Stages in Demand Paging**

Page fault가 발생하면, 먼저 **운영체제에 trap이 발생**한다. 이때부터 **page fault handler**가 동작하여 다음과 같은 절차를 수행한다.

1. **현재 프로세스의 레지스터와 상태(process state)를 저장**한다.  이후에 이 프로세스를 다시 재개할 수 있도록 하기 위함이다.
    
2. **해당 접근이 유효한 가상 주소 범위 내에 있는지 확인**한다.
    
    - 유효하지 않은 접근이라면, 잘못된 메모리 접근이므로 프로세스를 abort시킨다.
        
    - 유효한 접근이라면, 해당 페이지는 아직 메모리에 없고 디스크에 있다는 뜻이므로, **디스크 상에서 해당 페이지의 위치를 확인**한다.
        
3. 디스크로부터 데이터를 읽어올 **free frame**을 찾는다. 만약 사용 가능한 프레임이 없다면, 페이지 교체 알고리즘을 통해 기존 페이지 중 하나를 제거해야 한다.
        
4. 디스크에서 페이지를 읽어와 메모리의 free frame에 적재한다.  이 작업은 디스크 I/O를 수반하기 때문에 시간이 오래 걸린다.  **그 동안 CPU는 다른 프로세스에게 할당된다.**
    
5. 디스크 I/O가 완료되면 **interrupt가 발생**하고, 운영체제는 **Interrupt Service Routine (ISR)** 을 실행한다. ISR 역시 CPU에서 실행되기 때문에, 현재 수행 중이던 프로세스의 레지스터와 상태를 저장한다.
    
6. 이번 interrupt가 **앞서 요청한 페이지 적재 작업이 완료된 것인지 확인**한다.  맞다면, 이제 해당 페이지가 메모리에 존재하므로, **페이지 테이블을 업데이트**하고  관련된 캐시나 TLB 등도 필요 시 갱신한다.
    
7. 이후, Page Fault를 야기했던 프로세스를 **ready queue에 다시 등록**한다.  이제 이 프로세스는 실행할 준비가 된 것이다.
    
8. 나중에 스케줄러가 이 프로세스를 다시 선택하면,  **저장해 두었던 레지스터와 프로세스 상태를 복원**하고, **Page Fault가 발생했던 instruction부터 다시 실행을 시작**한다.

---
## **Performance of Demand Paging**

Demand Paging에서는 처음부터 필요한 모든 데이터를 메모리에 올리지 않기 때문에,  실행 도중 필요한 페이지를 참조할 때마다 **Page Fault**가 발생할 수 있다.  이로 인해 프로그램 실행 중간 중간 **심각한 성능 저하**가 생길 수 있다.

Page Fault가 발생하면 운영체제는 여러 작업을 수행하게 되며, 그 중에서도 핵심적인 세 가지는 다음과 같다:

1. **Service the interrupt**  
    → 인터럽트를 처리하고 관련 context 저장 및 복원 등 기본적인 오버헤드를 수반함  
    → 수백 개의 명령어 수준으로 상대적으로 가벼움
    
2. **Read the page from disk / Write the victim page**  
    → 디스크로부터 필요한 페이지를 읽어오고, 교체 대상 페이지(victim page)가 있다면 디스크에 기록  
    → **가장 시간이 오래 걸리는 단계** (디스크 I/O는 메모리 접근보다 수천 배 느림)
    
3. **Restart the process**  
    → 저장해 둔 레지스터와 상태를 복원하고, fault가 발생한 명령어부터 다시 실행  
    → 짧은 시간 소요

#####  Page Fault Rate (P)

- **Page Fault Rate (P)** 는 **메모리 접근 1회당 page fault가 발생할 확률**을 의미한다.
    
- `P = 0` → 모든 메모리 접근이 메모리에 있는 경우 (최적의 상황)
    
- `P = 1` → 모든 메모리 접근이 page fault를 야기하는 경우 (최악의 상황)

##### Effective Access Time (EAT) 공식

```
EAT = (1 - P) × memory access time + P × (page fault overhead + swap out time + swap in time)
```

- **(1 - P)**: 메모리에 이미 존재하는 경우 → 정상적인 접근 시간
    
- **P**: Page Fault 발생 확률
    
- **page fault overhead**: 인터럽트 처리, 페이지 테이블 검사 등
    
- **swap out/in time**: 디스크 I/O (가장 큰 시간 소모)


---

> [!example] 예시
> Memory Access Time = 100ns
> Average page-fault service time = 8ms(page fault overhead + swap in/out time)


EAT=(1-p) * 100ns +p * 8000000ns   = 100 + p *7999900

이 경우에 만약에 p = 0.001 이라면 EAT은 8.1us가 된다. 그니까 1000번 중 한 번 page fault 가 발생하면 원래 memory access time인 100ns 보다 80배 증가한다는 것이다. 

하지만 사용자들은 이렇게 길어지는 것을 원하지 않고, 예를 들어 EAT < 110 ns를 원한다면

110 > 100 + p * 7999900
10 > p * 7999900
p<0.00000125

즉 page fault는 800000 번 중에 한 번 일어나야한다.

> page fault는 생각보다 성능에 미치는 영향이 크기 때문에 훨씬 작은 수를 가져야 한다

---
# **Page Selection**

위에서는 page fault에 대해서 알아보았다. 하지만 page fault가 안 일어나는 게 제일 좋기 때문에, Page를 잘 골라 와야 한다. page selection 전략들은 여러 개가 있다.

**Demand paging**  
기존에 알고 있던 방식으로, 처음 시작할 때는 필요한 페이지가 없다는 것을 가정하고, page fault가 발생하면 그제서야 page를 load해오고, 메모리 안에 불러올 때까지 기다린다. 대부분의 paging system에서는 이 방법을 사용한다.

**Prepaging**  
사용되기 전에 미리 메모리에 페이지를 가져다 놓는 방식이다.  만약 어떤 페이지가 reference 될 때 그 옆에 것도 같이 가져오면 어떨까?  하지만 미래를 정확히 예측할 수는 없기 때문에, 효과적으로 사용되기 어렵다.  따라서 일반적으로는 잘 사용되지 않으며, **연속적으로 다음 데이터를 읽는 형태의 프로세스**라면 적용할 수 있다.

**Request paging**  
다음에 사용될 페이지를 예측하기 어렵다면, 차라리 **다음에 사용할 페이지를 미리 사용자에게 알려달라고 요청하는 방식**이다.  이론적으로 괜찮은 방법일 수 있으나, 모든 책임을 사용자에게 넘기는 구조이기 때문에 한계가 있다.  사용자가 언제 어떤 페이지가 필요한지 명확히 알지 못하면 사용이 어렵고,  필요한 데이터를 과도하게 가져오는 경우 다른 프로세스가 피해를 볼 수 있으며,  결과적으로 전체 시스템의 효율이 떨어질 수 있다.

---

`fork()`를 통해 부모 프로세스를 자식 프로세스로 복제할 때, 자식은 부모와 **같은 데이터를 복제**한다. 여기에는 코드나 데이터가 포함되지만, 실제로 데이터를 복제하지 않고 **부모의 데이터를 가리키기만 한다**.

>이 방식을 **Copy on Write (COW)** 라고 한다.

COW는 부모와 자식 프로세스가 **초기에는 동일한 페이지를 공유**하는 방식이다. 두 프로세스는 같은 메모리 페이지를 가리키며, 만약 둘 중 하나가 공유된 페이지에 **수정을 시도하면**, 그때 해당 페이지가 복제되어 각각의 프로세스가 독립적으로 데이터를 수정할 수 있게 된다.

일반적으로 **free pages는 0으로 초기화된 "zero-fill-on-demand" 페이지 풀(pool)** 에서 할당된다.  이 풀은 **빠른 demand paging 수행을 위해 항상 일정량의 free frame을 유지**해야 한다.

>그렇다면, 왜 free page를 할당하기 전에 0으로 채워야 할까?  

그 이유는 **이전에 사용되던 데이터가 여전히 메모리에 남아 있을 수 있기 때문**이다.  즉, 다른 프로세스가 사용하던 민감한 데이터가 남아 있을 경우,  새로운 프로세스가 해당 페이지를 그대로 참조하면 **보안상의 문제가 발생할 수 있다.**  따라서 free page는 반드시 0으로 초기화하여 **이전 프로세스의 흔적을 제거**하고 **안전한 환경을 보장**해야 한다.

한편, `vfork()`는 일반적인 `fork()`와는 다르게,  **부모 프로세스를 일시적으로 멈추고 자식 프로세스가 필요한 작업을 먼저 수행하도록 하는 방식**이다.  자식은 보통 바로 `exec()` 시스템 콜을 호출하여 새로운 프로그램을 실행하도록 설계된다.  이 방식은 **불필요한 메모리 복사를 피할 수 있으므로 매우 효율적**이다.

![](../images/Pasted%20image%2020250604103517.png)
처음에는 똑같은 code와 데이터를 가리키지만, 수정이 되면 그제서야 복제하고, 거기서 수정한다. 

![](../images/Pasted%20image%2020250604103544.png)
단순히 모든 page를 복제하는 것보다 더 효율적인 방식이 될 수 있다 .


---
## **Page Replacement

page replacement 는 우선 디스크에서 가져올 페이지를 찾고, 메인 메모리에 빈 칸을 찾는데, 빈 칸이 없을경우 발생한다. 교체 될 대상에 수정 사항이 있다면, 즉 dirty bit이 켜져 있다면 디스케 적용이 필요하다. 빈 칸이 생기면 이제 새로 생긴 빈칸에 디스크에서 가져온 page를 넣는다. 그리고 page table과 같이 연관된 것들을 update 해준다. trap을 발생한 instruction을 재시작하므로써 프로세스를 진행하게 된다.  

![](../images/Pasted%20image%2020250605000901.png)

---
## **Page Replacement Algorithm**

**Victim page를 잘 고르는 것은 매우 중요하다.** 왜냐하면 **자주 사용되고 있던 페이지를 잘못 내보내게 되면**, 곧바로 다시 해당 페이지에 접근하게 되어 **또다시 Page Fault가 발생하게 된다.** 이 경우, 불필요한 디스크 접근이 추가로 발생하고, 전체 성능이 저하된다.  따라서 **Page Fault를 줄이기 위해서 어떤 페이지를 Page Out할지 신중히 결정해야 한다.**

Page Replacement 알고리즘의 성능은 다음과 같은 방식으로 평가할 수 있다:  

**특정한 메모리 참조 문자열(reference string)** 에 대해 알고리즘을 실행해 보고,  **그 문자열에 대해 발생하는 Page Fault의 개수를 계산**하는 것이다.

> [!note]
> - Reference string은 단순히 **페이지 번호의 나열**이다.  
>     (주소 전체가 아니라, 어떤 page number에 접근했는지만 나타낸다.)
>     
> - 같은 페이지를 반복해서 접근하는 경우에는 **추가적인 Page Fault는 발생하지 않는다.**
>     
> - 실험 결과는 **사용 가능한 프레임 수에 따라 달라진다.**  프레임이 많을수록 일반적으로 Page Fault는 줄어든다.

---

여러 가지 **Page Replacement Algorithm**이 존재한다.
##### **Random**
가장 단순한 방식으로, **무작위로 하나의 페이지를 선택하여 page out**한다.  Page 수가 많고, Victim Page를 고르는 데 드는 연산 비용이 클 경우,  복잡한 알고리즘을 사용하는 것보다 오히려 **랜덤 방식이 효율적일 수 있다.**  실제로 성능도 나쁘지 않으며, **간단하고 빠르다는 장점**이 있다.

##### **FIFO (First-In, First-Out)**
가장 먼저 메모리에 들어온 페이지를 가장 먼저 내보내는 방식이다.  즉, **메모리에 가장 오래 머문 페이지를 page out**한다.  모든 페이지가 공평하게 동일한 시간을 메모리에 머문다고 가정하므로,  **단순하고 직관적**이지만, **자주 사용하는 페이지도 쫓겨날 수 있다는 단점**이 있다.  이로 인해 발생하는 문제가 Belady's Anomaly다.

##### **Optimal (OPT)**
**가장 이상적인 알고리즘**으로, **미래에 가장 오랫동안 사용되지 않을 페이지를 선택**해 교체한다. 이 방식은 이론적으로 가장 적은 수의 Page Fault를 발생시키며, **어떤 알고리즘보다도 성능이 좋다.** 그러나 미래의 참조를 정확히 알 수 없기 때문에 **실제 시스템에서는 구현 불가능**하다. 다만, **다른 알고리즘을 비교·평가할 때 기준점(benchmark)으로 유용**하다.

##### **LRU (Least Recently Used)**
OPT는 미래를 예측하는 방식이라면, LRU는 **과거의 사용 이력을 기반으로 예측**한다. **가장 오랫동안 사용되지 않은 페이지는 앞으로도 사용되지 않을 가능성이 높다**는 가정에 기반한다. 즉, 지금까지 가장 오랫동안 참조되지 않은 페이지를 page out한다. 현실적인 구현도 가능하고, **OPT와 가장 유사한 결과를 내는 근사 알고리즘**이다. 다만, 정확한 LRU 구현에는 시간/공간 오버헤드가 발생할 수 있다.

----
## **FIFO Algorithm**

Reference String은 다음과 같다. 이는 **다음과 같은 순서로 페이지가 참조된다는 것**을 의미한다.

```c
 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1
```

또한, **3개의 프레임**이 있다고 가정한다. 이는 **하나의 프로세스가 동시에 최대 3개의 페이지만 메모리에 올릴 수 있다**는 의미이다.

![](../images/Pasted%20image%2020250605002820.png)
Page Hit : 5 / Page Fault : 15

이 예시는 **FIFO(First-In, First-Out)** 알고리즘을 기반으로 한 것이다.  가장 먼저 메모리에 들어온 페이지부터 **순차적으로 교체 대상이 된다.**  심지어 **Page Hit이 발생한 페이지라도**, 교체 순서에서 예외는 없다.

이 방식의 **장점**은 구조가 간단하고 **구현이 매우 쉽다**는 점이다.  하지만 **단점**으로는 전체적인 Page Fault Rate가 높은 경향이 있으며,  **Belady의 이상현상(Belady's Anomaly)** 이 발생할 수 있다는 점이 있다.  

> [!note] Belay's anomaly
> Belady의 이상 현상이란 더 많은 프레임을 추가했는데 불구하고, 더 많은 page fault가 발생하는 현상을 말한다. 


![](../images/Pasted%20image%2020250605003952.png)
왼쪽 3Frame 보다 4Frame 에서 Page fault가 더 많이 발생했다. 

----
## **Optimal (OPT) Algorithm**

가장 오랜 시간 동안 사용 안 될 Page를 교체하는 알고리즘이다 .

![](../images/Pasted%20image%2020250605004414.png)

성능이 가장 좋지만, 미래를 예측은 불가능하기 때문에 구현이 불가능하다.

---
## **LRU (Least Recently Used) Algorithm**

미래보다 과거의 결과를 바탕으로 알고리즘을 수행한다. 가장 오랫동안 사용되지 않았던 Page는 앞으로도 사용이 잘 안될 것이라는 관점에서 Page를 교체한다. 

![](../images/Pasted%20image%2020250605004739.png)

앞에서도 언급했듯이, **LRU는 최적(OPT) 알고리즘과 유사한 동작을 한다.**  하지만 **구현 난이도가 높은 편**이며, 정확한 LRU를 구현하려면 추가적인 자료구조나 하드웨어 지원이 필요할 수 있다.

>LRU는 **Stack 알고리즘**을 기반으로 구현된다.  

이 방식에서는 각 페이지 참조가 스택의 최상단으로 이동하며,  최근 사용된 순서대로 정렬되어 있는 구조를 유지한다.

**FIFO에서는 프레임 수가 바뀌면 전체 교체 순서도 함께 바뀔 수 있다.**  하지만 **Stack 알고리즘 기반의 LRU나 OPT에서는 항상 n+1개의 프레임을 가진 경우의 페이지 집합이, n개의 프레임을 가진 경우의 집합을 포함하는 구조**가 된다.  즉, **프레임 수가 증가하면 Page Fault는 절대 증가하지 않는다.**

>이러한 특성 덕분에, **LRU와 OPT 알고리즘은 Belady’s Anomaly가 발생하지 않는다.**

---

위의 방식을 구현하기 위해서는 2가지 방법이 있다.

#### 1️⃣ Counter implementation

모든 페이지는 **하나의 카운터(counter)** 를 가진다.  각 페이지가 **접근될 때마다** 해당 시점의 시간(시계 값, clock)을 기록한다.  이렇게 하면 **페이지가 마지막으로 언제 참조되었는지를 알 수 있다.**

Page Fault가 발생하고, 페이지를 교체해야 할 경우에는  **모든 페이지의 카운터 값을 확인하여, 가장 오래전에 사용된 페이지(가장 낮은 시각)를 선택하여 교체**한다.  이 과정에서는 페이지 테이블을 확인하며 교체 대상을 찾아야 한다.

페이지 **접근 시**에는 해당 페이지의 clock 값을 **갱신하는 작업만 수행**하므로,  시간 복잡도는 **O(1)** 이다.
페이지 **교체 시**에는 **모든 페이지의 카운터 값을 순회하며 비교**해야 하므로, 시간 복잡도는 **O(n)** 이 된다.

이러한 **O(n) 비교 작업은 메인 메모리에서 수행될 경우 성능에 큰 영향을 줄 수 있으며**,  시스템 전체의 응답 속도나 처리량에 부정적인 영향을 미칠 수 있다.

#### 2️⃣ Stack implementation

**더블 링크드 리스트(Double Linked List)** 안에서 페이지 번호들을 **스택 구조로 표현**할 수 있다.  각 노드는 하나의 페이지를 나타내며, **최근에 사용된 페이지는 리스트의 맨 앞(top)** 에 위치하게 된다.

만약 어떤 페이지가 사용된다면, 그 페이지를 **리스트의 맨 앞으로 이동시켜야** 한다. 이 과정에서 해당 노드를 삭제하고 다시 삽입해야 하므로, **최대 6개의 포인터(앞/뒤 노드의 연결, head 위치 등)가 변경**되어야 한다. 따라서 **접근할 때마다 포인터 업데이트가 발생하므로 비용이 크다.**

반면, 페이지 교체가 필요할 경우에는, **가장 오래된 페이지는 리스트의 tail에 위치**하므로 단순히 tail 노드를 제거하면 되며, 이 경우 시간 복잡도는 **O(1)** 이다.

![](../images/Pasted%20image%2020250605010033.png)

하지만 어쨌든, **매번 페이지가 접근될 때마다 리스트 구조를 수정해야 하며**,  이 과정에서 **추가적인 메모리 접근이 수반되기 때문에**  전체적으로 보면 **상당한 성능 오버헤드(Cost)가 발생할 수 있다.**

특히, 캐시와 레지스터보다 접근 속도가 느린 **메인 메모리에서 포인터 수정이 반복적으로 발생**하면  **LRU 알고리즘의 정확성은 유지되더라도, 효율성 측면에서는 불리해질 수 있다.**

---
## **LRU Approximations**

LRU를 그대로 구현하는 것은 Cost가 생각보다 크기 때문에실제로는 **하드웨어의 도움을 받는 `reference bit`(참조 비트)** 를 활용하여 LRU에 근접한 방식으로 페이지 교체를 구현한다.

초기에는 모든 페이지의 `reference bit`가 **0으로 초기화**되어 있다.  이후 페이지에 **접근이 발생하면 해당 페이지의 `reference bit`는 1로 설정**된다.  그리고 운영체제는 **주기적으로 모든 페이지의 `reference bit`를 다시 0으로 초기화**한다.

이러한 방식으로, `reference bit = 1` 라면 **최근에 접근된 페이지** `reference bit = 0` 이라면 **일정 시간 동안 접근되지 않은 페이지**로 해석할 수 있다.
    
따라서 페이지 교체 시에는 **`reference bit`가 0인 페이지들 중에서 교체 대상을 선택**하는 것이 일반적이다. 이때, **정확한 순서는 알 수 없지만**, **최근에 사용된 페이지들만은 피하자**는 관점에서 접근하는 방식이다.

> [!example]
> - Additional-Reference Bits Algorithm
> - Second-Chance Algorithm (Clock Alogorithm)
> - Enhanced Clock-Algorithm
> - Counting-Based Algorithm

---
## **Additional-Reference Bits**

추가적인 reference bit를 하나 더 두자! 
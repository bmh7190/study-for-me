
지금까지 다뤘던 외부 단편화(external fragmentation)를 줄이기 위한 방법인 **페이징(Paging)** 은, 메인 메모리에 프로세스가 원하는 데이터가 이미 존재한다는 가정을 바탕으로 설명되었다. 이제부터 다룰 **가상 메모리(Virtual Memory)** 는, 메인 메모리에 원하는 데이터가 존재하지 않을 수도 있다는 상황까지 포함하는 개념이다. 즉, 가상 메모리는 실제 메모리에 전부 올리지 않고도 프로그램을 실행할 수 있게 해주는 확장된 메모리 관리 기법이다.

---
## **Motivation of Virtual Memory**

프로그램이 실행되는 매 순간을 살펴보면, 실제로 그들이 사용하는 코드나 데이터 전체가 항상 필요한 것은 아니다. 실행 상황에 따라 사용되는 코드가 달라지고, 전체 코드가 모두 사용되는 경우는 드물다. 예를 들어, 오류 처리 코드(error code)나 특이한 경로의 루틴(unusual routines) 등은 프로그램 실행 중에 한 번도 사용되지 않을 수 있다.

페이징을 통해 외부 단편화를 줄일 수는 있었지만, 페이지 단위로 메모리에 적재할 때 굳이 사용되지도 않을 데이터를 모두 불러올 필요는 없지 않을까?라는 의문이 생겼고, 결국 필요한 부분만 메모리에 가져오기 위한 방법이 고민되었다.

그 해결책이 바로 **프로그램을 부분적으로 나누어 필요한 부분만 가져오자!** 는 접근이다. 이렇게 하면 프로그램은 더 이상 물리적 메모리 크기에 제약을 받지 않게 된다. 예를 들어, 8GB짜리 게임이 있다고 해도, 실제 실행 시 필요한 부분만 메모리에 불러온다면, 메인 메모리 크기가 4GB여도 실행이 가능해지는 것이다.

또한 각 프로그램이 실행 중에 사용하는 메모리의 양이 줄어들게 되므로, 전체적으로 더 많은 프로세스를 동시에 메모리에 적재할 수 있게 된다. 이로 인해 **CPU utilization**과 **throughput**도 자연스럽게 향상된다.

뿐만 아니라, 메모리에 실제로 데이터를 로드하거나 교체(swap)하는 횟수가 줄어들게 되므로, 불필요한 I/O가 감소하고 결과적으로 프로그램의 실행 속도도 빨라진다. 이러한 개념과 기법이 바로 **가상 메모리(Virtual Memory)** 이다.

---
## **Virtual Memory**

**Logical 메모리와 Physical 메모리를 분리**하면, **논리 주소 공간(logical address space)** 은 실제 물리 주소 공간(physical address space)보다 훨씬 커질 수 있다. 그 이유는, 어차피 실행 시점에 **실제로 필요한 데이터만 메모리에 가져오면 되기 때문**이다.

> [!note] 예시
> 한 프로그램이 8GB라는 Logical한 데이터 크기를 가지고 있지만, 애초에 그 데이터 중 일부만 physical memory에서 사용하기 때문에 훨씬 커져도 상관없다고 이해하면 될 듯 하다.

즉, 모든 데이터를 한꺼번에 물리 메모리에 올릴 필요가 없으므로, 전체 논리 주소 공간이 물리 메모리의 크기를 넘어도 문제가 되지 않는다. 이러한 분리를 통해 **프로세스는 매우 큰 주소 공간을 할당받는 것처럼 동작할 수 있으며**, 이는 가상 메모리 덕분에 가능하다. 결과적으로 주소 공간의 제약이 사라지고, 더 유연하고 효율적인 메모리 관리가 가능해진다.

**Virtual address space**는 "프로세스가 메모리에 어떻게 저장되는가"에 대한 **논리적인 관점(logical view)** 을 제공하며, 이는 어떤 프로세스든지 동일한 방식으로 사용된다.

> [!note]
> - 가상 주소 공간은 보통 **주소 0부터 시작하여**, **연속적인 주소들**로 끝까지 이어지는 형식을 갖는다.
>     
> - 반면에 실제 물리 메모리는 **페이지 프레임(page frame)** 단위로 나뉘어 저장된다.
>     
> - 따라서 **MMU(Memory Management Unit)**는 논리 주소(logical address)를 물리 주소(physical address)로 적절히 **매핑(mapping)**해 주어야 한다.

Virtual 메모리는 **Demand paging, Demand Segmentation**으로 구현할 수 있다.

![](../images/Pasted%20image%2020250531180248.png)

현재 사용되는 데이터만 올라간다. 유효하긴 하지만 현재 사용되지 않는 애들은 메모리에 있을 수도 있고, 디스크에 있을 수도 있다. 

---

![](../images/Pasted%20image%2020250531181502.png)

> **가상 주소 공간은 0번 주소부터 시작해서 최대 주소까지 연속된 메모리 공간처럼 보인다.**

사용자가 보기엔 연속된 메모리이지만, 실제 물리 메모리는 전혀 그렇지 않고, 이공간에는 코드, 데이터, 힙, 스택 등이 배치된다.

> **스택과 힙 사이에는 사용되지 않는 (reserved or guard) 주소 공간이 존재할 수 있다.**

힙은 위로, 스택은 아래로 성장하기 때문에 둘 사이에 **비워진 가상 주소 공간이 존재**한다.  
이 영역은 아직 물리 메모리로 매핑되지 않았지만,  **힙 또는 스택이 확장될 경우를 대비해 예약된 공간**으로 볼 수 있다.

> **시스템 라이브러리(shared libraries)는 각 프로세스의 가상 주소 공간에 매핑된다.**

물리 메모리에서는 같은 페이지를 여러 프로세스가 공유하므로 공간 절약을 할 수 있다. 또 매핑만 다르고 물리적 내용은 동일하다. 

> **공유 메모리는 읽기-쓰기 권한을 가진 페이지를 여러 프로세스의 가상 주소 공간에 매핑함으로써 구현된다.**

예: `mmap`, `shmget`, `shmat` 등을 사용한 IPC. 실제로는 하나의 물리 페이지가 여러 가상 주소로 접근되는 구조.

> **fork() 시 부모 프로세스의 메모리 페이지를 자식과 공유(Copy-on-Write 방식)하면 프로세스 생성이 빨라진다.**


![](../images/Pasted%20image%2020250531181639.png)

shared library같은 경우 stack heap 사이 어딘가가 있을 것이고, 그냥 shared pages 에서 잘 맵핑해주면 된다. 

---
## **Demand Paging**

**Demand paging**은 실행 시점에 **필요한 데이터만 메모리에 적재**하는 방식이다. 불필요한 데이터는 메모리에 올라오지 않기 때문에, **불필요한 I/O가 줄고**, **메모리 사용량도 절약**된다.

과거에는 컨텍스트 스위치 시 프로세스 전체 메모리를 모두 불러와야 했기 때문에 시간이 오래 걸렸지만, Demand paging은 실행에 필요한 페이지만 메모리에 적재하므로 **프로세스 생성 및 전환 속도**가 빨라진다.

이 방식은 **paging system과 swapping 개념과 유사**하지만, 중요한 차이가 있다.  기존의 **swapping**은 프로세스 전체를 통째로 디스크와 메모리 사이에서 옮겼던 반면,  Demand paging은 **페이지 단위로 데이터를 디스크에서 가져오거나 내보낼 수 있다.**

어떤 페이지가 필요한지는 **실제로 접근하는 순간에 확인된다.**  해당 페이지가 메모리에 없으면 운영체제가 **page fault**를 발생시키고, 그 접근이 올바른 경우라면 디스크에서 해당 페이지를 불러온다. 접근 자체가 잘못되었다면, 해당 프로세스는 **abort(비정상 종료)** 된다.

이처럼, **필요할 때만 페이지를 메모리에 적재하는 방식**은 **Lazy Swapper** 또는 **Demand Paging**이라고 하며, 시스템의 **메모리 효율성과 응답성**을 크게 향상시킨다.

![](../images/Pasted%20image%2020250531182518.png)

기존의 MMU(Memory Management Unit)는 단순히 **논리 주소를 물리 주소로 변환하는 역할**만 수행했다.  하지만 Demand Paging이 도입되면서, 이제 MMU는 **주소 변환뿐만 아니라 해당 페이지가 메모리에 존재하는지도 확인**해야 한다.

만약 요청한 페이지가 이미 메모리에 존재한다면, 기존 방식과 동일하게 정상적으로 접근하면 된다.  하지만 **해당 페이지가 아직 메모리에 존재하지 않는다면**, MMU는 **page fault 예외를 발생시키고**,  운영체제가 디스크에서 해당 페이지를 읽어와 메모리에 적재한 뒤, 다시 접근을 시도하게 된다.

이 모든 과정은 **프로그램의 동작 흐름에 영향을 주지 않고**, **자동으로 투명하게 처리되어야 한다.**  즉, 프로그래머는 이 과정에 대해 신경 쓰지 않아도 되며, **프로그램 코드 역시 변경할 필요가 없다.**

---
## **Valid-Invalid Bit**

MMU는 주소를 변환하는 과정에서 **해당 페이지가 실제 메모리에 존재하는지도 확인**해야 한다. 이때 사용할 수 있는 것이 바로 **페이지 테이블의 Valid/Invalid 비트**이다.

처음에는 대부분의 페이지 엔트리가 **Invalid(i)** 상태로 표시되어 있으며, 이는 해당 페이지가 아직 메모리에 적재되지 않았음을 의미한다.

만약 변환된 주소가 **Invalid로 표시된 페이지**를 참조하면, 운영체제는 **디스크에서 해당 페이지를 메모리로 로딩**해야 한다. 이 과정은 매우 느리다. 왜냐하면 **디스크 접근 속도는 메모리보다 훨씬 느리기 때문**이다.

아무리 하나의 페이지를 불러오는 작업이라 하더라도, 수천 배 이상 느린 디스크 I/O가 발생하게 되며, 이 때 발생하는 예외 상황을 **Page Fault**라고 한다.

![](../images/Pasted%20image%2020250531183820.png)

---
## **Page Fault**

프로세스가 어떤 페이지에 **처음 접근**하려고 할 때, 해당 페이지가 아직 메모리에 존재하지 않는다면, **trap이 발생하고 이것이 바로 Page Fault이다.**

Page Fault가 발생하면 운영체제는 **우선 해당 접근이 유효한 주소 범위인지 검사**한다. 만약 **접근이 허용되지 않은 주소라면**, 프로세스는 **abort(비정상 종료)** 된다. 반대로, **정상적인 접근이지만 그 페이지가 메모리에 없는 상태**라면(즉, 페이지 테이블의 해당 엔트리가 invalid 상태), 다음 절차를 따른다.
    
1. **Free frame(빈 프레임)** 을 찾아본다.
    
    - 만약 프레임이 없다면, **기존 페이지 중 하나를 선택해 교체(eviction)** 해야 한다.
        
    - 교체 대상이 수정된 페이지(dirty)라면 디스크에 쓰는 작업까지 함께 이뤄진다.
        
2. 디스크에서 필요한 페이지를 읽어와, **선택된 프레임에 적재(swap-in)** 한다.  이 과정은 **디스크 I/O를 동반**하기 때문에 비교적 시간이 오래 걸린다.
    
3. 페이지가 메모리에 올라오면, **페이지 테이블을 업데이트**하여  해당 페이지의 Valid/Invalid 비트를 **Valid**로 바꾼다.
    
4. 마지막으로, **Page Fault를 유발한 명령어는 아직 실행되지 않았기 때문에**,  해당 명령어를 **다시 실행(restart)** 하여 정상적으로 수행을 마친다.

![](../images/Pasted%20image%2020250531184915.png)

---
## **Characteristics of Demand Paging**

**Demand Paging**은 프로세스가 시작할 때 **메모리에 어떤 페이지도 존재하지 않는다고 가정**하는 방식이다. 따라서 운영체제가 **프로세스의 첫 번째 명령어를 수행하려고 instruction pointer를 설정**하는 순간,  해당 명령어가 메모리에 없기 때문에 **즉시 Page Fault가 발생**한다.  

>결국, **모든 페이지에 대한 첫 접근 시에는 반드시 Page Fault가 발생**한다.

예를 들어, 메모리에서 **두 개의 숫자를 더하고 그 결과를 다시 메모리에 저장**하는 명령어를 실행한다고 하자.  이 명령어가 여러 개의 페이지를 참조하고, 각 페이지가 아직 메모리에 없는 상태라면,  **하나의 명령어 수행을 위해 여러 번의 Page Fault가 발생할 수도 있다.**

하지만 실제로는 이렇게 많은 Page Fault가 연달아 발생하는 경우는 드물다.  그 이유는 바로 **공간적 지역성(spatial locality)** 때문이다.  즉, 한 페이지를 참조하면 그 주변의 데이터도 함께 사용하는 경우가 많아,  **한 번 적재된 페이지가 연속된 명령어 수행에 반복적으로 활용**되기 때문이다.

##### Demand Paging을 지원하는 주요 메커니즘

> [!note] Demand Paging을 지원하는 주요 메커니즘
> 
> - **Valid/Invalid Bit**  
>     → 페이지가 메모리에 존재하는지 확인하는 플래그.
>
> - **Backing Store (Secondary Storage / Swap Device)**  
>     → 메모리에 없는 페이지를 디스크에서 가져올 수 있는 저장 공간.
>     
> - **Instruction Restart**  
>     → Page Fault 발생으로 중단된 명령어를, 페이지 적재 후 **다시 처음부터 재실행**할 수 있는 기능.

---
## **Instruction Restart**

**Page fault**는 보통 **명령어가 실행되는 도중에 발생**한다.  이 경우 해당 명령어는 완전히 수행되지 않았기 때문에, **프로세스는 그 상태에서 계속 진행될 수 없다.**

운영체제는 페이지를 디스크에서 메모리로 적재한 뒤,  **중단된 명령어를 정확히 처음부터 다시 실행해야 한다.**  이 과정을 **Instruction Restart**라고 한다.

중요한 점은, **프로세스는 자신이 Page Fault를 경험했는지 전혀 알지 못해야 하며**,  코드 또한 그로 인해 변경되거나 영향을 받아서는 안 된다.

예를 들어 `increment`나 `decrement` 명령처럼 **레지스터나 메모리를 수정하는 연산**은  명령이 **중간까지 수행되고 멈춘 경우** 문제가 될 수 있다.  따라서 하드웨어는 이러한 상황을 감안하여, **해당 명령이 완전히 끝나기 전에는 아무런 부작용(side-effect)이 남지 않도록 설계**해야 한다.

즉, **하드웨어는 instruction의 재시작을 지원할 수 있어야 하며**,  PageFault가 발생해도 **정확하게 이전 상태로 되돌아가 명령을 처음부터 다시 수행**할 수 있어야 한다.

---
## **TLB fault VS Page Fault**

많은 사람들이 **TLB Fault(TLB Miss)** 와 **Page Fault** 를 혼동한다.  하지만 이 둘은 **역할과 의미가 완전히 다르다.**

**TLB Fault** 는 단순히 “최근에 이 페이지에 접근한 적이 있는가?”를 확인하는 과정이다.  TLB는 **최근 접근한 페이지의 주소 변환 정보를 캐싱**해둔 구조이며,  해당 페이지가 TLB에 없다면 **TLB miss (TLB fault)** 가 발생한다.
    
TLB miss가 발생하면, 그제서야 **운영체제나 하드웨어가 페이지 테이블을 조회하여**  해당 페이지가 **실제로 메모리에 존재하는지** 확인한다.
    
이때 페이지 테이블에 해당 페이지가 **Valid** 하다면 **Page Hit**,  페이지가 **Invalid** 하다면 **Page Fault** 가 발생하게 된다.
    
> ✅ **Page Fault** 는 **실제로 메모리에 페이지가 없는 상태**를 의미하고,  
> ✅ **TLB Fault** 는 **주소 변환 캐시에 정보가 없는 상태**를 의미한다.

---
## **Stages in Demand Paging**

Page fault가 발생하면, 먼저 **운영체제에 trap이 발생**한다. 이때부터 **page fault handler**가 동작하여 다음과 같은 절차를 수행한다.

1. **현재 프로세스의 레지스터와 상태(process state)를 저장**한다.  이후에 이 프로세스를 다시 재개할 수 있도록 하기 위함이다.
    
2. **해당 접근이 유효한 가상 주소 범위 내에 있는지 확인**한다.
    
    - 유효하지 않은 접근이라면, 잘못된 메모리 접근이므로 프로세스를 abort시킨다.
        
    - 유효한 접근이라면, 해당 페이지는 아직 메모리에 없고 디스크에 있다는 뜻이므로, **디스크 상에서 해당 페이지의 위치를 확인**한다.
        
3. 디스크로부터 데이터를 읽어올 **free frame**을 찾는다. 만약 사용 가능한 프레임이 없다면, 페이지 교체 알고리즘을 통해 기존 페이지 중 하나를 제거해야 한다.
        
4. 디스크에서 페이지를 읽어와 메모리의 free frame에 적재한다.  이 작업은 디스크 I/O를 수반하기 때문에 시간이 오래 걸린다.  **그 동안 CPU는 다른 프로세스에게 할당된다.**
    
5. 디스크 I/O가 완료되면 **interrupt가 발생**하고, 운영체제는 **Interrupt Service Routine (ISR)** 을 실행한다. ISR 역시 CPU에서 실행되기 때문에, 현재 수행 중이던 프로세스의 레지스터와 상태를 저장한다.
    
6. 이번 interrupt가 **앞서 요청한 페이지 적재 작업이 완료된 것인지 확인**한다.  맞다면, 이제 해당 페이지가 메모리에 존재하므로, **페이지 테이블을 업데이트**하고  관련된 캐시나 TLB 등도 필요 시 갱신한다.
    
7. 이후, Page Fault를 야기했던 프로세스를 **ready queue에 다시 등록**한다.  이제 이 프로세스는 실행할 준비가 된 것이다.
    
8. 나중에 스케줄러가 이 프로세스를 다시 선택하면,  **저장해 두었던 레지스터와 프로세스 상태를 복원**하고, **Page Fault가 발생했던 instruction부터 다시 실행을 시작**한다.

---
## **Performance of Demand Paging**

Demand Paging에서는 처음부터 필요한 모든 데이터를 메모리에 올리지 않기 때문에,  실행 도중 필요한 페이지를 참조할 때마다 **Page Fault**가 발생할 수 있다.  이로 인해 프로그램 실행 중간 중간 **심각한 성능 저하**가 생길 수 있다.

Page Fault가 발생하면 운영체제는 여러 작업을 수행하게 되며, 그 중에서도 핵심적인 세 가지는 다음과 같다:

1. **Service the interrupt**  
    → 인터럽트를 처리하고 관련 context 저장 및 복원 등 기본적인 오버헤드를 수반함  
    → 수백 개의 명령어 수준으로 상대적으로 가벼움
    
2. **Read the page from disk / Write the victim page**  
    → 디스크로부터 필요한 페이지를 읽어오고, 교체 대상 페이지(victim page)가 있다면 디스크에 기록  
    → **가장 시간이 오래 걸리는 단계** (디스크 I/O는 메모리 접근보다 수천 배 느림)
    
3. **Restart the process**  
    → 저장해 둔 레지스터와 상태를 복원하고, fault가 발생한 명령어부터 다시 실행  
    → 짧은 시간 소요

#####  Page Fault Rate (P)

- **Page Fault Rate (P)** 는 **메모리 접근 1회당 page fault가 발생할 확률**을 의미한다.
    
- `P = 0` → 모든 메모리 접근이 메모리에 있는 경우 (최적의 상황)
    
- `P = 1` → 모든 메모리 접근이 page fault를 야기하는 경우 (최악의 상황)

##### Effective Access Time (EAT) 공식

```
EAT = (1 - P) × memory access time + P × (page fault overhead + swap out time + swap in time)
```

- **(1 - P)**: 메모리에 이미 존재하는 경우 → 정상적인 접근 시간
    
- **P**: Page Fault 발생 확률
    
- **page fault overhead**: 인터럽트 처리, 페이지 테이블 검사 등
    
- **swap out/in time**: 디스크 I/O (가장 큰 시간 소모)


---

> [!example] 예시
> Memory Access Time = 100ns
> Average page-fault service time = 8ms(page fault overhead + swap in/out time)


EAT=(1-p) * 100ns +p * 8000000ns   = 100 + p *7999900

이 경우에 만약에 p = 0.001 이라면 EAT은 8.1us가 된다. 그니까 1000번 중 한 번 page fault 가 발생하면 원래 memory access time인 100ns 보다 80배 증가한다는 것이다. 

하지만 사용자들은 이렇게 길어지는 것을 원하지 않고, 예를 들어 EAT < 110 ns를 원한다면

110 > 100 + p * 7999900
10 > p * 7999900
p<0.00000125

즉 page fault는 800000 번 중에 한 번 일어나야한다.

> page fault는 생각보다 성능에 미치는 영향이 크기 때문에 훨씬 작은 수를 가져야 한다

---
# **Page Selection**

위에서는 page fault에 대해서 알아보았다. 하지만 page fault가 안 일어나는 게 제일 좋기 때문에, Page를 잘 골라 와야 한다. page selection 전략들은 여러 개가 있다.

**Demand paging**  
기존에 알고 있던 방식으로, 처음 시작할 때는 필요한 페이지가 없다는 것을 가정하고, page fault가 발생하면 그제서야 page를 load해오고, 메모리 안에 불러올 때까지 기다린다. 대부분의 paging system에서는 이 방법을 사용한다.

**Prepaging**  
사용되기 전에 미리 메모리에 페이지를 가져다 놓는 방식이다.  만약 어떤 페이지가 reference 될 때 그 옆에 것도 같이 가져오면 어떨까?  하지만 미래를 정확히 예측할 수는 없기 때문에, 효과적으로 사용되기 어렵다.  따라서 일반적으로는 잘 사용되지 않으며, **연속적으로 다음 데이터를 읽는 형태의 프로세스**라면 적용할 수 있다.

**Request paging**  
다음에 사용될 페이지를 예측하기 어렵다면, 차라리 **다음에 사용할 페이지를 미리 사용자에게 알려달라고 요청하는 방식**이다.  이론적으로 괜찮은 방법일 수 있으나, 모든 책임을 사용자에게 넘기는 구조이기 때문에 한계가 있다.  사용자가 언제 어떤 페이지가 필요한지 명확히 알지 못하면 사용이 어렵고,  필요한 데이터를 과도하게 가져오는 경우 다른 프로세스가 피해를 볼 수 있으며,  결과적으로 전체 시스템의 효율이 떨어질 수 있다.

---

`fork()`를 통해 부모 프로세스를 자식 프로세스로 복제할 때, 자식은 부모와 **같은 데이터를 복제**한다. 여기에는 코드나 데이터가 포함되지만, 실제로 데이터를 복제하지 않고 **부모의 데이터를 가리키기만 한다**.

>이 방식을 **Copy on Write (COW)** 라고 한다.

COW는 부모와 자식 프로세스가 **초기에는 동일한 페이지를 공유**하는 방식이다. 두 프로세스는 같은 메모리 페이지를 가리키며, 만약 둘 중 하나가 공유된 페이지에 **수정을 시도하면**, 그때 해당 페이지가 복제되어 각각의 프로세스가 독립적으로 데이터를 수정할 수 있게 된다.

일반적으로 **free pages는 0으로 초기화된 "zero-fill-on-demand" 페이지 풀(pool)** 에서 할당된다.  이 풀은 **빠른 demand paging 수행을 위해 항상 일정량의 free frame을 유지**해야 한다.

>그렇다면, 왜 free page를 할당하기 전에 0으로 채워야 할까?  

그 이유는 **이전에 사용되던 데이터가 여전히 메모리에 남아 있을 수 있기 때문**이다.  즉, 다른 프로세스가 사용하던 민감한 데이터가 남아 있을 경우,  새로운 프로세스가 해당 페이지를 그대로 참조하면 **보안상의 문제가 발생할 수 있다.**  따라서 free page는 반드시 0으로 초기화하여 **이전 프로세스의 흔적을 제거**하고 **안전한 환경을 보장**해야 한다.

한편, `vfork()`는 일반적인 `fork()`와는 다르게,  **부모 프로세스를 일시적으로 멈추고 자식 프로세스가 필요한 작업을 먼저 수행하도록 하는 방식**이다.  자식은 보통 바로 `exec()` 시스템 콜을 호출하여 새로운 프로그램을 실행하도록 설계된다.  이 방식은 **불필요한 메모리 복사를 피할 수 있으므로 매우 효율적**이다.

![](../images/Pasted%20image%2020250604103517.png)
처음에는 똑같은 code와 데이터를 가리키지만, 수정이 되면 그제서야 복제하고, 거기서 수정한다. 

![](../images/Pasted%20image%2020250604103544.png)
단순히 모든 page를 복제하는 것보다 더 효율적인 방식이 될 수 있다 .


---
## **Page Replacement

page replacement 는 우선 디스크에서 가져올 페이지를 찾고, 메인 메모리에 빈 칸을 찾는데, 빈 칸이 없을경우 발생한다. 교체 될 대상에 수정 사항이 있다면, 즉 dirty bit이 켜져 있다면 디스케 적용이 필요하다. 빈 칸이 생기면 이제 새로 생긴 빈칸에 디스크에서 가져온 page를 넣는다. 그리고 page table과 같이 연관된 것들을 update 해준다. trap을 발생한 instruction을 재시작하므로써 프로세스를 진행하게 된다.  

![](../images/Pasted%20image%2020250605000901.png)

---
## **Page Replacement Algorithm**

**Victim page를 잘 고르는 것은 매우 중요하다.** 왜냐하면 **자주 사용되고 있던 페이지를 잘못 내보내게 되면**, 곧바로 다시 해당 페이지에 접근하게 되어 **또다시 Page Fault가 발생하게 된다.** 이 경우, 불필요한 디스크 접근이 추가로 발생하고, 전체 성능이 저하된다.  따라서 **Page Fault를 줄이기 위해서 어떤 페이지를 Page Out할지 신중히 결정해야 한다.**

Page Replacement 알고리즘의 성능은 다음과 같은 방식으로 평가할 수 있다:  

**특정한 메모리 참조 문자열(reference string)** 에 대해 알고리즘을 실행해 보고,  **그 문자열에 대해 발생하는 Page Fault의 개수를 계산**하는 것이다.

> [!note] 참고사항
> - Reference string은 단순히 **페이지 번호의 나열**이다.  
>     (주소 전체가 아니라, 어떤 page number에 접근했는지만 나타낸다.)
>     
> - 같은 페이지를 반복해서 접근하는 경우에는 **추가적인 Page Fault는 발생하지 않는다.**
>     
> - 실험 결과는 **사용 가능한 프레임 수에 따라 달라진다.**  프레임이 많을수록 일반적으로 Page Fault는 줄어든다.

---

여러 가지 **Page Replacement Algorithm**이 존재한다.
##### **Random**
가장 단순한 방식으로, **무작위로 하나의 페이지를 선택하여 page out**한다.  Page 수가 많고, Victim Page를 고르는 데 드는 연산 비용이 클 경우,  복잡한 알고리즘을 사용하는 것보다 오히려 **랜덤 방식이 효율적일 수 있다.**  실제로 성능도 나쁘지 않으며, **간단하고 빠르다는 장점**이 있다.

##### **FIFO (First-In, First-Out)**
가장 먼저 메모리에 들어온 페이지를 가장 먼저 내보내는 방식이다.  즉, **메모리에 가장 오래 머문 페이지를 page out**한다.  모든 페이지가 공평하게 동일한 시간을 메모리에 머문다고 가정하므로,  **단순하고 직관적**이지만, **자주 사용하는 페이지도 쫓겨날 수 있다는 단점**이 있다.  이로 인해 발생하는 문제가 Belady's Anomaly다.

##### **Optimal (OPT)**
**가장 이상적인 알고리즘**으로, **미래에 가장 오랫동안 사용되지 않을 페이지를 선택**해 교체한다. 이 방식은 이론적으로 가장 적은 수의 Page Fault를 발생시키며, **어떤 알고리즘보다도 성능이 좋다.** 그러나 미래의 참조를 정확히 알 수 없기 때문에 **실제 시스템에서는 구현 불가능**하다. 다만, **다른 알고리즘을 비교·평가할 때 기준점(benchmark)으로 유용**하다.

##### **LRU (Least Recently Used)**
OPT는 미래를 예측하는 방식이라면, LRU는 **과거의 사용 이력을 기반으로 예측**한다. **가장 오랫동안 사용되지 않은 페이지는 앞으로도 사용되지 않을 가능성이 높다**는 가정에 기반한다. 즉, 지금까지 가장 오랫동안 참조되지 않은 페이지를 page out한다. 현실적인 구현도 가능하고, **OPT와 가장 유사한 결과를 내는 근사 알고리즘**이다. 다만, 정확한 LRU 구현에는 시간/공간 오버헤드가 발생할 수 있다.

----
## **FIFO Algorithm**

Reference String은 다음과 같다. 이는 **다음과 같은 순서로 페이지가 참조된다는 것**을 의미한다.

```c
 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1
```

또한, **3개의 프레임**이 있다고 가정한다. 이는 **하나의 프로세스가 동시에 최대 3개의 페이지만 메모리에 올릴 수 있다**는 의미이다.

![](../images/Pasted%20image%2020250605002820.png)
Page Hit : 5 / Page Fault : 15

이 예시는 **FIFO(First-In, First-Out)** 알고리즘을 기반으로 한 것이다.  가장 먼저 메모리에 들어온 페이지부터 **순차적으로 교체 대상이 된다.**  심지어 **Page Hit이 발생한 페이지라도**, 교체 순서에서 예외는 없다.

이 방식의 **장점**은 구조가 간단하고 **구현이 매우 쉽다**는 점이다.  하지만 **단점**으로는 전체적인 Page Fault Rate가 높은 경향이 있으며,  **Belady의 이상현상(Belady's Anomaly)** 이 발생할 수 있다는 점이 있다.  

> [!note] Belay's anomaly
> Belady의 이상 현상이란 더 많은 프레임을 추가했는데 불구하고, 더 많은 page fault가 발생하는 현상을 말한다. 


![](../images/Pasted%20image%2020250605003952.png)
왼쪽 3Frame 보다 4Frame 에서 Page fault가 더 많이 발생했다. 

----
## **Optimal (OPT) Algorithm**

가장 오랜 시간 동안 사용 안 될 Page를 교체하는 알고리즘이다 .

![](../images/Pasted%20image%2020250605004414.png)

성능이 가장 좋지만, 미래를 예측은 불가능하기 때문에 구현이 불가능하다.

---
## **LRU (Least Recently Used) Algorithm**

미래보다 과거의 결과를 바탕으로 알고리즘을 수행한다. 가장 오랫동안 사용되지 않았던 Page는 앞으로도 사용이 잘 안될 것이라는 관점에서 Page를 교체한다. 

![](../images/Pasted%20image%2020250605004739.png)

앞에서도 언급했듯이, **LRU는 최적(OPT) 알고리즘과 유사한 동작을 한다.**  하지만 **구현 난이도가 높은 편**이며, 정확한 LRU를 구현하려면 추가적인 자료구조나 하드웨어 지원이 필요할 수 있다.

>LRU는 **Stack 알고리즘**을 기반으로 구현된다.  

이 방식에서는 각 페이지 참조가 스택의 최상단으로 이동하며,  최근 사용된 순서대로 정렬되어 있는 구조를 유지한다.

**FIFO에서는 프레임 수가 바뀌면 전체 교체 순서도 함께 바뀔 수 있다.**  하지만 **Stack 알고리즘 기반의 LRU나 OPT에서는 항상 n+1개의 프레임을 가진 경우의 페이지 집합이, n개의 프레임을 가진 경우의 집합을 포함하는 구조**가 된다.  즉, **프레임 수가 증가하면 Page Fault는 절대 증가하지 않는다.**

>이러한 특성 덕분에, **LRU와 OPT 알고리즘은 Belady’s Anomaly가 발생하지 않는다.**

---

위의 방식을 구현하기 위해서는 2가지 방법이 있다.

#### 1️⃣ Counter implementation

모든 페이지는 **하나의 카운터(counter)** 를 가진다.  각 페이지가 **접근될 때마다** 해당 시점의 시간(시계 값, clock)을 기록한다.  이렇게 하면 **페이지가 마지막으로 언제 참조되었는지를 알 수 있다.**

Page Fault가 발생하고, 페이지를 교체해야 할 경우에는  **모든 페이지의 카운터 값을 확인하여, 가장 오래전에 사용된 페이지(가장 낮은 시각)를 선택하여 교체**한다.  이 과정에서는 페이지 테이블을 확인하며 교체 대상을 찾아야 한다.

페이지 **접근 시**에는 해당 페이지의 clock 값을 **갱신하는 작업만 수행**하므로,  시간 복잡도는 **O(1)** 이다.
페이지 **교체 시**에는 **모든 페이지의 카운터 값을 순회하며 비교**해야 하므로, 시간 복잡도는 **O(n)** 이 된다.

이러한 **O(n) 비교 작업은 메인 메모리에서 수행될 경우 성능에 큰 영향을 줄 수 있으며**,  시스템 전체의 응답 속도나 처리량에 부정적인 영향을 미칠 수 있다.

#### 2️⃣ Stack implementation

**더블 링크드 리스트(Double Linked List)** 안에서 페이지 번호들을 **스택 구조로 표현**할 수 있다.  각 노드는 하나의 페이지를 나타내며, **최근에 사용된 페이지는 리스트의 맨 앞(top)** 에 위치하게 된다.

만약 어떤 페이지가 사용된다면, 그 페이지를 **리스트의 맨 앞으로 이동시켜야** 한다. 이 과정에서 해당 노드를 삭제하고 다시 삽입해야 하므로, **최대 6개의 포인터(앞/뒤 노드의 연결, head 위치 등)가 변경**되어야 한다. 따라서 **접근할 때마다 포인터 업데이트가 발생하므로 비용이 크다.**

반면, 페이지 교체가 필요할 경우에는, **가장 오래된 페이지는 리스트의 tail에 위치**하므로 단순히 tail 노드를 제거하면 되며, 이 경우 시간 복잡도는 **O(1)** 이다.

![](../images/Pasted%20image%2020250605010033.png)

하지만 어쨌든, **매번 페이지가 접근될 때마다 리스트 구조를 수정해야 하며**,  이 과정에서 **추가적인 메모리 접근이 수반되기 때문에**  전체적으로 보면 **상당한 성능 오버헤드(Cost)가 발생할 수 있다.**

특히, 캐시와 레지스터보다 접근 속도가 느린 **메인 메모리에서 포인터 수정이 반복적으로 발생**하면  **LRU 알고리즘의 정확성은 유지되더라도, 효율성 측면에서는 불리해질 수 있다.**

---
## **LRU Approximations**

LRU를 그대로 구현하는 것은 Cost가 생각보다 크기 때문에실제로는 **하드웨어의 도움을 받는 `reference bit`(참조 비트)** 를 활용하여 LRU에 근접한 방식으로 페이지 교체를 구현한다.

초기에는 모든 페이지의 `reference bit`가 **0으로 초기화**되어 있다.  이후 페이지에 **접근이 발생하면 해당 페이지의 `reference bit`는 1로 설정**된다.  그리고 운영체제는 **주기적으로 모든 페이지의 `reference bit`를 다시 0으로 초기화**한다.

이러한 방식으로, `reference bit = 1` 라면 **최근에 접근된 페이지** `reference bit = 0` 이라면 **일정 시간 동안 접근되지 않은 페이지**로 해석할 수 있다.
    
따라서 페이지 교체 시에는 **`reference bit`가 0인 페이지들 중에서 교체 대상을 선택**하는 것이 일반적이다. 이때, **정확한 순서는 알 수 없지만**, **최근에 사용된 페이지들만은 피하자**는 관점에서 접근하는 방식이다.

> [!example]
> - Additional-Reference Bits Algorithm
> - Second-Chance Algorithm (Clock Alogorithm)
> - Enhanced Clock-Algorithm
> - Counting-Based Algorithm

---
### **Additional-Reference Bits**

추가적인 방법으로는, **기존의 reference bit 외에 추가로 8비트의 shift 레지스터를 도입**하는 방식이 있다.  즉, 각 페이지마다 **총 9비트(1비트 reference bit + 8비트 레지스터)** 를 가지게 되는 것이다.

이 방식에서는 **주기적으로 모든 페이지의 레지스터를 오른쪽으로 한 비트씩 shift**한다.  그 직전에 해당 페이지에 접근이 있었다면, **최상위 비트(MSB)를 1로 설정**하고 shift를 수행한다.  접근이 없었다면 그냥 shift만 된다.

이 과정을 반복하면, **최근의 접근 이력을 이진수 패턴으로 표현**할 수 있게 된다.  따라서 **레지스터 값이 작을수록, 오랫동안 접근되지 않았던 페이지라는 의미**가 된다.  결국, **가장 작은 레지스터 값을 가진 페이지가 교체 대상(LRU 후보)** 이 된다.

##### 장점
**모든 메모리 접근마다 오버헤드를 발생시키지 않기** 때문에, 접근 시 단순히 reference bit만 설정하면 되므로 효율적이다.
**시간 간격(shift 주기)을 조절할 수 있기** 때문에  시스템 상황에 따라 민감도를 조정 가능하다.

##### 단점
앞서 설명한 **카운터 방식과 마찬가지로**,  **모든 페이지를 주기적으로 스캔하여 레지스터 값을 비교**해야 한다.  따라서 페이지 수가 많을 경우, 이 스캔 작업이 **비효율적이고 성능 저하**를 유발할 수 있다.

---
### **Second-Chance Algorithm**

**Clock 알고리즘**은 FIFO 방식에 **하드웨어에서 제공하는 reference bit를 결합한 형태**로, 실제 시스템에서 널리 사용된다.  이 알고리즘은 **"Second-Chance" 알고리즘**이라고도 불린다.

페이지 교체가 필요할 때, **현재 포인터가 가리키는 페이지의 reference bit를 확인**한다.

- 만약 reference bit가 **0이라면**, 해당 페이지는 **최근에 사용되지 않았다고 판단**하고, **곧바로 교체 대상이 된다.**

- 반면 reference bit가 **1이라면**, 이 페이지는 **최근에 사용된 페이지**이므로 교체하지 않고,  reference bit를 0으로 리셋한 뒤 **다음 페이지로 포인터를 이동**한다.

이 과정을 반복하면서, 결국 reference bit가 0인 페이지를 만날 때까지 순환하며 확인한다.  모든 페이지가 reference bit = 1인 상태라면, 한 바퀴를 돌면서 **모든 reference bit가 0으로 바뀌게 되므로**,  결국 교체할 수 있는 페이지를 반드시 찾을 수 있다.

![](../images/Pasted%20image%2020250606153255.png)

##### 장점
오버헤드가 매우 낮기 때문에 페이지가 교체 될 때만 작동하며, 페이지 접근 시에 별도의 무거운 작업은 필요없다.
간단한 하드웨어 지원인 reference bit만 있으면 충분히 구현 가능하다.

##### 단점
단순히 reference bit 하나로 판단하므로, 최근 사용 순서에 대한 정밀한 정보는 알 수 없다. 즉 정확하지는 않다.
**모든 페이지가 reference bit = 1인 경우**, 알고리즘이 **한 바퀴를 돌면서 모든 reference bit를 초기화해야 하고**, 결국 일반적인 FIFO와 거의 비슷한 결과를 낼 수 있으며, 이로 인해 성능이 떨어질 수 있다.

---
## **Enhanced Clock Algorithm**

위에서 다룬 Clock 알고리즘은 reference bit만 사용하는 방식이었다.  하지만 여기에 **페이지가 수정되었는지를 나타내는 dirty bit(modify bit)** 를 추가하면,  알고리즘의 효율을 한층 더 향상시킬 수 있다.

페이지에 **수정된 내용이 있다면 디스크에 write-back이 필요하므로**,  교체 시간이 더 오래 걸리게 된다.  
따라서 **수정된 페이지는 교체 우선순위를 낮추고**,  **수정되지 않은 페이지를 먼저 교체 대상으로 고려**하는 것이 더 효율적이다.

다음은 reference bit와 modify bit 조합에 따른 페이지 상태를 요약한 것이다:

| Reference Bit | Modify Bit | 설명 |
|---------------|-------------|------|
| 0             | 0           | 최근에 사용되지 않았고, 수정도 되지 않음 → **교체 대상에 가장 적합** |
| 0             | 1           | 최근에 사용되진 않았지만, 수정됨 → **교체 가능하지만 write-back 필요** |
| 1             | 0           | 최근에 사용되었고, 수정은 안 됨 → **아직 사용 중일 가능성 있음** |
| 1             | 1           | 최근에 사용되었고, 수정도 됨 → **곧 다시 사용될 수 있고, write-back도 필요** |

페이지 교체가 호출되면, **기본적인 Clock 알고리즘을 사용하되**,  각 페이지를 위의 **4가지 클래스 중 하나로 분류**하여,  **가장 우선순위가 낮은 비어 있지 않은 클래스의 페이지를 선택**한다.

단, 원하는 조건을 만족하는 페이지를 찾기 위해  **원형 큐(circular queue)를 여러 번 순회해야 할 수도 있다.**

---
## **Counting-Based Algorithm**

이 방식은 **각 페이지마다 참조(reference) 횟수를 기록**하여 페이지 교체 시 활용하는 방법이다.  
하지만 **현실적인 구현이나 성능 측면에서 일반적으로 사용되지는 않는다.**

##### **Least Frequently Used (LFU)**
가장 적게 참조된 페이지를 선택하여 교체한다.  즉, **reference count가 가장 낮은 페이지가 교체 대상**이 된다.  이 알고리즘은 **"오래되었고, 자주 쓰이지도 않은 페이지는 앞으로도 사용될 가능성이 낮다"**는 가정에 기반한다.

##### **Most Frequently Used (MFU)**

반대로, **가장 많이 참조된 페이지를 교체 대상으로 선택**한다.  이 알고리즘은 다음과 같은 관점을 따른다:

- 어떤 페이지가 이미 많이 참조되었다면, **이미 쓸 만큼 썼고**,  **앞으로는 사용될 가능성이 낮을 수 있다.**
    
- 반면, 참조 횟수가 적은 페이지는 **처음 로드되었을 가능성이 높고**,  **곧 사용될 가능성이 있다.**

---
# **Page Frame Allocation**

각 프로세스는 **정상적인 실행을 위해 최소한의 frame**을 필요로 한다.  하지만 시스템 전체에서 사용할 수 있는 frame의 수는 **정해져 있으므로**,  각 프로세스에 **얼마만큼의 frame을 할당할지**에 대해 고민할 필요가 있다.

우선, 두 가지 기본적인 할당 방식이 있다. 하나는 **프로세스마다 고정된 개수의 frame을 할당**하는 방식이고, 다른 하나는 **프로세스의 우선순위에 따라 frame을 차등 할당**하는 방식이다.

##### Fixed Allocation  
각 프로세스에 동일한 수의 frame을 할당할 수도 있고, 프로세스의 크기에 따라 frame 수를 조정할 수도 있다.

##### Priority Allocation  
프로세스의 크기보다는 **우선순위**를 기준으로 frame을 할당한다.

또한, **frame을 Global하게 할당할지, Local하게 할당할지**를 선택할 수 있다. Global 할당은 모든 프로세스가 **공유된 frame pool**에서 frame을 가져오는 방식이며, Local 할당은 각 프로세스가 **자신에게 미리 할당된 frame 범위 내에서만** 페이지 교체를 수행하는 방식이다.

##### Global Allocation  
Global하게 할당하면, 다른 프로세스가 사용하던 frame도 **교체 대상**이 될 수 있다. 이때 동시 실행 중인 다른 프로세스의 메모리 요구에 따라 성능이 달라질 수 있지만, **전체 시스템의 처리량(throughput)은 향상**된다.

##### Local Allocation  
각 프로세스는 오직 자신에게 할당된 frame 범위 내에서만 **교체 대상을 선택**한다. 따라서 다른 프로세스가 어떤 작업을 하든 간에 **일관된 성능을 유지**할 수 있지만, 때로는 **할당된 frame을 충분히 활용하지 못해 자원이 낭비**될 수 있다.

일반적으로는 **Global Allocation 방식이 더 많이 사용**된다.

---
# Thrashing

**Thrashing**은 **가상 메모리(Virtual Memory)** 환경에서 발생할 수 있는 대표적인 성능 저하 문제이다.  
만약 하나의 프로세스가 **충분한 양의 page 또는 frame을 가지고 있지 않다면**,  **Page Fault 비율은 자연스럽게 증가**하게 된다.

Page Fault가 발생하면, 운영체제는 교체할 페이지를 찾아야 한다.  그런데 **할당된 frame이 충분하지 않기 때문에**,  **최근에 사용된 페이지조차 교체 대상이 될 수 있다.**  만약 그렇게 교체된 페이지가 **곧 다시 필요하게 된다면**,  또다시 Page Fault가 발생하게 된다.

이런 식으로 **Page Fault가 반복적으로 계속 발생하면**,  CPU는 그에 따른 처리(디스크 I/O, 인터럽트, 페이지 테이블 갱신 등)에만 자원을 소모하게 되고, **실제 사용자 프로그램이 수행해야 할 작업을 거의 하지 못하는 상황**이 벌어진다.

하지만 운영체제는 이를 인지하지 못하고, **CPU가 일이 없는 상태(idle)라고 잘못 판단하여**, **더 많은 프로세스를 스케줄링**하거나, 기존 프로세스에 **더 많은 작업을 부여**할 수도 있다.

그 결과, **기존 Page Fault에 새로운 Page Fault까지 추가**되며, **Page 교체가 끊임없이 일어나게 되고, 전체 시스템의 성능은 급격히 저하된다.** 이러한 상태를 **Thrashing**이라고 한다.

> 즉, **Thrashing**이란 "프로세스가 Page를 swap in/out하는 데 바빠서, **실제로 해야 할 작업을 거의 수행하지 못하는 상태**"를 의미한다.

![](../images/Pasted%20image%2020250606160301.png)

---

>**Why does demand paging work?**  

Demand paging이 효과적인 이유는 **프로세스가 전체 메모리를 한 번에 사용하는 것이 아니라**,  
**일정한 지역(Locality) 내에서 집중적으로 메모리를 참조하는 경향**이 있기 때문이다.

>**Why does thrashing occur?**  

Thrashing은 **하나의 지역(Locality)의 크기가 전체 물리 메모리보다 클 때** 발생한다.  즉, **프로세스가 필요한 모든 페이지를 메모리에 유지할 수 없는 상황**에서  계속해서 페이지가 교체되며 Page Fault가 급증하게 된다.

> How to prevent thrashing?

기본적으로 현대 OS에서는 thrashing을 막지 않지만 Working-set model과 Page-fault frequency로 방지할 수 있다. 

---
## **Working-Set Model**

**Working-Set Model**은 **working-set window**라는 개념을 사용한다.  이는 일정한 크기의 **page reference 수**를 기준으로 한다고 이해하면 된다.

예를 들어, working-set window가 **10,000 instruction**이라면,  **최근 10,000개의 명령어가 실행되는 동안 참조된 모든 페이지의 집합**이 그 시점의 **working set**이 된다.

이 모델의 핵심 아이디어는,  **프로세스가 일정 시간 동안 실제로 사용하는 페이지 집합만 메모리에 유지**하면  Page Fault를 줄이고, Thrashing을 방지할 수 있다는 것이다.

**WSS<sub>i</sub>**(Working Set of Process P<sub>i</sub>) = 프로세스 P<sub>i</sub>가 **가장 최근 Δ 기간 동안 참조한 페이지들의 총 수**

> [!note] 추가 설명
> - Δ는 시간에 따라 변할 수 있으며, **Working Set Window의 크기**를 의미한다.
>     
> - 만약 **Δ가 너무 작으면**, 전체 지역성(locality)을 포괄하지 못하게 된다.
>     
> - 반대로, **Δ가 너무 크면**, 여러 지역성을 한꺼번에 포함하게 된다.
>     
> - **Δ = ∞**라면, 전체 프로그램이 참조한 모든 페이지를 포함하게 된다.

**D = Σ WSS<sub>i</sub>** → 모든 프로세스의 Working Set 크기를 더한 값 = **Total Demand Frames**  

이는 시스템 전체에서 필요한 **총 프레임 수(Demand for frames)** 를 의미하며, **지역성(Locality)의 근사값**으로 볼 수 있다.

만약 **D > m** (여기서 m은 실제 사용 가능한 물리 프레임 수) 이면, 시스템은 **Thrashing 상태에 빠질 수 있다.** 따라서 위의 상황이 발생하면 **하나 이상의 프로세스를 일시 중단(suspend)하거나, 디스크로 swap out** 하여  **총 프레임 수요를 줄이는 방식**으로 Thrashing을 방지해야 한다.

![](../images/Pasted%20image%2020250606161426.png)

---
# **Allocating Kernel Memory**

지금까지는 **사용자 공간(User Memory)** 에 대한 내용을 다루었고, 이제는 **커널 메모리(Kernel Memory)** 에 대해 알아보겠다.

커널은 **Page Fault가 발생하지 않도록 설계**되어 있다.  그 이유는, **커널의 동작이 실시간으로 끊기지 않고 안정적으로 수행되어야 하기 때문**이다.  따라서 운영체제는 **Free Memory Pool에서 미리 커널 영역의 메모리 공간을 확보**해 두며,  필요 시 다른 프로세스나 커널 서브시스템이 메모리를 요청하면,  
**이 미리 확보된 공간에서 필요한 만큼 분할하여 할당**해 준다.

또한, 커널 메모리는 **연속된 물리 메모리(contiguous memory)** 를 요구하는 경우가 많다.  이는 **입출력 장치(DMA) 등에서 연속된 메모리 주소를 필요로 하기 때문**이다.

커널 메모리를 할당하는 대표적인 방법으로 Buddy system allocator과 slap allocator가 있다.

---
## **Buddy System Allocator**

**메모리는 물리적으로 연속된 페이지들로 구성된 고정 크기 세그먼트에서 할당된다.**  이는 커널이 요구하는 **빠른 접근성과 하드웨어 호환성**을 보장하기 위한 방식이다.

이 방식에서는 **메모리를 2<sup>n</sup> 크기로 할당**한다.  즉, 요청된 메모리 크기보다 현재 사용 가능한 블록이 더 크다면,  해당 블록을 **2로 계속 나누어**, 요청된 크기와 가장 근접한 블록 크기가 될 때까지 분할한 후,  그 블록을 할당한다.

이러한 방식은 **나중에 메모리를 병합(merge)하거나 반환할 때도 효율적으로 관리할 수 있게 해주며**,  
**외부 단편화(External Fragmentation)** 를 방지할 수 있다.  다만, 항상 2의 거듭제곱 단위로만 메모리를 할당하기 때문에,  **내부 단편화(Internal Fragmentation)** 는 발생할 수 있다.

![](../images/Pasted%20image%2020250606162348.png)

---
## **Slab Allocator**

운영체제에서 메모리 할당은 보통 **커널 내부의 객체(object)** 들을 위해 수행된다.  예를 들어 **프로세스 테이블, 프로세스 엔트리** 등과 같은 커널 내부의 **데이터 구조(data structure)** 들은  크기와 개수가 **사전에 정해져 있는 경우가 많다.**

따라서 커널은 이러한 **데이터 구조마다 별도의 메모리 공간을 구분하여 관리**하며,  이 각각의 공간을 **Cache**라는 이름으로 관리한다.

Slab은 하나 혹은 그 이상의 물리적으로 연속적인 페이지를 말하고, Cache는 이런 slab이 하나 이상으로 이뤄져 있다고 보면 된다.

**Cache가 생성될 때**, 객체들이 메모리 상에 배치되고, **모두 free 상태로 초기화**된다. 이후 해당 구조체가 사용되면, 해당 객체는 **used 상태로 표시**된다. 만약 slab이 **모든 객체가 사용 중**이라면, **다음 객체는 다른 slab(비어 있는 slab)에서 할당**된다. 만약 **비어 있는 slab이 없다면**, **새로운 slab을 추가로 할당**하여 객체를 제공한다.

**단편화(fragmentation)가 발생하지 않으며**, **메모리 요청을 빠르게 처리**할 수 있어 효율적이다.

![](../images/Pasted%20image%2020250606164205.png)

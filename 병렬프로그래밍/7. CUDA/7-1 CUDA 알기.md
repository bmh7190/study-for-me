NVIDIA의 CUDA는 한마디로 **GPU를 활용해 병렬 연산을 수행하기 위한 소프트웨어 플랫폼**이라고 보면 된다. 

![](../../images/Pasted%20image%2020251122180526.png)

일반적으로 GPU는 컴퓨터의 I/O 디바이스 포트에 장착되며, 기본적인 동작 방식도 다른 I/O 장치와 유사하다. 즉, CPU처럼 시스템 RAM에 직접 접근하는 구조가 아니라, **GPU 자체의 전용 메모리(VRAM)** 이 따로 존재한다. 그래서 연산을 수행하려면 CPU 메모리에 있는 데이터를 먼저 GPU의 RAM으로 복사해 가져와야 하고, 계산이 끝나면 다시 CPU 메모리로 결과를 돌려줘야 한다.

또 GPU의 가장 큰 특징 중 하나는 **압도적으로 많은 수의 쓰레드를 동시에 실행할 수 있다**는 점이다. 이전에 배웠듯이 CPU에서 각 쓰레드가 실행할 함수를 정해주듯, GPU에서도 많은 쓰레드가 실행할 코드를 정의하는데, 이때 GPU에서 병렬로 수행될 함수를 **커널(kernel)** 이라고 부른다. 개발자는 이 커널을 작성해 GPU가 어떤 연산을 어떻게 수행할지를 지정하게 된다.

### Hello World

```c++
// CUDA Hello World
#include <stdio.h>
__global__ void hello_kernel() {
// calculate global thread identifier, note blockIdx.x=0 here
const int thid = blockDim.x*blockIdx.x + threadIdx.x;
printf("Hello from thread %d!\n", thid); // print greeting
}
int main(int argc, char * argv[]) {
cudaSetDevice(0); // set the ID of the CUDA device
// invoke kernel using 4 threads executed in 1 thread block
hello_kernel<<<1, 4>>>();
// synchronize the GPU preventing premature termination
cudaDeviceSynchronize();
}
```


위 코드는 가장 기본적인 형태의 **CUDA Hello World** 예제다. `__global__` 키워드로 선언된 함수가 바로 **GPU에서 실행되는 커널 함수(kernel)** 이다. 이 커널은 CPU가 직접 호출하는 것이 아니라, GPU 내부의 수많은 쓰레드들에 의해 병렬로 실행된다.

GPU에는 쓰레드가 지나치게 많기 때문에 CPU처럼 “단일 쓰레드 단위”가 아닌, **쓰레드들을 묶은 단위인 블록(block)** 으로 스케줄링한다. 커널 실행 구문인 `hello_kernel<<<1, 4>>>();` 는 “블록을 1개 생성하고, 각 블록 안에 쓰레드를 4개 실행한다”는 의미다. 즉, 총 4개의 쓰레드가 동시에 이 커널을 수행하게 된다.

커널 내부에서는 `blockDim.x`, `blockIdx.x`, `threadIdx.x` 값들을 조합해 각 쓰레드가 자신의 전역 ID(`thid`)를 계산한다. 이 ID는 “이 쓰레드가 전체 쓰레드 중 몇 번째인지”를 나타내며, printf에 출력되도록 작성되어 있다.

마지막으로 `cudaDeviceSynchronize()`는 GPU가 커널 실행을 끝낼 때까지 기다리기 위한 함수로, 프로그램이 너무 빨리 종료되지 않도록 해준다.

---
## CUDA Program Structure

CUDA 프로그램을 이해하려면 먼저 **CPU와 GPU가 각각 독립된 실행 영역을 가진다**는 점을 명확히 해야 한다. CPU에서 실행되는 코드를 **host 영역**, GPU에서 실행되는 코드를 **device 영역**이라고 부른다. 이 두 영역은 메모리도 분리돼 있고 실행 방식 컴파일 방식도 다르기 때문에, 어떤 코드가 어디에서 호출되고 어디에서 실행되는지를 구분하는 것이 매우 중요하다. 

![](../../images/Pasted%20image%2020251122180819.png)

CUDA에서는 함수 선언 시 붙는 키워드로 이 구분을 명확히 나타낸다. `__device__` 로 선언된 함수는 **GPU(device) 쪽에서 호출되며 GPU에서 실행**된다. 반대로 `__global__` 로 선언된 함수는 **CPU(host)에서 호출되지만 GPU(device)에서 실행되는 커널 함수**이다. 마지막으로 `__host__` 는 CPU에서 호출되어 CPU에서 실행되는 일반적인 함수이며, 사실상 기본값이기 때문에 명시적으로 적는 경우는 많지 않다.

즉, 함수에 어떤 키워드가 붙어 있느냐에 따라 “어디에서 호출되고 어디에서 실행되는지”가 결정된다. 이는 CPU와 GPU가 독립된 실행 공간을 갖는 CUDA의 핵심 개념이기도 하다.

### __ global__ 의 리턴값은 항상 void
여기서 한 가지 더 중요한 점은, **CPU에서 호출되고 GPU에서 실행되는 함수(`__global__` 함수)** 는 CPU와 GPU가 서로 다른 메모리 공간을 사용한다는 특성 때문에 **CPU로 직접 값을 반환할 수 없다**는 것이다. GPU에서 실행된 함수가 어떤 값을 CPU에 “return” 방식으로 넘기려면 CPU와 GPU 사이의 메모리 복사가 필요하지만, 커널 함수는 수천~수백만 개의 쓰레드가 동시에 실행되기 때문에 일반적인 함수처럼 return 값을 하나로 정의할 수 없다.

이런 이유로 CUDA에서는 `__global__` 함수의 반환 타입을 **반드시 `void`로 고정**한다. 커널 실행 결과가 필요하다면, GPU 메모리에 결과를 저장한 다음 CPU가 그 메모리를 다시 복사해 가져오는 방식으로 처리해야 한다. 즉, 커널의 실행 결과는 return 값으로 직접 가져오는 것이 아니라, **메모리 버퍼를 통해 간접적으로 전달**되는 구조라고 이해하면 된다.

---
## Interconnection between Host and Device

CPU에서 GPU로 데이터를 넘길 때 반드시 기억해야 하는 점이 있다. 바로 **CPU와 GPU는 서로 다른 메모리(RAM ↔ VRAM)를 사용한다는 것**이다. CPU는 시스템 RAM을 사용하고, GPU는 자체적인 VRAM(HBM 등 고대역폭 메모리)을 사용한다. 이 구조 때문에 GPU가 연산을 수행하려면 먼저 CPU 메모리에 있던 데이터를 **PCIe 같은 버스를 통해 VRAM으로 복사**해야 한다.

![](../../images/Pasted%20image%2020251122181832.png)

문제는 이 구간의 속도가 GPU 내부의 메모리 속도에 비해 훨씬 느리다는 점이다. 예를 들어 GPU 내부의 HBM 메모리는 매우 높은 대역폭을 제공하지만, CPU ↔ GPU 사이 데이터 이동 속도는 그보다 훨씬 낮다. 즉, GPU가 아무리 빠른 연산 성능을 갖고 있어도, **초기 데이터 전송 구간이 병목**이 될 수 있다.

결국 GPU 연산을 사용할 때는 “GPU가 CPU RAM에 직접 접근할 수 없다”는 사실 때문에 **매번 데이터를 넘기는 비용이 발생한다**는 점을 반드시 고려해야 한다. 병렬 연산 자체는 GPU가 압도적으로 빠르지만, CPU↔GPU 전송 비용이 큰 작업에서는 오히려 전체 성능이 저하될 수도 있다는 의미다.

---
## Compute to Global Memory Access Ratio

GPU의 성능은 일반적으로 **FLOPS(Floating-point Operations Per Second)** 로 표현된다. 즉, 초당 얼마나 많은 부동소수점 연산을 수행할 수 있는지를 기준으로 평가하는 것이다. 하지만 실제 프로그램의 성능은 단순히 연산 속도만으로 결정되지 않는다. GPU가 연산을 하기 위해서는 먼저 데이터를 VRAM에서 읽어와야 하고, 이 데이터 접근 속도가 연산 속도에 비해 훨씬 느릴 수 있기 때문이다.

그래서 GPU의 실제 효율을 판단할 때 중요한 기준이 **Compute-to-Global-Memory Access Ratio**이다. 이는 “VRAM에서 데이터를 한 번 가져왔을 때, 그 데이터를 가지고 몇 번의 연산을 할 수 있는가?”를 의미한다. 이 비율이 높다는 것은 데이터를 가져오는 비용에 비해 연산을 매우 많이 수행한다는 뜻이고, 반대로 이 비율이 낮으면 아무리 GPU의 FLOPS가 높아도 데이터 이동에 발목이 잡혀 성능이 잘 나오지 않는다.

결국 GPU 성능을 제대로 끌어내기 위해서는 **전송 비용(메모리 접근)** 대비 **연산량**을 최대화해야 하며, 이 compute-to-memory 비율이 GPU 컴퓨팅 효율을 판단하는 핵심 지표가 된다.

----
## GPU Architecture: Streaming Multiprocessors

GPU는 여러 개의 **클러스터(또는 GPC, TPC 같은 상위 구조)** 로 이루어져 있으며, 그 내부에 실제 연산을 담당하는 핵심 단위가 바로 **SM(Stream Multiprocessor)** 이다. CUDA 관점에서 SM은 가장 기본적인 실행 단위로, GPU의 병렬 연산 능력을 결정하는 핵심 요소라고 볼 수 있다.

![](../../images/Pasted%20image%2020251122182643.png)

각 SM 내부에는 다양한 연산 장치들이 포함되어 있다. 예를 들어 FP32 연산을 담당하는 부동소수점 연산기, FP64 연산기, 그리고 메모리를 읽고 쓰는 작업을 처리하는 Load/Store Unit(LDST)이 있다. 또한 코사인·사인·제곱근 같은 복잡한 수학 연산을 빠르게 계산할 수 있도록 **SFU(Special Function Unit)** 도 포함되어 있다.

![](../../images/Pasted%20image%2020251122182745.png)

즉, 하나의 SM 안에 여러 종류의 연산 유닛들이 다수 배치되어 있어서, 이 연산기 하나에 GPU는 매우 많은 쓰레드를 동시에 처리하면서도 다양한 종류의 연산을 빠르게 실행할 수 있다.

그리고 현대 GPU 아키텍처에서는 하나의 SM이 내부적으로 두 개의 파이프라인 그룹(또는 듀얼 이슈 구조)으로 나뉘어 있는 경우가 많다. 이를 통해 한 SM이 동시에 여러 명령을 병렬로 발행할 수 있어 처리 효율을 더욱 높인다. 이 구조 덕분에 GPU는 엄청난 수준의 병렬성을 유지하면서도, 연산 및 메모리 접근을 효율적으로 분배해 전체 성능을 극대화할 수 있다.

---
## CUDA Thread Id

GPU에는 수많은 쓰레드가 존재하며, 각각 고유한 ID를 가진다. 하지만 쓰레드의 수가 워낙 많기 때문에 효율적인 관리와 스케줄링을 위해 이 쓰레드들을 **블록(block)** 단위로 묶어 구성한다. 블록은 단순한 1차원 배열이 아니라, 문제의 구조에 맞춰 **최대 3차원(x, y, z)** 형태로 배치할 수 있어 이미지, 행렬, 볼륨 데이터처럼 고차원 데이터 처리에 자연스럽게 대응할 수 있다. 이렇게 구성된 여러 개의 블록들이 모여 전체 실행 단위인 **그리드(grid)** 를 이루며, 커널이 실행될 때 GPU는 블록 단위로 쓰레드를 배치하고, 이 블록들이 서로 모여 하나의 3차원 그리드 구조로 대규모 병렬 작업을 수행한다.

![500](../../images/Pasted%20image%2020251122183707.png)

예를 들어, 어떤 문제를 2×2 형태의 블록(grid) 구조로 나누고, 각 블록 내부를 다시 3×3 형태의 쓰레드 배열로 구성한다고 하자. CUDA에서는 이런 다차원 구조를 `dim3` 타입으로 표현할 수 있으며, 다음과 같이 설정한다.

```c++
dim3 grid_dim(2, 2);   // 2 × 2 개의 블록
dim3 block_dim(3, 3);  // 각 블록 안에 3 × 3 개의 쓰레드
```

그 다음 커널 실행 시 다음과 같이 전달하면 된다.

```c
kernel<<<grid_dim, block_dim>>>();
```

이 구조에서는 특정 쓰레드에 접근하기 위해 두 단계의 인덱싱이 필요하다.

**어떤 블록인지 찾기 위해** 그리드가 2차원이므로 `blockIdx.x`, `blockIdx.y`  두 인덱스가 필요하다. **그 블록 안에서 몇 번째 쓰레드인지 찾기 위해** 블록 역시 2차원이므로 `threadIdx.x`, `threadIdx.y` 두 인덱스를 사용한다.

즉, 전체 그리드가 2D, 블록도 2D이기 때문에 **각 쓰레드는 4개의 좌표(blockIdx.x, blockIdx.y, threadIdx.x, threadIdx.y)** 로 자신의 위치를 표현한다. 이러한 구조 덕분에 2차원 이미지, 행렬, 시뮬레이션 격자 등을 GPU 쓰레드에 직관적으로 매핑할 수 있다.

---
## Mapping CUDA Thread Blocks Onto SMs

위에서 설명한 grid·block 구조는 우리가 **쓰레드를 논리적으로 어떻게 나누어 생각할지**에 대한 개념적인 단위이고, 실제로 하드웨어에서는 이 쓰레드들이 **SM** 위에 매핑되어 실행된다. 이때 몇 가지 중요한 규칙이 있다.

첫째, **하나의 thread block은 반드시 하나의 SM에 통째로 할당**된다. 하나의 블록이 여러 SM으로 쪼개지는 일은 없다. 각 SM 내부에는 그 블록만을 위해 할당되는 **공유 메모리(shared memory)** 와 레지스터 같은 로컬 자원이 있기 때문에, 같은 블록 안의 쓰레드끼리는 이 자원을 공유하며 비교적 쉽게 데이터를 주고받을 수 있다. 그래서 서로 긴밀하게 협력하고 자주 동기화가 필요한 쓰레드들은 **가능하면 같은 블록 안에 배치하는 것이 유리**하다.

둘째, **하나의 SM에 여러 개의 블록이 동시에 올라가는 것은 가능**하다. SM은 자신의 자원(공유 메모리 용량, 레지스터 수 등)이 허용하는 한 여러 블록을 동시에 스케줄링할 수 있다. 다만 **동기화(synchronization)는 블록 내부에서만 보장**되며, 서로 다른 블록끼리는 CUDA에서 기본적으로 동기화 메커니즘을 제공하지 않는다. 즉, 같은 블록 안의 쓰레드는 `__syncthreads()` 같은 동기화 primitive를 통해 실행을 맞출 수 있지만, **블록 간에는 하드웨어가 동기화를 보장하지 않으며**, 필요하다면 커널 재호출이나 전역 메모리 기반의 별도 프로토콜로 처리해야 한다.

정리하면, **하드웨어는 각 SM의 자원(공유 메모리, 레지스터 등) 상황에 따라 어떤 블록을 어느 SM에 넣을지 자유롭게 결정**한다. 우리는 grid와 block 크기를 설계하고, “연관 있는 쓰레드는 같은 block에 넣고, block 단위로 독립적인 작업을 하게 하자”라는 식으로 설계해주면, 나머지 block → SM 매핑과 스케줄링은 하드웨어가 알아서 자원 제약에 맞게 처리하는 구조라고 보면 된다.

---
## Mapping CUDA Thread to Warp

예를 들어, 하나의 블록에 1024개의 쓰레드가 있다고 가정하면, 이 블록 전체는 하나의 SM에 통째로 배치된다. 하지만 SM이 내부에 여러 연산 유닛(FP32, FP64, LD/ST, SFU 등)을 가지고 있다고 해도, **1024개의 쓰레드를 한 번에 모두 실행할 만큼 충분한 실행 파이프라인을 갖고 있는 것은 아니다.** 그래서 하드웨어는 블록 내부의 쓰레드를 더 작은 단위로 나누어 실행한다.

이때 등장하는 개념이 **워프(warp)** 이다. 워프는 보통 **32개의 쓰레드로 구성되는 실행 단위**이며, SM은 이 warp 단위로 명령을 발행(issue)하고 실행한다. 즉, 1024개의 쓰레드는 실제로는 1024 ÷ 32 = **32개의 워프로 나누어지고**, SM은 이 여러 워프를 번갈아 가며 스케줄링하면서 실행한다.

결과적으로 warp는 GPU에서 **실행의 최소 단위**이며, SM은 warp 단위로 명령을 해석(SIMT 방식)하고 해당 warp에 속한 32개의 쓰레드를 동시에 동일한 명령으로 실행한다. 이렇게 함으로써 GPU는 수천 개의 쓰레드를 효율적으로 처리하면서도 높은 병렬도를 유지할 수 있다.

---
## Relationship between Thread block, Warp, SM

block, warp, SM이 헷갈릴 수 있으니까 다시 한번 정리해보면 다음과 같다.

먼저 **block은 우리가 논리적으로 묶어놓은 쓰레드 그룹**이다. 프로그래머가 “이 연산은 이만큼씩 그룹으로 묶어서 처리하자”라는 의미로 정해놓는 단위다. 이 block 전체는 GPU 하드웨어의 규칙에 따라 **SM 하나에 통째로 배치**된다. 여기까지가 block의 역할이다. block이 SM에 배치되는 순간부터는 block이라는 개념은 사실상 사라지고, 실제 실행은 다른 방식으로 이루어진다.

그다음 단계에서 SM 내부에서는 block 안의 수많은 쓰레드를 작게 나누어 **warp 단위(보통 32개의 thread)** 로 묶고, 이 warp가 GPU에서 실제 연산을 수행하는 실행 단위가 된다. 즉, block이 SM으로 들어오면 SM은 block을 여러 warp로 쪼개서 warp 단위로 명령을 실행하게 된다.

이 구조를 쉽게 비유하면 다음과 같다.

> 유치원 통학버스(block)가 아이들(thread)을 태우고 어린이집(SM)으로 데려다주는 역할을 한다.  
> 어린이집에 내려놓는 순간 버스의 역할(block의 역할)은 끝난다.  
> 이제 아이들은 어린이집 내부에서 각각의 반(warp)으로 나누어 활동한다.

즉, block은 **쓰레드를 SM으로 옮기는 운송 단위**, warp는 **실행 단위**, SM은 **실행 장소**라고 이해하면 전체 구조가 선명하게 잡힌다.

#### Warp Schedule
SM에는 여러 개의 워프가 존재하고, 이 워프들은 **SM 내부의 warp 스케줄러**에 의해 하드웨어적으로 스케줄링된다. CPU와 GPU의 설계 철학이 다르다는 점이 여기서 뚜렷하게 드러난다. CPU는 DRAM 접근 비용이 매우 크기 때문에, 캐시 계층을 활용해 최대한 메모리 접근을 줄이는 데 초점을 둔다. 반면 GPU는 연산을 매우 빠르게, 대량으로 수행하는 데 최적화되어 있으므로, 어떤 워프가 VRAM 접근 등으로 인해 잠시 대기해야 한다면 **그 워프를 즉시 스케줄링에서 제외하고 다음 워프를 실행하는 방식**을 사용한다. 즉, GPU는 _warp 단위의 초고속 context switching_ 을 통해 연산 장치가 놀지 않도록 만든다.

이 context switching을 빠르게 수행하기 위해 GPU는 중요한 선택을 한다. CPU처럼 워프의 실행 상태를 메모리에 저장하고 다시 복원하는 방식은 너무 느리기 때문에, **아예 각 워프의 상태를 모두 레지스터 안에 유지한다.** 이를 위해 GPU의 SM은 막대한 양의 레지스터 파일을 갖고 있으며, 덕분에 워프를 교체할 때 별도의 저장·복원 과정 없이 즉시 다른 워프로 전환할 수 있다. 즉, 하드웨어적 지원 덕분에 context switch 비용이 사실상 0에 가깝게 유지되는 것이다.

이런 구조적 특성 때문에 GPU는 **쓰레드가 많으면 많을수록 유리**하다. 대기하는 워프가 발생하더라도 즉시 다른 워프를 투입해 연산 장치가 한순간도 쉬지 않도록 만들 수 있기 때문이다. 이것이 GPU가 수천~수만 개의 쓰레드를 동시에 관리하도록 설계된 이유이자, GPU 아키텍처의 핵심 철학이다.

---
## Thread and Memory Hierarchy

이러한 GPU의 설계 철학 때문에 GPU는 **CPU와는 다른 형태의 메모리 계층 구조**를 가지고 있다. 

![](../../images/Pasted%20image%2020251122191339.png)

우선 GPU에서 각 쓰레드는 자신만의 **레지스터**를 갖는다. CPU의 레지스터와 비슷하지만 훨씬 대량으로 존재하며, context switching 시 쓰레드 상태를 빠르게 유지하기 위해 매우 큰 레지스터 파일을 가지는 것이 특징이다.

![](../../images/Pasted%20image%2020251122191316.png)

CPU에서 일반적으로 DRAM이라고 부르는 시스템 메모리에 대응하는 것이 GPU에서는 **global memory(VRAM)** 이다. 이 global memory는 GPU 전체가 공유하는 메모리로, 모든 쓰레드가 접근할 수 있다. 다만 global memory 접근은 상대적으로 느리기 때문에, GPU 성능을 극대화하려면 이 접근을 최소화하도록 프로그램을 구성하는 것이 중요하다.

또 하나의 계층은 **local memory** 이다. 이름만 보면 쓰레드 지역 메모리처럼 보이지만, 사실 물리적으로는 global memory의 일부에 위치한다. 즉, 레지스터만으로 저장하기 어려운 임시 데이터나 컴파일러가 spilling 처리한 값을 저장하는 용도로 사용된다. 개념적으로는 각 쓰레드의 스택 메모리 같은 역할을 한다고 보면 된다.

여기에 더해 GPU에는 **shared memory** 가 존재한다. 이는 같은 block 안의 쓰레드들이 공동으로 사용하는 고속 메모리 공간이다. 예를 들어 block 내부의 여러 쓰레드가 동일한 데이터를 반복해서 사용할 때, 이 데이터를 각 쓰레드의 레지스터에 따로 저장해두는 것은 비효율적이다. 이런 경우 shared memory에 한 번만 저장해두면 block 내의 모든 쓰레드가 고속으로 접근할 수 있다. shared memory는 일반 global memory보다 훨씬 빠르기 때문에, GPU 병렬 최적화에서 핵심적인 역할을 한다.

정리하자면, GPU의 메모리는 두 영역으로 나뉜다. 쓰레드가 가장 빠르게 접근하는 레지스터와 블록 내부에서 함께 사용하는 shared memory는 모두 **SM 내부**에 있어 매우 빠르게 동작한다. 반면, 우리가 보통 VRAM이라고 부르는 global memory와 그 안에서 쓰레드마다 따로 잡히는 local memory는 **SM 바깥의 디바이스 메모리(VRAM)** 에 위치한다. 즉, 빠른 메모리들은 SM 안에, 용량이 크지만 상대적으로 느린 메모리들은 VRAM 쪽에 있다고 이해하면 된다.

---
## CUDA Memory Model Overview
CPU에서 처리하던 데이터를 GPU가 사용하려면, 결국 **CPU의 메인 메모리(RAM)** 에 있는 값을 **GPU의 VRAM으로 옮겨 놓아야 한다.**  하지만 GPU는 CPU 메모리에 직접 접근할 수 없기 때문에, 우리가 명시적으로 VRAM 공간을 확보하고 그곳에 데이터를 복사해야 한다.

![](../../images/Pasted%20image%2020251122191518.png)

### CUDA Memory Allocation

GPU에서 데이터를 사용하려면 먼저 **VRAM(global memory)에 사용할 공간을 직접 할당해야 한다.** 즉, CPU 메모리에 있는 데이터를 복사해 넣을 “목적지”를 GPU 쪽에 미리 만들어 두는 것이다. CUDA에서는 이를 위해 `cudaMalloc()`을 사용한다.

예를 들어 아래 코드는 정수 10개를 저장할 수 있는 공간을 GPU의 global memory에 할당하는 예시다.
```c++
#include <stdio.h>
#include <cuda_runtime.h>

int main(){
    int *d_data;
    int N = 10;

    // GPU 메모리(VRAM)에 N개의 int를 저장할 공간을 할당
    cudaMalloc((void**)&d_data, N * sizeof(int));

    printf("Allocated GPU Memory for %d integers.\n", N);

    // 사용이 끝난 GPU 메모리는 반드시 해제
    cudaFree(d_data);

    printf("Freed GPU Memory.\n");

    return 0;
}

```

이렇게 `cudaMalloc()`으로 global memory에 공간을 확보해 둬야 그 뒤에 CPU 메모리 → GPU 메모리로 데이터를 복사할 수 있고, GPU 커널에서도 이 공간을 사용해 연산을 수행할 수 있다.

```c++
cudaMalloc((void**)&d_data, N * sizeof(int));
```

위 코드에서 `d_data`는 **GPU 메모리를 가리키게 될 포인터 변수**이지만, 처음에는 아무것도 가리키지 않는 상태다. `cudaMalloc()`이 호출되면 CUDA 런타임이 VRAM(global memory) 안에 필요한 만큼 공간을 할당하고, 그 **GPU 메모리의 시작 주소를 d_data 안에 기록해준다.**

중요한 점은, `cudaMalloc()` 자체가 포인터를 반환하는 함수가 아니라는 것이다. 대신 **포인터 변수(d_data)의 값 자체를 바꾸는 방식으로 GPU 주소를 넣어야 한다.** 이를 위해서는 당연히 **d_data라는 변수의 주소**가 필요하고, 그래서 `cudaMalloc()`에 전달할 때 포인터의 포인터(`**`) 형태가 사용된다.

다시 말해서, `d_data`는 “주소를 저장하는 변수”이므로 그 값을 변경하려면 **그 변수 자체의 주소를 넘겨주는 것**이다.

그 결과 `cudaMalloc()` 호출이 끝나면 `d_data`는 GPU VRAM의 실제 메모리 공간을 가리키게 되고, 이후 GPU 커널이나 `cudaMemcpy()`에서 이 포인터를 사용해 데이터를 읽거나 쓸 수 있게 된다.

```c++
cudaFree(d_data);
```

`cudaFree(d_data)`는 GPU에서 사용이 끝난 메모리를 해제하기 위한 함수다. 앞서 `cudaMalloc()`을 통해 VRAM에 공간을 할당하면, 그 GPU 메모리의 주소가 `d_data` 변수 안에 저장된다. 그리고 작업이 모두 끝난 뒤 `cudaFree()`를 호출할 때는, 이 `d_data`가 가리키고 있는 VRAM 주소를 인자로 넘겨줌으로써 CUDA 런타임에게 “이 주소의 GPU 메모리를 반환해달라”는 요청을 하게 된다. 결국 `cudaFree()`는 `d_data`에 들어 있는 실제 GPU 메모리의 주소를 기반으로 해당 공간을 해제하는 역할을 하며, 이후 그 메모리는 더 이상 유효하지 않기 때문에 다시 사용하기 위해서는 새로 `cudaMalloc()`을 호출해야 한다.

---
### CUDA Host-Device Data Transfer
앞에서 `cudaMalloc()`을 통해 VRAM(global memory)에 공간을 확보했다면, 이제 그곳에 실제 데이터를 채워 넣기만 하면 된다. 이를 위해 사용하는 함수가 `cudaMemcpy()`이다.

`cudaMemcpy()`는 다음과 같은 형태를 가진다.

```c++
cudaMemcpy(void* destination, void* source, int size, direction )
```

여기서 `destination`에는 **데이터가 복사될 목적지의 주소**, 즉 앞에서 VRAM에 할당된 GPU 메모리 주소를 넣는다. `source`에는 **보낼 데이터가 있는 CPU 메모리 주소**를 넣는다. 그리고 `size`는 복사할 데이터의 크기를 바이트 단위로 전달한다.

마지막 인자인 `direction`은 복사 방향을 지정하는데, CUDA에서는 미리 정해진 값을을 사용한다.

- **H2D (cudaMemcpyHostToDevice)**  
    CPU → GPU 로 데이터 복사  
    즉, host 메모리에서 device 메모리로 보내는 경우
    
- **D2H (cudaMemcpyDeviceToHost)**  
    GPU → CPU 로 데이터 복사  
    즉, device 메모리에서 host 메모리로 가져오는 경우

이렇게 방향을 명시적으로 지정해야 CUDA 런타임이 어떤 주소 체계를 사용해야 하는지 정확히 이해할 수 있다. 결국, `cudaMemcpy()`는 우리가 준비한 데이터를 host에서 device로, 혹은 그 반대로 이동시키는 핵심 함수이며, GPU 연산의 시작과 끝을 연결하는 필수 단계라고 보면 된다.
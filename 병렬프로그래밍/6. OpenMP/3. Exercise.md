## Example 1

```c++
dotp = 0;
for (i=0; i<n; i++)
	dotp += a[i] + b[i];
```

첫 번째 코드는 단순히 `a`와 `b`의 같은 인덱스 원소를 더해서, 그 값을 `dotp`에 계속 누적하는 코드다. 여기서 `dotp`는 반복문 바깥에서 정의되어 있기 때문에 **공유 변수**이고, 이 상태로 반복문을 병렬화하면 여러 스레드가 동시에 `dotp`를 갱신하게 되어 race condition이 발생할 수 있다. 이런 상황에서는 `atomic`이나 `critical`을 사용해서 보호할 수도 있지만, 이 경우에는 OpenMP의 `reduction`을 쓰는 게 더 자연스럽다.

```c++
dotp = 0;
#pragma omp parallel for reduction(+:dotp)
for (i=0; i<n; i++)
	dotp += a[i] + b[i];
```

`reduction(+:dotp)`을 붙이면, OpenMP가 각 스레드마다 **자기만의 로컬 `dotp`** 를 하나씩 만들어서 그 안에 부분 합을 계산하게 한다. 그리고 병렬 영역이 끝날 때, 모든 스레드의 로컬 `dotp`를 `+` 연산으로 한 번에 합쳐서 최종 `dotp`에 넣어준다.  

결과적으로, race condition 없이 각 스레드가 나눠서 계산한 부분 합을 안전하게 모아 도트 프로덕트를 구할 수 있게 되는 것이다.

---
## Example 2

```c++
for (i=0; i<(int)sqrt(x); i++) {
	a[i] = 2.3 * i;
	if (i<10) b[i] = a[i];
}
```

두 번째 예시는 병렬화하기에 비교적 단순한 구조다. 우선 반복문의 조건을 보면, 반복 범위가 `sqrt(x)`로 주어져 있다. `x`는 반복문 바깥에서 이미 정의되어 있고, 루프가 시작될 때 `sqrt(x)`가 계산되어 반복 범위가 **고정된 값**으로 확정된다. OpenMP가 `parallel for`를 적용하려면, 반복문의 크기가 컴파일 시점 또는 루프 시작 시점에 **확실하게 결정**되어 있어야 하므로 이 부분은 문제가 없다.

반복문 내부에서는 `a[i]`가 인덱스에 따라 갱신되고, `i < 10`일 때만 `b[i]`가 업데이트된다. 각 반복에서 접근하는 인덱스 `i`는 서로 다르므로, 스레드 간에 **동일한 메모리 위치에 접근하는 충돌(race condition)** 이 발생하지 않는다. 즉, 공유 변수를 동시에 갱신하거나, 서로의 계산 결과를 사용해야 하는 의존성도 없다.

따라서 이 루프는 OpenMP로 안전하게 병렬화할 수 있다.

```c++
#pragma omp parallel for
for (i=0; i<(int)sqrt(x); i++) {
	a[i] = 2.3 * i;
	if (i<10) b[i] = a[i];
}
```

---
## Example 3

```c++
for (i=k; i<n; i++)
	a[i] = b * a[i-k];
```

이 코드를 딱 보면, 오른쪽에 있는 `a[i-k]`가 왼쪽의 `a[i]`를 만드는 데 사용되고 있다. 즉, **현재 항(`a[i]`)이 이전 항(`a[i-k]`)에 의존**하고 있는 구조라서, 반복들 사이에 **의존성(dependence)** 이 존재한다. 이런 경우 그대로 `#pragma omp parallel for`를 걸어버리면, 서로 참조해야 하는 값이 아직 계산되지 않았을 수도 있기 때문에 안전하게 병렬화할 수 없다.

그런데 의존성을 조금만 잘 들여다보면, 모든 인덱스가 서로 얽혀 있는 건 아니다.  
의존 관계는 항상 `i`와 `i - k` 사이에서만 생긴다.

예를 들어 k=2라고 치면:

- `a[2]`는 `a[0]`에 의존
    
- `a[4]`는 `a[2]`에 의존
    
- `a[6]`는 `a[4]`에 의존
    

이렇게 **짝수끼리만 연결된 체인**이 하나 생기고,

- `a[3]`는 `a[1]`에 의존
    
- `a[5]`는 `a[3]`에 의존
    
- `a[7]`는 `a[5]`에 의존
    

이렇게 **홀수끼리만 연결된 체인**이 또 하나 생기는 식이다.

즉, 인덱스를 `k`로 나눈 나머지별로 묶으면, 각 체인 안에서는 순서가 중요하지만 **체인끼리는 서로 독립**이다. 그렇다면 “각 체인 하나씩을 각 스레드에 맡기면 되겠네?”라는 아이디어가 나온다.

그걸 코드로 옮기면 이렇게 된다

```C++
#pragma omp parallel for
for (int s = 0; s < k; s++) {           // 시작점 s: 0, 1, ..., k-1
    for (int i = s + k; i < n; i += k){ // s와 같은 나머지 클래스만 순회
        a[i] = b * a[i-k];
    }
}
```

여기서 바깥 루프의 `s`는 **각 체인의 시작점(나머지 클래스)** 을 의미하고, 안쪽 루프의 `i`는 `s + k, s + 2k, s + 3k, ...`처럼 항상 `k`씩 건너뛰면서 **같은 체인(같은 나머지)** 만 방문한다.

이렇게 바꾸면 같은 체인 안에서는 여전히 순서가 중요하니까 한 스레드가 해당 체인을 차례대로 처리하고, 서로 다른 `s`에 해당하는 체인들끼리는 서로 의존성이 없으므로 바깥쪽 `s` 루프를 `parallel for`로 병렬화해도 안전하다.

정리하면,

> 원래 루프는 `a[i]`가 `a[i-k]`에 의존해서 그대로는 병렬화할 수 없지만, `i`를 `k`로 나눈 나머지별로 체인을 나누고, 각 체인을 하나의 스레드가 처리하도록 2중 루프로 재구성하면 의존성은 체인 내부에만 남고, 체인끼리는 독립이기 때문에 OpenMP `parallel for`를 쓸 수 있게 된다.

---
## Example 4

```c++
for (i=0; i<m; i++)
	for (j = 1; j<n; j++)
		a[i][j] = a[i][j]/a[i][j-1]
```

이 코드는 한눈에 봐도 **j에 대한 의존성(dependence)** 이 있다는 걸 알 수 있다.  
각 원소 `a[i][j]`는 바로 **이전 열**인 `a[i][j-1]`을 사용해서 계산되기 때문에,  
같은 행(`i` 고정) 안에서는 `j`가 1부터 n-1까지 순서대로 진행되어야 한다.

같은 행 i 안에서는 `j = 1 → 2 → 3 → ...` 순서를 반드시 지켜야 하고, 하지만 **서로 다른 행들 i끼리는 서로 독립**이다. `a[0][*]`를 계산하는 것과 `a[1][*]`를 계산하는 것은 서로 간섭하지 않는다.

그래서 이 경우에는 **가장 바깥쪽 루프(i 루프)** 를 병렬화해 주면 된다.

```c++
#pragma omp parallel for
for (i=0; i<m; i++)
	for (j = 1; j<n; j++)
		a[i][j] = a[i][j]/a[i][j-1]
```

---

## Example 5

```c++
for (i=1; i<m-1; i++)
	for (j=1; j<n-1; j++)
		a[i][j]=(a[i][j-1]+a[i][j+1]+a[i-1][j]+a[i+1][j])/4.0;
```

이 경우에는 하나의 지점을 기준으로 상하좌우 4방향 값에 의존성이 생긴다. 즉, `a[i][j]`를 계산할 때 이웃한 원소들의 값에 동시에 의존하고 있기 때문에, 단순히 이 루프에 병렬화를 적용하면 계산 순서에 따라 읽히는 값이 달라질 수 있다. 이런 의존성 때문에 이 코드는 지금 형태 그대로는 그냥 병렬화해서 사용할 수 없다.

---
## Example 6

```c++
for (i = 1; i<N; i++)
	for (j = 1; j<M; j++)
		A[i][j]=max3(A[i-1][j]+g,
		A[i][j-1]+g, A[i-1][j-1]+
		sbt[sq1[i]][sq2[j]]);
```

이 반복문에서는 각 셀 `A[i][j]`를 계산할 때, 위쪽(`A[i-1][j]`), 왼쪽(`A[i][j-1]`), 그리고 왼쪽 위 대각선(`A[i-1][j-1]`) 값을 동시에 참조하고 있다. 즉, **위 / 왼쪽 / 좌상단 대각선 방향으로 의존성**이 존재하는 전형적인 DP 패턴이라, 그대로 `#pragma omp parallel for`를 걸면 순서가 꼬여서 병렬화가 불가능해 보인다.

![](../../images/Pasted%20image%2020251203020147.png)

그런데 조금 관점을 바꿔서 보면, **좌하단에서 우상단으로 올라가는 대각선(anti-diagonal)** 위에 있는 원소들끼리는 서로 의존성이 없다.  

왜냐하면 `A[i][j]`는 항상 인덱스 합이 `i + j = s`일 때, 의존하는 값들은 모두 `i + j = s - 1`인 이전 대각선에만 있기 때문이다.

그래서 “행 우선, 열 우선” 순서가 아니라,  
**대각선(s = i + j)이 같은 것끼리 묶어서 한 번에 계산**하는 방향으로 루프를 재구성할 수 있다.

이를 코드로 옮기면 다음과 같이 작성할 수 있다.

```c++
for ( int s = 2; s<= M+N-2; s++)
	#pragma omp prallel for
	for (int i = max(1,S-(M-1)), i< min(N,S); i++){
		int j = s-i;
		A[i][j]=max3(A[i-1][j]+g,A[i][j-1]+g, A[i-1][j-1]+ sbt[sq1[i]][sq2[j]]);

	}
```
---
## Example 7

```c++
area = 0.0;
for (i=0; i<n; i++) {
	x = (i+0.5)/n;
	area += 4.0/(1.0+x*x);
}
pi = area / n;
```

이 코드는 간단한 수치 적분 방식으로 π를 근사하는 예시인데, OpenMP로 병렬화하려고 보면 두 가지 문제가 있다.

첫 번째는 `area` 변수다. `area`는 반복문 바깥에서 선언된 **공유 변수**이고, for 문을 돌면서 각 반복에서 값을 계속 누적하고 있다. 이런 형태는 여러 스레드가 동시에 `area`를 갱신하게 되면 race condition이 발생할 수 있기 때문에, OpenMP의 `reduction`을 사용하는 것이 적절하다.

두 번째는 `x` 변수다. 코드 상에서 `x`는 for 문 안이 아니라 바깥에서 선언되어 있으므로 기본적으로 **공유 변수**가 된다. 하지만 이 변수는 누적되거나 공유되는 값이 아니라, 각 반복에서 **독립적으로 새로 계산되는 임시 변수**이기 때문에 reduction이 아니라, 반복마다 스레드별로 따로 갖고 있으면 충분하다. 이런 경우에는 `private(x)`를 사용해 각 스레드가 자신만의 `x`를 가지도록 설정하는 것이 적절하다.

그래서 최종적으로는 다음과 같이 병렬화할 수 있다.

```c++
area = 0.0;

#pragme omp parall for reduction(+:area) private(x)
for (i=0; i<n; i++) {
	x = (i+0.5)/n;
	area += 4.0/(1.0+x*x);
}
pi = area / n;
```

정리하자면, `area`는 `reduction(+:area)`를 통해 각 스레드가 부분 합을 따로 계산한 뒤 마지막에 한 번에 더해주고, `x`는 `private(x)`로 선언해서 각 스레드가 자기 범위에서만 사용하는 임시 변수로 만들어 race condition 없이 안전하게 병렬화를 적용한 것이다.
익숙한 예시다. MNIst 문제라고 그니까 0부터 9까지 숫자를 인식하는 건데, 하나의 이미지는 28 * 28 = 784 픽셀로 이루어져 있고, 이런 이미지가 65000개가 있다고 가정한다.

![](../../images/Pasted%20image%2020251203003556.png)

---
## 1NN Classifier for MNIST ( All-paris compute )

1-NN 분류는 테스트 이미지와 훈련 이미지들 간의 거리를 모두 계산한 뒤, 그중에서 가장 가까운 훈련 이미지를 하나 고르는 방식이다. 여기서 말하는 “가깝다”는 것은 두 이미지(벡터) 사이의 **제곱 유클리드 거리**, 즉 각 픽셀 차이를 제곱해서 모두 더한 값이 얼마나 작은지를 기준으로 판단한다. 결국 픽셀 단위로 비교했을 때 차이가 적을수록 두 이미지는 더 비슷하다고 보는 것이다. 그래서 테스트 이미지를 분류할 때는 단순히 “가장 작은 거리”를 가진 훈련 이미지를 찾고, 그 이미지의 레이블(정답)을 그대로 가져와서 테스트 이미지의 예측 값으로 사용한다.

```c++
//OpenMP Nearest Neighbor Classification: distance computation
template <typename value_t, typename index_t>
void all_vs_all(value_t* test,
	value_t* train,
	value_t* delta,
	index_t num_test,
	index_t num_train,
	index_t num_features,
	bool parallel) {
	#pragma omp parallel for collapse(2) if(parallel)
	for (index_t i = 0; i < num_test; i++)
		for (index_t j = 0; j < num_train; j++) {
			value_t accum = value_t(0);
			for (index_t k = 0; k < num_features; k++) {
				const value_t residue = test[i*num_features+k] -
				train[j*num_features+k];
				accum += residue*residue;
			}
		delta[i*num_train+j] = accum;
	}
}
```

`all_vs_all` 함수는 테스트 데이터와 훈련 데이터 간의 모든 쌍 거리($Δ_ij$)를 계산하는데, 이 과정을 OpenMP를 이용해 병렬화할 수 있도록 설계되어 있다. 특히 함수 인자로 `parallel` 값을 받아서, `parallel == true`일 때만 OpenMP 병렬 실행을 하고, 아니라면 일반적인 단일 스레드 실행을 하도록 `if(parallel)` 옵션으로 제어하고 있다.

여기서 눈에 띄는 부분은 `#pragma omp parallel for collapse(2)` 구문이다. 
일반적으로 `parallel for`는 **바로 아래 한 개의 for-loop만 병렬화**한다. 그러나 이 코드처럼 이중 for문(i, j)이 있을 경우, 단순히 첫 번째 루프(i)만 병렬 처리되고, 내부 루프(j)는 각 스레드가 순차적으로 수행하게 된다. 하지만 collapse(2)를 사용하면 **두 개의 for문(i, j)을 하나의 큰 반복문으로 병합한 뒤**, OpenMP가 그 전체 반복 범위를 병렬로 나누어 처리할 수 있게 된다.

결국 이것은 i와 j의 조합을 단순한 2D 반복이 아니라 **총 num_test × num_train 개의 독립적인 작업 단위**로 보고, 그 전체를 스레드들이 나누어 가져가게 하는 방식이다. 즉, 원래는 스레드 하나가 “test 이미지 하나(i 고정)”를 맡고 그 내부에서 모든 train 이미지(j 전체)를 계산해야 했지만, collapse를 사용하면 테스트 이미지와 훈련 이미지의 조합 전체가 하나의 큰 작업 공간이 되어, 스레드들이 자유롭게 (i, j) 조합을 나누어 가져간다. 이렇게 하면 작업 분배가 훨씬 세밀해지고, 특히 test/train 크기가 불균형하거나 캐시 효과를 고려해야 할 때 더 효율적인 병렬 성능을 낼 수 있다.


그니까 아래와 같이 하나의 for행렬로 나타내주는 것이다!

```c++
#pragma omp parallel for
for (index_t h = 0; h < num_test*num_train; h++) {
const index_t i = h / num_train;
const index_t j = h % num_train;
...
}
```

여기서 핵심은 2차원을 1차원처럼 바라보는 것이다. 전체 `(num_test × num_train)`개의 조합을 하나의 긴 배열이라고 생각하면, `h / num_train`은 하나의 테스트 이미지가 몇 번 반복되었는지를 기반으로 “지금 몇 번째 test인지”를 알려주고, `h % num_train`은 그 test 안에서 “몇 번째 train인지”를 알려준다.

결국 collapse(2)는 이러한 평탄화를 OpenMP 내부적으로 자동 적용하도록 만들어주고, 그 결과 스레드들이 test 단위가 아니라 **(test, train) 조합 전체를 균일하게 나누어 가져가면서** 더 효율적으로 병렬 처리가 이루어지게 된다.

결론은 for문이 여러개일 경우 최외곽 for문만 병렬화 하면 성능이 크게 향상 안 할 수도 있다 이런 경우에 collapse를 활용해서 여러 for 문을 하나로 만든 다음 병렬로 돌리면 성능 향상을 기대할 수 있다. 


---
### Fine-Grained Parallelization

지금까지는 **train 이미지와 test 이미지 쌍 (i, j)** 를 기준으로 병렬화를 적용했다. 그런데 조금 더 생각해보면, 이미지 단위가 아니라 **픽셀(특징) 단위로 반복문을 쪼개서 병렬화를 할 수도 있을 것 같다.

예를 들어, 다음과 같이 가장 안쪽의 거리 계산 루프에 OpenMP를 적용하는 방식이다.

```c++
for (index_t i = 0; i < num_test; i++) {
    for (index_t j = 0; j < num_train; j++) {
        value_t accum = some_initial_value;
        #pragma omp parallel for
        for (index_t k = 0; k < num_features; k++)
            // introducing a race condition on accum
            accum += some_value;
    }
}
```

원래 상황을 떠올려 보면, 각 (i, j) 쌍에 대해 train 이미지의 어떤 픽셀과 같은 위치의 test 픽셀을 빼서 제곱한 뒤, 그 값을 `accum`이라는 변수에 계속 더해 거리 값을 만드는 구조다.

그런데 이 `k` 루프에 `#pragma omp parallel for`를 붙이면, **여러 스레드가 동시에 `accum`을 갱신**하게 되고 그 순간부터 `accum`은 여러 스레드가 동시에 접근·수정하는 **공유 변수**가 된다.  

이렇게 되면 update 순서가 뒤섞이면서 **race condition**이 발생할 수 있다.

이를 해결하기 위해 OpenMP에서는 몇 가지 동기화 기법을 제공하는데,  
그 중 하나가 `atomic`이다.

```c++
#pragma omp parallel for
for (index_t k = 0; k < num_features; k++)
    // sanitizing the race condition using atomics
    #pragma omp atomic
    accum += some_value;
```

`#pragma omp atomic`은 `accum += some_value;` 라는 한 줄 연산을 **원자적(atomic)** 으로 수행하도록 만들어서, 한 시점에 정확히 **하나의 스레드만** 이 연산을 실행하도록 보장한다.  

직관적으로는 “이 한 줄 앞뒤로 잠깐 락을 건다”고 생각하면 된다.

이와 비슷한 역할을 하는 또 다른 방법으로 `critical`을 사용할 수도 있다.

```c++
#pragma omp parallel for
for (index_t k = 0; k < num_features; k++)
    #pragma omp critical
    accum += some_value;
```

`#pragma omp critical`은 해당 블록 전체를 **크리티컬 섹션**으로 만들어서 여러 스레드가 동시에 들어가지 못하게 한다. 
`atomic`은 **단일 메모리 연산(+=, ++, = 등)에만** 적용할 수 있는 대신 가벼운 편이고,  
`critical`은 여러 줄의 복잡한 연산에도 적용할 수 있지만 그만큼 오버헤드가 크다.  
그래서 `critical`이 더 범용적으로 보일 수는 있지만, 단순 누적 연산처럼 패턴이 명확할 때는 보통 `atomic`이 선호된다.

---
### Parllel block wise reduction

그런데 방금 봤던 `atomic`이나 `critical` 방식은 이 연산 패턴에서는 성능적으로 비효율적이다. 여러 스레드가 동시에 일을 하더라도, `accum`에 접근하는 순간에는 결국 한 스레드씩 순서대로 처리해야 해서 병렬성이 많이 죽어버리기 때문이다.

이 문제를 OpenMP에서는 **reduction**이라는 개념으로 더 우아하게 해결할 수 있다. 쉽게 말해서, 각 스레드가 자기 몫의 연산 결과를 **자기만의 로컬 변수에 따로 누적해 두었다가**, 병렬 구간이 끝날 때 OpenMP가 이 로컬 값들을 한 번에 모아서 최종 결과로 합쳐주는 방식이다.

```c
for (index_t i = 0; i < num_test; i++) {
    for (index_t j = 0; j < num_train; j++) {
        value_t accum = some_initial_value;
        
        #pragma omp parallel for reduction(+:accum)
        for (index_t k = 0; k < num_features; k++)
            accum += some_value;
    }
}

```

여기서 `reduction(+:accum)`의 의미는 다음과 같다

`accum`을 각 스레드마다 **private 변수**로 따로 하나씩 만들어서, 병렬 루프 안에서는 각자 자기 `accum`에만 더한다. 병렬 루프가 모두 끝난 시점에, OpenMP가 각 스레드의 `accum` 값을 **+ 연산으로 모두 합쳐서** 바깥에 있는 원래 `accum` 변수에 최종 결과를 넣어준다.

즉, `atomic`이나 `critical`처럼 매번 공유 변수에 줄 서서 접근하는 대신, 각 스레드가 독립적으로 계산을 끝낸 뒤 마지막에 한 번만 합쳐주기 때문에 동기화 오버헤드가 훨씬 줄어들고, 병렬 효율이 더 좋아진다.

---
## 1NN Classifier for MNIST ( Label prediction )

지금까지는 test 이미지와 train 이미지 사이에서, 각 픽셀끼리 빼서 제곱하고 그것들을 `accum`에 모두 더해서 **두 이미지 간의 거리(Δ_ij)** 를 구하는 단계였다. 이제 이렇게 구해진 거리들을 바탕으로, “이 test 이미지는 어떤 숫자(라벨)다!”라고 최종 예측을 내리고, 그 예측이 맞았는지 틀렸는지를 세어서 **accuracy(정확도)** 를 계산해야 한다.

여기서 라벨은 **one-hot encoding** 형태로 표현되어 있다. 즉, 0부터 9까지 10개의 칸이 있는 배열에서, 정답 숫자 위치만 1이고 나머지는 0인 형태다. 예를 들어 숫자 ‘3’이면 `[0,0,0,1,0,0,0,0,0,0]` 이런 식이다. 따라서 예측할 때도, test 이미지에 대해 “가장 가까운 train 이미지 하나”를 찾은 다음, 그 train 이미지의 one-hot 라벨과 test 이미지의 정답 one-hot 라벨을 **원소별로 비교**해서 맞았는지 여부를 판단한다.

```c++
//OpenMP Nearest Neighbor Classification: distance computation
template <typename label_t, typename value_t, typename index_t>
value_t accuracy(label_t* label_test, label_t* label_train, value_t* delta,
index_t num_test, index_t num_train, index_t num_features, bool parallel) {
    index_t counter = index_t(0);
    #pragma omp parallel for reduction(+:counter) if(parallel)
    for (index_t i = 0; i < num_test; i++) {
        value_t bsf = std::numeric_limits<value_t>::max(); // initial distance = float::max
        index_t jst = std::numeric_limits<index_t>::max(); // initial index j_star = dummy

        // 1) 이 test 샘플 i와 가장 가까운 train 샘플 j 찾기
        for (index_t j = 0; j < num_train; j++) {
            const value_t value = delta[i*num_train+j];
            if (value < bsf) {
                bsf = value;
                jst = j;
            }
        }

        // 2) 그 train 샘플(jst)의 라벨과 test 라벨 비교
        bool match = true;
        for (index_t k = 0; k < num_classes; k++)
            match &&= label_test[i*num_classes+k] == label_train[jst*num_classes+k];

        // 3) 완전히 동일하다면, 즉 예측이 맞았다면 counter 증가
        counter += match;
    }
    return value_t(counter)/value_t(num_test);
}

```

여기서 `bsf`는 _best so far_의 약자로, “지금까지 본 거리 중 **가장 작은 거리 값**”을 의미하고, `jst`는 그 최소 거리를 주는 train 인덱스 j 라고 생각하면 된다. 즉, `jst`가 바로 1-NN에서 **가장 가까운 이웃의 index**다.

그 다음 `match`는 one-hot 벡터를 통해 **예측 라벨과 실제 라벨이 완전히 같은지** 확인하는 역할을 한다. `match &&= ...` 식으로 모든 클래스에 대해 원소를 비교해서, 하나라도 다르면 `match`가 false가 된다. 최종적으로 `match`가 true라면 이 test 샘플은 **정답을 맞춘 것**이므로 `counter`를 1 증가시킨다.

여기서 OpenMP의 `reduction(+:counter)`가 다시 등장하는데, 

> `counter`를 각 스레드마다 **로컬로 따로** 가지고 있게 한 뒤,  
> 루프가 끝난 뒤에 모든 스레드의 `counter` 값을 **+ 연산으로 한 번에 합쳐라**

라는 뜻이다. 즉, 여러 스레드가 동시에 맞춘 개수를 세더라도 race condition 없이, 마지막에 전체 맞춘 개수를 안전하게 더해서 최종 accuracy를 구할 수 있다.

마지막에 `value_t(counter)/value_t(num_test);` 를 반환하므로,  
**전체 test 샘플 중에서 몇 개를 맞췄는지 비율** → 즉 0.0~1.0 사이의 정확도가 결과가 된다.
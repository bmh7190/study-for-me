익숙한 예시다. MNIst 문제라고 그니까 0부터 9까지 숫자를 인식하는 건데, 하나의 이미지는 28 * 28 = 784 픽셀로 이루어져 있고, 이런 이미지가 65000개가 있다고 가정한다.

![](../../images/Pasted%20image%2020251203003556.png)

---
## 1NN Classifier for MNIST

1-NN 분류는 테스트 이미지와 훈련 이미지들 간의 거리를 모두 계산한 뒤, 그중에서 가장 가까운 훈련 이미지를 하나 고르는 방식이다. 여기서 말하는 “가깝다”는 것은 두 이미지(벡터) 사이의 **제곱 유클리드 거리**, 즉 각 픽셀 차이를 제곱해서 모두 더한 값이 얼마나 작은지를 기준으로 판단한다. 결국 픽셀 단위로 비교했을 때 차이가 적을수록 두 이미지는 더 비슷하다고 보는 것이다. 그래서 테스트 이미지를 분류할 때는 단순히 “가장 작은 거리”를 가진 훈련 이미지를 찾고, 그 이미지의 레이블(정답)을 그대로 가져와서 테스트 이미지의 예측 값으로 사용한다.

```c++
//OpenMP Nearest Neighbor Classification: distance computation
template <typename value_t, typename index_t>
void all_vs_all(value_t* test,
	value_t* train,
	value_t* delta,
	index_t num_test,
	index_t num_train,
	index_t num_features,
	bool parallel) {
	#pragma omp parallel for collapse(2) if(parallel)
	for (index_t i = 0; i < num_test; i++)
		for (index_t j = 0; j < num_train; j++) {
			value_t accum = value_t(0);
			for (index_t k = 0; k < num_features; k++) {
				const value_t residue = test[i*num_features+k] -
				train[j*num_features+k];
				accum += residue*residue;
			}
		delta[i*num_train+j] = accum;
	}
}
```

`all_vs_all` 함수는 테스트 데이터와 훈련 데이터 간의 모든 쌍 거리($Δ_ij$)를 계산하는데, 이 과정을 OpenMP를 이용해 병렬화할 수 있도록 설계되어 있다. 특히 함수 인자로 `parallel` 값을 받아서, `parallel == true`일 때만 OpenMP 병렬 실행을 하고, 아니라면 일반적인 단일 스레드 실행을 하도록 `if(parallel)` 옵션으로 제어하고 있다.

여기서 눈에 띄는 부분은 `#pragma omp parallel for collapse(2)` 구문이다. 
일반적으로 `parallel for`는 **바로 아래 한 개의 for-loop만 병렬화**한다. 그러나 이 코드처럼 이중 for문(i, j)이 있을 경우, 단순히 첫 번째 루프(i)만 병렬 처리되고, 내부 루프(j)는 각 스레드가 순차적으로 수행하게 된다. 하지만 collapse(2)를 사용하면 **두 개의 for문(i, j)을 하나의 큰 반복문으로 병합한 뒤**, OpenMP가 그 전체 반복 범위를 병렬로 나누어 처리할 수 있게 된다.

결국 이것은 i와 j의 조합을 단순한 2D 반복이 아니라 **총 num_test × num_train 개의 독립적인 작업 단위**로 보고, 그 전체를 스레드들이 나누어 가져가게 하는 방식이다. 즉, 원래는 스레드 하나가 “test 이미지 하나(i 고정)”를 맡고 그 내부에서 모든 train 이미지(j 전체)를 계산해야 했지만, collapse를 사용하면 테스트 이미지와 훈련 이미지의 조합 전체가 하나의 큰 작업 공간이 되어, 스레드들이 자유롭게 (i, j) 조합을 나누어 가져간다. 이렇게 하면 작업 분배가 훨씬 세밀해지고, 특히 test/train 크기가 불균형하거나 캐시 효과를 고려해야 할 때 더 효율적인 병렬 성능을 낼 수 있다.


그니까 아래와 같이 하나의 for행렬로 나타내주는 것이다!

```c++
#pragma omp parallel for
for (index_t h = 0; h < num_test*num_train; h++) {
const index_t i = h / num_train;
const index_t j = h % num_train;
...
}
```

여기서 핵심은 2차원을 1차원처럼 바라보는 것이다. 전체 `(num_test × num_train)`개의 조합을 하나의 긴 배열이라고 생각하면, `h / num_train`은 하나의 테스트 이미지가 몇 번 반복되었는지를 기반으로 “지금 몇 번째 test인지”를 알려주고, `h % num_train`은 그 test 안에서 “몇 번째 train인지”를 알려준다.

결국 collapse(2)는 이러한 평탄화를 OpenMP 내부적으로 자동 적용하도록 만들어주고, 그 결과 스레드들이 test 단위가 아니라 **(test, train) 조합 전체를 균일하게 나누어 가져가면서** 더 효율적으로 병렬 처리가 이루어지게 된다.

결론은 for문이 여러개일 경우 최외곽 for문만 병렬화 하면 성능이 크게 향상 안 할 수도 있다 이런 경우에 collapse를 활용해서 여러 for 문을 하나로 만든 다음 병렬로 돌리면 성능 향상을 기대할 수 있다. 

### Fine Gained Parallelization
지금까지는 train 이미지와 test 이미지 사이에서 병렬화를 적용했다. 조금 더 생각해보면  이미지가 아니라 픽셀 단위로 쪼개서 병렬화를 할 수 잇을거라고 생각할 것이다.

```c++
for (index_t i = 0; i < num_test; i++) {
	for (index_t j = 0; j < num_train; j++) {
		value_t accum = some_initial_value;
			#pragma omp parallel for
			for (index_t k = 0; k < num_features; k++)
			// introducing a race condition on accum
			accum += some_value;
	}
}
```

지금 pragma omp parallel for 을 가장 안 쪽에 distance 꼐산하는 부분에 넣었다. 원래 상황을 생각해보면 train의 어느 픽셀과 똑같은 위치의 test 픽셀을 빼서 제곱한 다음 accume 이라는 공유 변수에 다 더하는 과정이다. 근데 이 작업을 병렬적으로 수행하게 되면 accum 변수에는 동시에 여러 쓰레드가 접근할 수 있기 때문에 race condition 문제가 발생할 수 있다.

이걸 해결하기 위한 방법을 openmp에서 지원해주긴한다.

```c++
#pragma omp parallel for
for (index_t k = 0; k < num_features; k++)
	// sanitizing the race condition using atomics
	#pragma omp atomic
	accum += some_value;
```

공유 변수 위에 atomic 을 붙여서 pragma omp atomic을 적용했는데, 일단 이게 하는 역할은 accum 이라는 변수에 하나의 쓰레드만 접근할 수 있도록 하는 것이다. mutex 기반의 critical section 만드는 거라고 생각하면 된다. 똑같은 일을 하는게 atomic 대신에 crititcal을 사용할 수 있는데, 둘의 차이점은 atomic 단일 연산만 적용하고 ciritcal 은 여러 연산에 대해서 처리할 수 있다. ciritical이 여러 연산에 대해서 적용할 수 있다보니 범용적으로 사용되긴한다. 

### Parllel block wise reduction

근데 지금 연산에 대해서는 이것들을 적용하는게 성능적으로 비효율이다. 하나의 thread가 일을 끝냈 때까지 기다려야 하기 때문이다. 

이런 점을 또 openmp로 해결할 수 있다. reduction 이라는 개념인데, 쉽게 설명하면 각 쓰레드가 자신의 계산한 값을 따로 누적해놨다가 병렬 수행이 끝나면 이제 한꺼번에 합쳐서 반환하는 것이다. 

```c++
```

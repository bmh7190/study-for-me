이번 예제에서는 **MNIST 데이터**를 사용해서, **2개 레이어로 이루어진 기본적인 Softmax Regression 모델**을 구현하고, 이를 OpenMP를 활용해 병렬화해 보겠다.

![](../../images/Pasted%20image%2020251203164742.png)

Softmax Regression을 수행하면 하나의 모델이 만들어지고, 이 모델에 하나의 이미지를 입력으로 넣으면 각 픽셀에 대응되는 가중치 행렬과 곱해진다. 이렇게 선형 결합으로 나온 출력에 softmax 함수를 적용하면, 각 클래스에 대한 확률이 계산되고, 그중 가장 큰 값에 대응되는 클래스가 선택된다. 이를 통해 결과적으로 **하나의 클래스는 1, 나머지 클래스는 0으로 처리되는 형태의 분류(classification)** 가 이루어진다.

조금더 자세하게 알아보면 

![](../../images/Pasted%20image%2020251203165243.png)

가중치 행렬은 각 픽셀마다 하나의 회귀식을 가지는 구조라고 이해할 수 있다. MNIST 이미지는 28×28 크기이므로 총 784개의 픽셀이 있고, 따라서 784개의 회귀식이 존재한다. 각 회귀식은 10개의 클래스(0~9)에 대한 출력 값을 만들어야 하므로, 각 회귀식마다 10개의 가중치가 존재한다. 결국 전체 모델의 가중치 행렬은 **784 × 10** 크기를 가지게 된다.

Softmax regression에서는 입력 이미지 $x^{(i)}$가 주어졌을 때, 각 클래스 $j$에 대한 선형 결합 결과를 $z_j^{(i)}$ 라고 한다. 이 값은 **이미지 $x^{(i)}$가 클래스 $j$에 속한다는 근거(evidence)** 로 해석된다. $z_j^{(i)}$ 는 10개의 스코어를 담은 벡터이기에, 10 * 크기가 될 것이다. 

- $z_j^{(i)}$​가 클수록 해당 클래스일 가능성이 높다고 판단한다.
    
- 값이 작으면 그 클래스일 가능성이 낮다고 해석한다.


![](../../images/Pasted%20image%2020251203170135.png)

즉, 10개 클래스(0~9)에 대해 각각 하나씩 회귀식을 만든다고 볼 수 있다. 이 10개의 회귀식에 입력 이미지 (x)를 넣으면, 각 클래스에 해당할 가능성을 나타내는 10개의 값이 나온다. 그리고 이 출력 벡터에 softmax 함수를 적용하면, 값들이 0~1 범위로 정규화되어 각 클래스에 속할 확률 분포가 된다. 결국 softmax regression은 **여러 개의 로지스틱 회귀식을 동시에 수행하는 다중 로지스틱 회귀(multi-class logistic regression)** 라고 이해할 수 있다.

MNIST의 정답 레이블 $y^{(i)}$는 one-hot 벡터로 저장된다. 예를 들어 정답이 숫자 3이라면 다음과 같이 표현된다.
$$(0, 0, 0, 1, 0, 0, 0, 0, 0, 0)$$

즉, 해당 클래스만 1이고 나머지는 모두 0인 구조이다.


---
## Softmax Regression

자 이제 코드로 알아보자 

### 전체 코드

```c
//OpenMP Softmax Regression: initial portion
template <typename value_t, typename index_t>
void softmax_regression(value_t* input, value_t* output, value_t* weights, value_t* bias,
index_t n_input, index_t n_output) {

	# pragma omp parallel for schedule(static)
	for (index_t i = 0; i < n_output; i++) { // compute z_j = sum_i A_ji*x_i+b_j
		value_t accum = value_t(0);
		for (index_t j = 0; j < n_input; j++)
			accum += weights[i*n_input+j]*input[j];
		output[i] = accum + bias[i];
	}
	
	value_t norm = value_t(0);
	value_t mu = std::numeric_limits<value_t>::lowest();
	
	# pragma omp parallel for reduction(max:mu)
	for (index_t index = 0; index < n_output; index++)
		mu = std::max(mu, output[index]);
	
	# pragma omp parallel for
	for (index_t index = 0; index < n_output; index++)
		output[index] = std::exp(output[index]-mu);
	
	# pragma omp parallel for reduction(+ : norm)
	for (index_t index = 0; index < n_output; index++)
		norm += output[index];
	
	# pragma omp parallel for
	for (index_t index = 0; index < n_output; index++) // compute y_j = out_j/norm
	output[index] /= norm;
}
```

## 1) $z_j$ 계산하기

```c++
# pragma omp parallel for schedule(static)
for (index_t i = 0; i < n_output; i++) {
    value_t accum = value_t(0);
    for (index_t j = 0; j < n_input; j++)
        accum += weights[i*n_input+j]*input[j];
    output[i] = accum + bias[i];
}
```

- n_output(=10)개의 회귀식을 병렬로 수행한다.
- i번째 스레드는 클래스 i의 z값을 계산한다.
- 결국 output[i]는 $z_i$가 된다.


### 2) 가장 큰 z값(μ) 찾기 — Overflow 방지용

```c
value_t norm = value_t(0);
value_t mu = std::numeric_limits<value_t>::lowest();
# pragma omp parallel for reduction(max:mu)
for (index_t index = 0; index < n_output; index++)
    mu = std::max(mu, output[index]);
```

softmax 계산에서 overflow를 방지하기 위해 $mu = \max_j z_j$를 먼저 구한다.
reduction(max: mu)를 사용해 병렬로 최대값을 찾는다.

---

## 3) $exp(z_j - \mu)$계산

```c
# pragma omp parallel for
for (index_t index = 0; index < n_output; index++)
    output[index] = std::exp(output[index]-mu);
```

각 클래스별 z값에서 μ를 빼고 exponentiation을 적용한다.

---

### 4) 정규화 계수 $norm = Σ exp(z_j − μ)$계산

```c
# pragma omp parallel for reduction(+ : norm)
for (index_t index = 0; index < n_output; index++)
    norm += output[index];
```

모든 exp값의 합을 병렬로 계산한다.

---

## 5) softmax 계산: $(y_j = \frac{\exp(z_j - \mu)}{\text{norm}})$

```c
# pragma omp parallel for
for (index_t index = 0; index < n_output; index++)
    output[index] /= norm;
}
```

각 exp 결과를 norm으로 나누어 최종 softmax 확률 벡터를 얻는다.

---

지금 코드에서 고민해볼 만한 점이 몇 가지 있다.

### 1) `#pragma omp parallel for`의 과도한 반복 사용

현재 코드를 보면 `#pragma omp parallel for`가 여러 번 등장한다.  
OpenMP에서는 `#pragma omp parallel` 이 등장하는 순간 **쓰레드 팀이 생성**되고, 그 안의 `for`가 끝나면 **해당 영역을 빠져나오면서 쓰레드가 정리**된다.

지금 구조처럼 매번 `parallel for`를 쓰면,

> “스레드 팀 생성 → 루프 실행 → 스레드 정리”

이 과정이 여러 번 반복되기 때문에, 불필요하게 스레드를 만들고 없애는 오버헤드가 생긴다.

따라서 더 나은 방법은,

```c
#pragma omp parallel
{
    #pragma omp for
    for (...) {
        ...
    }

    #pragma omp for
    for (...) {
        ...
    }

    ...
}
```

이런 식으로 **가장 바깥에서 한 번만 `#pragma omp parallel`로 쓰레드 팀을 만들고**,  
각 루프 앞에는 `#pragma omp for`만 붙여서 **같은 쓰레드 팀을 재활용**하는 것이다.  
이렇게 하면 쓰레드 생성·소멸 비용을 줄일 수 있다.

---

### 2) Softmax 분모·분자 계산을 하나의 루프로 합치기

지금 코드는 softmax를 계산하기 위해 분자에 해당하는 `exp(z_j - mu)`를 먼저 계산해서 `output[j]`에 저장하고, 그다음 다른 루프에서 이를 모두 더해 `norm`(분모)을 구하는 식으로, 분자와 분모 계산을 **서로 다른 두 개의 병렬 루프**로 처리하고 있다.

그런데 잘 생각해보면, 이 두 작업은 **데이터 의존성이 없다기보다는, 사실상 “exp를 계산하면서 동시에 합을 누적할 수 있는” 관계**이기 때문에, 굳이 두 번 도는 대신 한 루프 안에서 `exp(z_j - mu)`를 계산해서 저장하고,동시에 reduction으로 합도 같이 구하는 구조로 합쳐서 돌리는 편이 더 효율적일 수 있다.

즉, 분자/분모를 완전히 분리된 두 개의 병렬 루프로 처리하기보다는, **하나의 병렬 루프로 합쳐서 캐시 친화적으로, 루프 오버헤드도 줄이는 방향**을 고민해볼 수 있다.

---

## Accuracy Evaluation
위에서의 코드는 쉽게 말해서 모델에 x 값 즉 이미지 한 장을 넣어서 그 이미지가 뭔지를 추정하는 모델을 만들었다고 보면 된다. 이 모델을 여러번 돌려서 몇 개를 맞추는지를 통해 이 모델의 정확도를 판별해보고자 한다. 

판별 방식은 이제 이미지가 여러장 주어지고, 그 이미지에 해당하는 정답도 같이 준비되어있다. label 이 있다. 그래서 이미지 한 장 넣고 모델 돌려서 나온 값을 label 과 비교해서 동일하면 counter 를 증가할 것이다.

```c++
// OpenMP Softmax Regression: accuracy evaluation
template <typename value_t, typename index_t>
value_t correct_labeled(value_t* input, value_t* label, value_t* weights, value_t* bias,
                        index_t num_entries, index_t num_features, index_t num_classes) {
    index_t counter = index_t(0);
    
    #pragma omp parallel for reduction(+: counter)
    for (index_t index = 0; index < num_entries; index++) {
        value_t output[num_classes];
        const uint64_t input_off = index*num_features;
        const uint64_t label_off = index*num_classes;

        softmax_regression(input + input_off, output, weights,
                           bias, num_features, num_classes);

        counter += argmax(output, num_classes) ==
                   argmax(label + label_off, num_classes);
    }
    return value_t(counter) / value_t(num_entries);
}

```

- **input**: 전체 입력 데이터(MNIST 이미지) 배열
- **label**: 전체 정답(one-hot) 라벨 배열
- **weights, bias**: softmax 회귀 모델의 가중치 WWW와 편향 bbb
- **num_entries**: 전체 이미지 개수
- **num_features**: 한 이미지의 픽셀 수 (MNIST 기준 784)
- **num_classes**: 클래스 수 (MNIST 기준 10)

`index * num_features`를 하면, 전체 입력 배열에서 **index번째 이미지가 시작되는 위치**가 계산된다. 이렇게 시작 위치를 구하면 해당 구간만을 하나의 이미지로 보고 처리할 수 있다.

`argmax` 함수는, 주어진 배열의 시작 주소와 크기를 받아서, 그 구간 안에 있는 element들 중 **최댓값의 인덱스**를 반환한다.  

`softmax_regression`을 거친 뒤 `output` 배열에는 각 클래스에 대한 **정규화된 확률 값**들이 들어 있고, 이 중에서 가장 큰 값의 인덱스는 곧 “이 이미지가 어느 클래스로 분류되었는가”를 의미한다. 즉, `argmax(output, num_classes)`가 바로 해당 이미지에 대한 **모델의 예측값**이라고 보면 된다.

한편, `counter` 변수는 `reduction(+: counter)`을 통해 병렬로 누적되고 있다. 각 스레드는 여러 개의 이미지를 나눠서 처리하면서,

- 예측이 맞으면 `counter += 1`,
    
- 틀리면 `counter += 0`

형태로 **자신이 담당한 구간에서 맞춘 개수만큼 counter를 증가**시키고, 마지막에 모든 스레드의 `counter` 값이 합쳐진다. 그 결과, 최종적으로는 **전체 input에 대해 모델이 맞춘 이미지의 개수**가 `counter`에 담기게 된다.

---
## Gradient Descent-based Parameter Optimazation

앞에서는 **이미 학습된 가중치 행렬이 주어져 있다**고 가정하고, 그걸 이용해 softmax regression을 수행했다. 이제는 그 **가중치 행렬이 어떻게 만들어지고 업데이트되는 과정**을 살펴보자.

가중치 행렬은 한 번에 끝나는 게 아니라, **초기값에서 시작해서 학습을 반복하면서 계속 업데이트되는 구조**이다. 아주 단순하게 말하면,

> 모델이 추측한 값(예측값)과 정답(레이블)을 비교해서,  
> 그 차이를 기준으로 가중치를 조금씩 더해주거나 빼주는 과정

이라고 볼 수 있다.

![[Pasted image 20251203175316.png]]

![](../../images/Pasted%20image%2020251203175353.png)

reduction의 대상을 array로 할 수 있다
### 전체 코드

```c++
template <typename value_t, typename index_t>
void train(value_t* input, value_t* label, value_t* weights, value_t* bias,
           index_t num_entries, index_t num_features, index_t num_classes,
           index_t num_iters = 128, value_t epsilon = 1E-1)
{
    value_t* grad_bias    = new value_t[num_classes];
    value_t* grad_weights = new value_t[num_features * num_classes];

    #pragma omp parallel   // spawn the team of threads once
    for (uint64_t iter = 0; iter < num_iters; iter++) {

        // zero gradients
        #pragma omp single
        for (index_t j = 0; j < num_classes; j++)
            grad_bias[j] = value_t(0);

        #pragma omp for collapse(2)
        for (index_t j = 0; j < num_classes; j++)
            for (index_t k = 0; k < num_features; k++)
                grad_weights[j * num_features + k] = value_t(0);


        #pragma omp for reduction(+:grad_bias[0:num_classes]) \
                         reduction(+:grad_weights[0:num_classes * num_features])
        for (index_t i = 0; i < num_entries; i++) {

            const index_t inp_off = i * num_features;
            const index_t out_off = i * num_classes;

            value_t output = new value_t[num_classes];   
            
            softmax_regression(input + inp_off, output, weights, bias,
                               num_features, num_classes);

            for (index_t j = 0; j < num_classes; j++) {

                const index_t out_ind = out_off + j;
                const value_t lbl_res = output[j] - label[out_ind];

                grad_bias[j] += lbl_res;

                const index_t wgt_off = k * num_features; 

                for (index_t k = 0; k < num_features; k++) {
                    const index_t wgt_ind = wgt_off + k;
                    const index_t inp_ind = inp_off + k;
                    grad_weights[wgt_ind] += lbl_res * input[inp_ind];
                }
            }
        }

        // update bias
        #pragma omp single
        for (index_t j = 0; j < num_classes; j++)
            bias[j] -= epsilon * grad_bias[j] / num_entries;

        // update weights
        #pragma omp for collapse(2)
        for (index_t j = 0; j < num_classes; j++)
            for (index_t k = 0; k < num_features; k++)
                weights[j * num_features + k]
                    -= epsilon * grad_weights[j * num_features + k] / num_entries;
    }
}

```

---

```cpp
template <typename value_t, typename index_t>
void train(value_t* input, value_t* label, value_t* weights, value_t* bias,
           index_t num_entries, index_t num_features,
           index_t num_classes, index_t num_iters=128, value_t epsilon=1E-1) {
```

- `input` : 전체 이미지 데이터 (num_entries × num_features)
- `label` : 전체 라벨 (num_entries × num_classes, one-hot)
- `weights`: 가중치 행렬 W (num_classes × num_features)
- `bias` : 편향 벡터 b (num_classes)
- `num_iters`: 학습 반복 횟수
- `epsilon` : 학습률(learning rate)

---

### 1. 그래디언트 메모리 준비

```cpp
value_t* grad_bias = new value_t[num_classes];
value_t* grad_weights = new value_t[num_features*num_classes];
```

- `grad_bias[j]` : 각 클래스 j에 대한 bias의 gradient ($\Delta b_j$)
- `grad_weights[...]`: 각 가중치 ($W_{j,k}$)에 대한 gradient ($\Delta W_{j,k}$)

매 iteration마다 이 배열에 gradient를 모아놓고, 마지막에 한 번에 `weights`, `bias`를 업데이트하는 구조.

---

### 2. 한 번만 thread 팀을 만들기 (외부 parallel 영역)

```cpp
# pragma omp parallel // spawn the team of threads once
for (uint64_t index = 0; index < num_iters; index++) {
    ...
}
```

---

### 3. iteration마다 gradient 0으로 초기화

```cpp
# pragma omp single // zero gradients and weights
for (index_t j = 0; j < num_classes; j++)
    grad_bias[j] = value_t(0);

# pragma omp for collapse(2)
for (index_t j = 0; j < num_classes; j++)
    for (index_t k = 0; k < num_features; k++)
        grad_weights[j*num_features+k] = value_t(0);
```

- `#pragma omp single`  
    → 여러 스레드 중 **딱 한 스레드만** 이 코드를 실행해서 `grad_bias`를 0으로 초기화.
    
- 그 다음 `#pragma omp for collapse(2)`  
    → `(j, k)` 이중 루프를 풀어서 **여러 스레드가 나눠서** `grad_weights` 전체를 0으로 초기화.

즉, **이번 iteration에서 사용할 gradient 배열들을 0으로 클리어**하는 단계.

---
### 4. 각 이미지에 대한 gradient 누적 (핵심 부분)

```cpp
# pragma omp for reduction(+:grad_bias[0:num_classes]) \
                 reduction(+:grad_weights[0:num_classes*num_features]) // softmax contributions
for (index_t i = 0; i < num_entries; i++) {
    const index_t inp_off = i*num_features;
    const index_t out_off = i*num_classes;

    value_t output = new value_t[num_classes];
    softmax_regression(input+inp_off, output, weights, bias, num_features, num_classes);

    for (index_t j = 0; j < num_classes; j++) {
        const index_t out_ind = out_off+j;
        const value_t lbl_res = output[j] - label[out_ind];

        grad_bias[j] += lbl_res;

        const index_t wgt_off = k*num_features;
        for (index_t k = 0; k < num_features; k++) {
            const index_t wgt_ind = wgt_off+k;
            const index_t inp_ind = inp_off+k;
            grad_weights[wgt_ind] += lbl_res*input[inp_ind];
        }
    }
}
```

여기가 **진짜 gradient 계산** 부분.

#### (1) `reduction(+:grad_bias[0:...])`, `reduction(+:grad_weights[0:...])`

- OpenMP 4.5 이상의 **배열 구간 reduction** 문법을 사용한 것.
- 각 쓰레드가 자신의 로컬 복사본에 gradient를 더해가고,
- 루프가 끝난 뒤 모든 쓰레드의 gradient들을 안전하게 더해 하나로 합친다.


즉, 여러 쓰레드가가 동시에 `grad_bias` / `grad_weights`를 건드려도 race 없이 합쳐지게 함.

#### (2) `inp_off`, `out_off`

```cpp
const index_t inp_off = i*num_features;
const index_t out_off = i*num_classes;
```

- `i`번째 이미지의 입력 시작 위치: `input + inp_off`
- `i`번째 라벨(one-hot)의 시작 위치: `label + out_off`

→ **배열을 2D처럼 쓰기 위한 인덱스 계산**.

#### (3) softmax forward 계산

```cpp
value_t output = new value_t[num_classes];
softmax_regression(input+inp_off, output, weights, bias, num_features, num_classes);
```

- `output[j]`에는 클래스 j에 대한 softmax 확률이 들어감.
- 여기 사실은 `value_t* output = new value_t[num_classes];` 가 되어야 맞음

#### (4) gradient 수식

```cpp
for (index_t j = 0; j < num_classes; j++) {
    const index_t out_ind = out_off+j;
    const value_t lbl_res = output[j] - label[out_ind];

    grad_bias[j] += lbl_res;
    ...
}
```

여기서

- `output[j]` : 예측 확률 ( $\hat{y}_j$ )
- `label[out_ind]` : 정답 one-hot ( $y_j$ )
- lbl_res = $\hat{y}_j - y_j$

softmax + cross entropy의 gradient 공식을 떠올리면:

$$\frac{\partial L}{\partial b_j} = \hat{y}_j - y_j  $$
$$\frac{\partial L}{\partial W_{j,k}} = (\hat{y}_j - y_j) \cdot x_k  $$

그래서

```cpp
grad_bias[j] += lbl_res;
...
grad_weights[wgt_ind] += lbl_res * input[inp_ind];
```

이게 바로 **모든 샘플에 대해 gradient를 누적하는 부분**이다.

---

### 5. bias 업데이트

```cpp
# pragma omp single // adjust bias vector
for (index_t k = 0; k < num_classes; k++)
    bias[k] -= epsilon*grad_bias[k]/num_entries;
```

- `grad_bias`는 이미 모든 스레드의 기여가 reduction으로 합쳐져 있음.
- 이제 **딱 한 스레드만** 이 값을 이용해 `bias`를 업데이트.
- 평균 gradient (`/ num_entries`)에 learning rate `epsilon`을 곱해서 내려가는 형태 (GD / mini-batch GD
    

---

### 6. weight 업데이트

```cpp
# pragma omp for collapse(2) // adjust weight matrix
for (index_t j = 0; j < num_classes; j++)
    for (index_t k = 0; k < num_features; k++)
        weights[j*num_features+k] -= epsilon*grad_weights[j*num_features+k]/num_entries;
```

- 이번에는 ($\Delta W_{j,k}$)를 가지고 실제 `weights`를 업데이트.
- 이중 루프를 `collapse(2)`로 병렬화해서, 각 스레드가 weight 행렬의 다른 부분을 나눠서 업데이트.

---

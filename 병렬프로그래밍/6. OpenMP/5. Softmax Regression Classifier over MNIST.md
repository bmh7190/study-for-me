이번 예제에서는 **MNIST 데이터**를 사용해서, **2개 레이어로 이루어진 기본적인 Softmax Regression 모델**을 구현하고, 이를 OpenMP를 활용해 병렬화해 보겠다.

![](../../images/Pasted%20image%2020251203164742.png)

Softmax Regression을 수행하면 하나의 모델이 만들어지고, 이 모델에 하나의 이미지를 입력으로 넣으면 각 픽셀에 대응되는 가중치 행렬과 곱해진다. 이렇게 선형 결합으로 나온 출력에 softmax 함수를 적용하면, 각 클래스에 대한 확률이 계산되고, 그중 가장 큰 값에 대응되는 클래스가 선택된다. 이를 통해 결과적으로 **하나의 클래스는 1, 나머지 클래스는 0으로 처리되는 형태의 분류(classification)** 가 이루어진다.

조금더 자세하게 알아보면 

![](../../images/Pasted%20image%2020251203165243.png)

가중치 행렬은 각 픽셀마다 하나의 회귀식을 가지는 구조라고 이해할 수 있다. MNIST 이미지는 28×28 크기이므로 총 784개의 픽셀이 있고, 따라서 784개의 회귀식이 존재한다. 각 회귀식은 10개의 클래스(0~9)에 대한 출력 값을 만들어야 하므로, 각 회귀식마다 10개의 가중치가 존재한다. 결국 전체 모델의 가중치 행렬은 **784 × 10** 크기를 가지게 된다.

Softmax regression에서는 입력 이미지 $x^{(i)}$가 주어졌을 때, 각 클래스 $j$에 대한 선형 결합 결과를 $z_j^{(i)}$ 라고 한다. 이 값은 **이미지 $x^{(i)}$가 클래스 $j$에 속한다는 근거(evidence)** 로 해석된다. $z_j^{(i)}$ 는 10개의 스코어를 담은 벡터이기에, 10 * 크기가 될 것이다. 

- $z_j^{(i)}$​가 클수록 해당 클래스일 가능성이 높다고 판단한다.
    
- 값이 작으면 그 클래스일 가능성이 낮다고 해석한다.


![](../../images/Pasted%20image%2020251203170135.png)

즉, 10개 클래스(0~9)에 대해 각각 하나씩 회귀식을 만든다고 볼 수 있다. 이 10개의 회귀식에 입력 이미지 (x)를 넣으면, 각 클래스에 해당할 가능성을 나타내는 10개의 값이 나온다. 그리고 이 출력 벡터에 softmax 함수를 적용하면, 값들이 0~1 범위로 정규화되어 각 클래스에 속할 확률 분포가 된다. 결국 softmax regression은 **여러 개의 로지스틱 회귀식을 동시에 수행하는 다중 로지스틱 회귀(multi-class logistic regression)** 라고 이해할 수 있다.

MNIST의 정답 레이블 $y^{(i)}$는 one-hot 벡터로 저장된다. 예를 들어 정답이 숫자 3이라면 다음과 같이 표현된다.
$$(0, 0, 0, 1, 0, 0, 0, 0, 0, 0)$$

즉, 해당 클래스만 1이고 나머지는 모두 0인 구조이다.


---
##

자 이제 코드로 알아보자 

### 전체 코드

```c
//OpenMP Softmax Regression: initial portion
template <typename value_t, typename index_t>
void softmax_regression(value_t* input, value_t* output, value_t* weights, value_t* bias,
index_t n_input, index_t n_output) {
# pragma omp parallel for schedule(static)
for (index_t i = 0; i < n_output; i++) { // compute z_j = sum_i A_ji*x_i+b_j
value_t accum = value_t(0);
for (index_t j = 0; j < n_input; j++)
accum += weights[i*n_input+j]*input[j];
output[i] = accum + bias[i];
}
value_t norm = value_t(0);
value_t mu = std::numeric_limits<value_t>::lowest();
# pragma omp parallel for reduction(max:mu)
for (index_t index = 0; index < n_output; index++) // compute mu = max(z_j)
mu = std::max(mu, output[index]);
# pragma omp parallel for
for (index_t index = 0; index < n_output; index++) // compute out_j = exp(z_j-mu)
output[index] = std::exp(output[index]-mu);
# pragma omp parallel for reduction(+ : norm)
for (index_t index = 0; index < n_output; index++) // compute norm = sum_j out_j
norm += output[index];
# pragma omp parallel for
for (index_t index = 0; index < n_output; index++) // compute y_j = out_j/norm
output[index] /= norm;
}
```

## $z_j$ 계산하기

```c++
# pragma omp parallel for schedule(static)
for (index_t i = 0; i < n_output; i++) {
    value_t accum = value_t(0);
    for (index_t j = 0; j < n_input; j++)
        accum += weights[i*n_input+j]*input[j];
    output[i] = accum + bias[i];
}
```

- n_output(=10)개의 회귀식을 병렬로 수행한다.
- i번째 스레드는 클래스 i의 z값을 계산한다.
- 결국 output[i]는 $z_i$가 된다.


### 가장 큰 z값(μ) 찾기 — Overflow 방지용

```c
value_t norm = value_t(0);
value_t mu = std::numeric_limits<value_t>::lowest();
# pragma omp parallel for reduction(max:mu)
for (index_t index = 0; index < n_output; index++)
    mu = std::max(mu, output[index]);
```

softmax 계산에서 overflow를 방지하기 위해  
$mu = \max_j z_j$
를 먼저 구한다.

- reduction(max: mu)를 사용해 병렬로 최대값을 찾는다.
    

---

## 3) (\exp(z_j - \mu)) 계산

```c
# pragma omp parallel for
for (index_t index = 0; index < n_output; index++)
    output[index] = std::exp(output[index]-mu);
```

각 클래스별 z값에서 μ를 빼고 exponentiation을 적용한다.

---

## 4) 정규화 계수 norm = Σ exp(z_j − μ) 계산

```c
# pragma omp parallel for reduction(+ : norm)
for (index_t index = 0; index < n_output; index++)
    norm += output[index];
```

모든 exp값의 합을 병렬로 계산한다.

---

## 5) softmax 계산: (y_j = \frac{\exp(z_j - \mu)}{\text{norm}})

```c
# pragma omp parallel for
for (index_t index = 0; index < n_output; index++)
    output[index] /= norm;
}
```

각 exp 결과를 norm으로 나누어 최종 softmax 확률 벡터를 얻는다.

---

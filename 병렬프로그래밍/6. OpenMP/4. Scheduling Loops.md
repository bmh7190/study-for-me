지금까지는 모든 연산량이 동일하다고 가정하고 병렬화를 적용했지만, 실제로는 그렇지 않은 경우가 많다. 예를 들어 아래와 같은 코드에서는 `i`가 작을수록 내부적으로 수행되는 `(i, j)` 조합의 수가 많아지기 때문에, 각 반복(iteration)마다 연산량이 균일하지 않다.

![](../../images/Pasted%20image%2020251203160618.png)

```c++
for (index_t i = 0; i < num_entries; i++)
	for (index_t j = i; j < num_entries; j++) {
		value_t accum = value_t(0);
		for (index_t k = 0; k < num_features; k++)
			accum += data[i*num_features+k] * data[j*num_features+k];
		delta[i*num_entries+j] = delta[j*num_entries+i] = accum;
}
```

지금까지는 모든 연산량이 동일하다고 가정하고 병렬화를 적용했지만, 실제로는 그렇지 않은 경우가 많다. 예를 들어 아래와 같은 코드에서는 `i`가 작을수록 내부적으로 수행되는 `(i, j)` 조합의 수가 많아지기 때문에, 각 반복(iteration)마다 연산량이 균일하지 않다.

---
## Scheduling Loops
사실 OpenMP에서는 각 스레드에게 작업을 어떻게 분배할지에 대한 방식을 지정해줄 수 있다.

![](../../images/Pasted%20image%2020251203160923.png)

위와 같은 형태로 스케줄링을 선언할 수 있으며, **mode**는 작업 분배 전략을, **chunk size**는 한 번에 각 스레드가 가져가는 작업 단위를 의미한다.  
OpenMP의 기본 설정은 **block(static) 분배 방식**이다.

---
### Static mode
가장 먼저 살펴볼 방식은 **고정(static) 할당 방식**이다.

![](../../images/Pasted%20image%2020251203161210.png)

`static` 모드에 chunk size를 지정하면, 예를 들어 3개의 스레드가 있을 때 다음과 같이 작업이 순서대로 분배된다. chunk size를 **1**로 설정하면 각 스레드가 번갈아가며 하나씩 가져가므로, 사실상 **cyclic distribution**과 동일한 형태가 된다.

![](../../images/Pasted%20image%2020251203161309.png)

chunk size를 **2**로 설정하면 이번에는 각 스레드가 한 번에 두 개씩의 작업을 가져가게 되고, 위 그림과 같은 식으로 작업이 배분된다. 

---
### Other mode
static 한 방식 말고도 여러 방버들이 있다.

### Dynamic
Dynamic 모드는 말 그대로 **각 스레드가 맡은 일을 끝내는 대로 새로운 작업을 가져가는 방식**이다. 즉, 스레드들이 작업을 “선착순”으로 가져가는 형태라고 보면 된다.

이때 모든 반복(iteration)은 일정한 크기의 chunk 단위로 잘려 있으며, **어떤 스레드가 먼저 비는지에 따라 다음 chunk가 순차적으로 할당된다.**

반복들은 동일한 크기의 chunk로 나누어지며, **작업을 끝낸 스레드에게 즉시 다음 chunk가 배정된다.** 스레드가 모두 마지막 chunk를 처리하고 있을 때만 idle 상태가 발생할 수 있다. 별도로 chunk size를 지정하지 않으면 **기본값은 1**이다.
### Guided
Guided 모드는 **처음에는 큰 chunk를 스레드에 할당하고, 진행할수록 chunk 크기를 점점 줄여 나가는 방식**이다. 마지막에는 사용자 지정 chunk size(기본값은 1)까지 감소한다.

즉, 작업을 큰 덩어리부터 빠르게 소진한 뒤, 후반부에는 작은 단위로 나누면서 스레드 간 부하 균형을 맞추려는 전략이다. 다만 작업량이 역삼각형 형태로 편중되어 있는 상황에서는, 이 방식이 완전한 해결책이 되지 않을 수도 있다.

전체 반복은 점점 줄어드는 크기의 chunk로 나뉘며 각 chunk는 dynamic 모드와 동일하게 작업을 마친 스레드에게 순차적으로 할당된다. chunk size를 지정하지 않을 경우 **기본값은 1**이다.

### Auto
`auto` 모드는 앞서 살펴본 여러 스케줄링 전략(static, dynamic, guided 등)을  
**어떤 방식으로 적용할지를 컴파일러나 런타임 시스템이 자동으로 결정하도록 맡기는 방식**이다.

즉, 프로그래머가 명시적으로 스케줄링 전략을 선택하지 않고,  
시스템이 내부적으로 가장 적합하다고 판단되는 분배 방식을 선택해 실행한다.

### Runtime 
`runtime` 모드는 **실제 스케줄링 방식이 코드 내부가 아니라 환경 변수에 따라 결정되는 방식**이다. 즉, 프로그램 실행 시점에서 환경 변수 `OMP_SCHEDULE`을 통해  
사용할 스케줄링 모드와 chunk size를 지정할 수 있다.

이를 사용하면 코드를 다시 컴파일하지 않고도 실험적으로 다양한 스케줄링 전략을 적용해볼 수 있다는 장점이 있다.

---
## Triangular Matrix

## Reduction Operator

마지막 x는 어떻게 될까?

```c++
uint64_t x = 5;
#pragma omp parallel for reduction(+:x) num_threads(2)
for (uint64_t i = 0; i < 10; i++)
	x += 1;
std::cout << x << std::endl;
```

우선 `i`에 대한 반복은 여러 thread에 나누어 할당된다. 이때 `reduction`이 동작할 때는, **어떤 연산을 쓰느냐에 따라** 각 thread가 사용하는 초기값(항등원, neutral element)이 달라진다.

이번 예제에서는 `reduction(+: x)`처럼 **더하기 연산**을 사용하고 있으므로, 각 thread는 `x`의 private 복사본을 **0으로 초기화**해서 사용한다. 그러면 각 thread는 자신에게 할당된 구간에서 반복문을 돌면서, 그 구간에 대해 `x`를 0에서부터 차곡차곡 증가시킨다.

모든 thread가 자신의 반복을 끝내고 나면, 이제 이 부분 결과들을 한 번에 모아서 더하게 된다. 이때 원래 전역 변수 `x`의 초기값이 5였고, 전체 반복에서 `+1`이 총 10번 수행되었으므로, 최종 결과는

> 5(초기값) + 10(모든 thread의 누적 증가량) = **15** 가 된다.

```c++
uint64_t x = 5;
#pragma omp parallel for reduction(min:x) num_threads(2)
for (uint64_t i = 0; i < 10; i++)
	x += 1;
std::cout << x << std::endl;
```

이 경우에는 reduction의 연산자가 `min`(최솟값)이다. 따라서 각 스레드에서 사용하는 `x`의 초기값(항등원)은 **시스템에서 표현할 수 있는 최댓값**이 된다. 그래야 어떤 값과 비교하더라도 올바르게 최솟값을 찾을 수 있다.

그런데 여기서 문제는, 반복문 안에서 `x`에 계속 `1`을 더하는 구조라는 점이다. 각 스레드의 `x`는 처음에 `UINT64_MAX`로 초기화되어 있을 텐데, 여기에 `1`을 더하면 오버플로우가 발생해서 `0`이 되고, 이후 반복적으로 더해지면서 `1, 2, 3, 4 …` 이런 식으로 증가한다. 예제처럼 각 스레드가 5번씩 더한다고 하면, 최종적으로 각 스레드의 로컬 `x` 값은 `4`가 된다.

마지막에 reduction을 수행할 때는 **원래 전역 변수의 값인 `5`와 각 스레드의 부분 결과들(`4`, `4`)** 이 모두 비교 대상이 된다. 결국 `min(5, 4, 4)`가 계산되면 4가 출력되는 것이다!

이처럼 reduction의 연산자와 loop안의 연산의 종류를 맞추는 것은 물론이고, 연산마다 항등원이 정해져 있으니까 알고 써야한다. 

---
## Declaring Custom Parallel Reductions

OpenMP에서는 기본적인 reduction 연산(+, *, min, max 등) 외에도, 사용자가 직접 **커스텀 reduction 연산**을 정의할 수 있다. 

우선 커스텀 reduction을 사용하기 위해서는 몇 가지 법칙을 만족해야한다. 결합법칙 교환법칙은 무조건 일단 만족해야 한다. thread 끼리의 연산 순서가 달라져서 값이 달라지면 안되기 때문이다. 그리고 항등원이 존재해야 한다

![](../../images/Pasted%20image%2020251203012154.png)

- **identifier**: reduction의 이름이다. `reduction(identifier:variable)` 형태로 사용된다.

- **type**: 이 reduction이 적용될 데이터 타입이다. 예를 들어 `int`, `double`, 아니면 사용자가 만든 struct 타입도 가능하다.

- **combiner**: 여러 스레드가 계산한 partial 값들을 **어떻게 합칠지 정의하는 식**이다. 이 식 안에는 `omp_in`과 `omp_out` 두 개의 특별한 변수가 등장한다.

    - `omp_in`: 현재 스레드가 가진 partial 값
    - `omp_out`: 지금까지 누적된 partial 값  

	 둘을 어떻게 합칠지 직접 지정해주는 형태다.
	 
	- 예: `omp_out = omp_out + omp_in;`

- **initializer**: 스레드별로 만들어지는 private 변수(= omp_priv)의 **초기값**을 설정한다. 보통 neutral element(중립 원소) 값을 세팅한다.

	- 예: `omp_priv = 0`
---
